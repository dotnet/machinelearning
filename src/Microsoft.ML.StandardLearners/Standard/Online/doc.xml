<?xml version="1.0" encoding="utf-8"?>
<doc>
  <members>
    
    <member name="OGD">
      <summary>
        Stochastic gradient descent is an optimization method used to train a wide range of models in machine learning. 
        In the ML.Net the implementation of OGD, is for linear regression. 
      </summary>
      <remarks>
        Stochastic gradient descent uses a simple yet efficient iterative technique to fit model coefficients using error gradients for convex loss functions.
        The OnlineGradientDescentRegressor implements the standard (non-batch) SGD, with a choice of loss functions,
        and an option to update the weight vector using the average of the vectors seen over time (averaged argument is set to True by default).
      </remarks>
    </member>
    <example name="OGD">
      <example>
        <code language="csharp">
          new OnlineGradientDescentRegressor
          {
            NumIterations = 10,
            L2RegularizerWeight = 0.6f,
            LossFunction = new PoissonLossRegressionLossFunction()
          }
        </code>
      </example>
    </example>

    <member name="AP">
      <summary>
        Averaged Perceptron Binary Classifier. 
      </summary>
      <remarks>
        Perceptron is a classification algorithm that makes its predictions based on a linear function.
        I.e., for an instance with feature values f0, f1,..., f_D-1, , the prediction is given by the sign of sigma[0,D-1] ( w_i * f_i), where w_0, w_1,...,w_D-1 are the weights computed by the algorithm.
        <para>
          Perceptron is an online algorithm, i.e., it processes the instances in the training set one at a time.
          The weights are initialized to be 0, or some random values. Then, for each example in the training set, the value of sigma[0, D-1] (w_i * f_i) is computed.
          If this value has the same sign as the label of the current example, the weights remain the same. If they have opposite signs,
          the weights vector is updated by either subtracting or adding (if the label is negative or positive, respectively) the feature vector of the current example,
          multiplied by a factor 0 &lt; a &lt;= 1, called the learning rate. In a generalization of this algorithm, the weights are updated by adding the feature vector multiplied by the learning rate,
          and by the gradient of some loss function (in the specific case described above, the loss is hinge-loss, whose gradient is 1 when it is non-zero).
        </para>
        <para>
          In Averaged Perceptron (AKA voted-perceptron), the weight vectors are stored,
          together with a weight that counts the number of iterations it survived (this is equivalent to storing the weight vector after every iteration, regardless of whether it was updated or not).
          The prediction is then calculated by taking the weighted average of all the sums sigma[0, D-1] (w_i * f_i) or the different weight vectors.
        </para>
        <para> For more information see:</para>
        <para><a href='https://en.wikipedia.org/wiki/Perceptron'>Wikipedia entry for Perceptron</a></para>
        <para><a href='https://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.48.8200'>Large Margin Classification Using the Perceptron Algorithm</a></para>
      </remarks>
    </member>
    <example>
      <example name="AP">
        <code language="csharp">
          new AveragedPerceptronBinaryClassifier
          {
            NumIterations = 10,
            L2RegularizerWeight = 0.01f,
            LossFunction = new ExpLossClassificationLossFunction()
          }
        </code>
      </example>
    </example>

  </members>
</doc>
