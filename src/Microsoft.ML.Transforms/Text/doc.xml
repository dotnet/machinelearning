<?xml version="1.0" encoding="utf-8" ?>
<doc>
  <members>

    <member name="TextTransform">
      <summary>
        A transform that turns a collection of text documents into numerical feature vectors.
        The feature vectors are normalized counts of (word and/or character) ngrams in a given tokenized text.
      </summary>
      <remarks>
        The TextFeaturizer transform gives user one-stop solution for doing:
        <list type="bullet">
          <item><description>Language Detection</description></item>
          <item><description>Tokenzation​</description></item>
          <item><description>Text normalization</description></item>
          <item><description>Predefined and custom stopwords removal.</description></item>
          <item><description>Word-based or character-based Ngram and SkipGram extraction.​</description></item>
          <item><description>TF, IDF or TF-IDF.</description></item>
          <item><description>L-p vector normalization.​</description></item>
        </list>
        The TextFeaturizer will show the transformed text, after being applied.
        It converts a collection of text columns to a matrix of token  ngrams/skip-grams counts.
        Features are made of (word/character) n-grams/skip-grams​ and the number of features are equal to the vocabulary size found by analyzing the data.
      </remarks>
      <example>
        <code>
          pipeline.Add(new TextFeaturizer(&quot;Features&quot;, &quot;SentimentText&quot;)
          {
            KeepDiacritics = false,
            KeepPunctuations = false,
            TextCase = TextNormalizerTransformCaseNormalizationMode.Lower,
            OutputTokens = true,
            StopWordsRemover = new PredefinedStopWordsRemover(),
            VectorNormalizer = TextTransformTextNormKind.L2,
            CharFeatureExtractor = new NGramNgramExtractor() { NgramLength = 3, AllLengths = false },
            WordFeatureExtractor = new NGramNgramExtractor() { NgramLength = 2, AllLengths = true }
          });
        </code>
      </example>
    </member>

    <member name="WordTokenizer">
      <summary>
        The input to this transform is text, and the output is a vector of text containing the words (tokens) in the original text. 
        The separator is space, but can be specified as any other character (or multiple characters) if needed.
      </summary>
      <remarks>
        The input for this transform is a <see cref="Microsoft.ML.Runtime.Data.DvText">DvText</see> or a vector of <see cref="Microsoft.ML.Runtime.Data.DvText">DvTexts</see>,
        and its output is a vector of DvTexts, corresponding to the tokens in the input text.
        The output is generated by splitting the input text, using a set of user specified separator characters.
        Empty strings and strings containing only spaces are dropped.
        This transform is not typically used on its own, but it is one of the transforms composing the Text Featurizer.
      </remarks>
      <example>
        <code>
          pipeline.Add( new WordTokenizer(&quot;TextColumn&quot;){ TermSeparators = &quot;&apos; &apos;, &apos;\t&apos;, &apos;;&apos;&quot;  } );
        </code>
      </example>
    </member>

    <member name="NgramTranslator">
      <summary>
        This transform produces a bag of counts of n-grams (sequences of consecutive values of length 1-n) in a given vector of keys. 
        It does so by building a dictionary of n-grams and using the id in the dictionary as the index in the bag.
      </summary>
      <remarks>
        This transform produces a matrix of token ngrams/skip-grams counts for a given corpus of text.
        The n-grams are represented as count vectors, with vector slots corresponding to n-grams.
        Embedding ngrams in a vector space allows their contents to be compared in an efficient manner. 
        The slot values in the vector can be weighted by the following factors:
        <list>
          <item><description>term frequency - The number of occurrences of the slot in the text</description></item>
          <item><description>
              inverse document frequency - A ratio (the logarithm of inverse relative slot frequency)
              that measures the information a slot provides by determining how common or rare it is across the entire text.
            </description></item>
            <item><description>term frequency-inverse document frequency - the product term frequency and the inverse document frequency.</description></item>
        </list>
        This transform is not typically used on its own, but it is one of the transforms composing the <see cref="Microsoft.ML.Transforms.TextFeaturizer">Text Featurizer</see> .
      </remarks>
      <seealso cref="Microsoft.ML.Transforms.WordTokenizer"/>
      <seealso cref="Microsoft.ML.Transforms.TextToKey"/>
      <seealso cref="Microsoft.ML.Transforms.TextFeaturizer"/>
      <seealso cref="Microsoft.ML.Transforms.CharacterTokenizer"/>
      <example>
        <code>
          pipeline.Add(new NGramTranslator(&quot;TextColumn&quot;){ Weighting=NgramTransformWeightingCriteria.TfIdf  } );
      </code>
      </example>
    </member>

    <member name="SentimentAnalyzer">
      <summary>
        Uses a pretrained sentiment model to score input strings.
      </summary>
      <remarks>
        <para>The Sentiment transform returns the probability that the sentiment of a natural text is positive. </para>
        <para>
          The model was trained with the <a href="http://anthology.aclweb.org/P/P14/P14-1146.pdf">Sentiment-specific word embedding (SSWE)</a>  and NGramFeaturizer on Twitter sentiment data,
          similarly to the sentiment analysis part of the
          <a href="https://www.microsoft.com/cognitive-services/en-us/text-analytics-api">Text Analytics cognitive service</a>. 
          The transform outputs a score between 0 and 1 as a sentiment prediction 
          (where 0 is a negative sentiment and 1 is a positive sentiment).</para> 
          <para>Currently it supports only English.</para>
      </remarks>
      <example>
        <code>
          pipeline.Add(new SentimentAnalyzer(){ Source = &quot;TextColumn&quot; }  );
        </code>
      </example>
    </member>

    <member name="CharacterTokenizer">
      <summary>
        This transform breaks text into individual tokens, each consisting of individual character.
      </summary>
      <remarks>
      This transform is not typically used on its own, but it is one of the transforms composing the 
      <see cref="Microsoft.ML.Transforms.TextFeaturizer">Text Featurizer</see>. 
      </remarks>
      <seealso cref="Microsoft.ML.Transforms.WordTokenizer"/>
      <seealso cref="Microsoft.ML.Transforms.TextToKey"/>
      <seealso cref="Microsoft.ML.Transforms.NGramTranslator"/>
      <seealso cref="Microsoft.ML.Transforms.TextFeaturizer"/>
      <example>
        <code>
          pipeline.Add(new CharacterTokenizer("TextCol1" , "TextCol2" ) );
        </code>
      </example>
    </member>

    <member name="LightLDA">
      <summary>
        The LDA transform implements LightLDA, a state-of-the-art implementation of Latent Dirichlet Allocation.
      </summary>
      <remarks>
        Latent Dirichlet Allocation is a well-known topic modeling algorithm that infers topical structure from text data,
        and can be used to featurize any text fields as low-dimensional topical vectors. 
        <para>LightLDA is an extremely efficient implementation of LDA developed in MSR-Asia that incorporates a number of 
         optimization techniques. See <a href="http://arxiv.org/abs/1412.1576">LightLDA: Big Topic Models on Modest Compute Clusters</a>.
        </para>
        <para>
          With the LDA transform, ML.NET users can train a topic model to produce 1 million topics with 1 million vocabulary
          on a 1-billion-token document set one a single machine in a few hours (typically, LDA at this scale takes days and requires large clusters).
          The most significant innovation is a super-efficient O(1) <a href="https://en.wikipedia.org/wiki/Metropolis–Hastings_algorithm">Metropolis-Hastings sampling algorithm</a>,
          whose running cost is (surprisingly) agnostic of model size,
          allowing it to converges nearly an order of magnitude faster than other <a href="https://en.wikipedia.org/wiki/Gibbs_sampling">Gibbs samplers.</a>
        </para>
        <para>
          For more details please see original LightLDA paper, and its open source implementation. 
          <list>
            <item><description><a href="http://arxiv.org/abs/1412.1576"> LightLDA: Big Topic Models on Modest Computer Clusters</a></description></item>
            <item><description><a href=" https://github.com/Microsoft/LightLDA">LightLDA </a></description></item>
          </list>
        </para>
      </remarks>
      <example>
        <code>
          pipeline.Add(new LightLda(("InTextCol" , "OutTextCol")));
        </code>
      </example>
    </member>

  </members>
</doc>
