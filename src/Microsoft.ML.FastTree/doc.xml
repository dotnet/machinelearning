<?xml version="1.0" encoding="utf-8"?>
<doc>
  <members>
    <member name="TreeEnsembleFeaturizerTransform">
      <summary>
        Trains a tree ensemble, or loads it from a file, then maps a numeric feature vector to outputs.
      </summary>
      <remarks>
        In machine learning it is a pretty common and powerful approach to utilize the already trained model in the process of defining features.
        <para>One such example would be the use of model&apos;s scores as features to downstream models. For example, we might run clustering on the original features, 
        and use the cluster distances as the new feature set.
        Instead of consuming the model&apos;s output, we could go deeper, and extract the &apos;intermediate outputs&apos; that are used to produce the final score. </para>
        There are a number of famous or popular examples of this technique:
        <list type='bullet'>
          <item><description>A deep neural net trained on the ImageNet dataset, with the last layer removed, is commonly used to compute the &apos;projection&apos; of the image into the &apos;semantic feature space&apos;.
            It is observed that the Euclidean distance in this space often correlates with the &apos;semantic similarity&apos;: that is, all pictures of pizza are located close together,
            and far away from pictures of kittens. </description></item>
          <item><description>A matrix factorization and/or LDA model is also often used to extract the &apos;latent topics&apos; or &apos;latent features&apos; associated with users and items.</description></item>
          <item><description>The weights of the linear model are often used as a crude indicator of &apos;feature importance&apos;. At the very minimum, the 0-weight features are not needed by the model,
            and there&apos;s no reason to compute them. </description></item>
        </list>
        <para>
          Tree featurizer uses the decision tree ensembles for feature engineering in the same fashion as above.
          It trains a tree ensemble, or loads it from a file, then maps a numeric feature vector to three outputs:
        </para>
        <list type='number'>
          <item><description>A vector containing the individual tree outputs of the tree ensemble.</description></item>
          <item><description>A vector indicating the leaves that the feature vector falls on in the tree ensemble.</description></item>
          <item><description>A vector indicating the paths that the feature vector falls on in the tree ensemble.</description></item>
        </list>
        If a both a model file and a trainer are specified - will use the model file. If neither are specified,
        will train a default FastTree model.
        This can handle key labels by training a regression model towards their optionally permuted indices.
        <para>Let&apos;s assume that we&apos;ve built a tree ensemble of 100 trees with 100 leaves each (it doesn&apos;t matter whether boosting was used or not in training). 
        If we associate each leaf of each tree with a sequential integer, we can, for every incoming example x, 
        produce an indicator vector L(x), where Li(x) = 1 if the example x &apos;falls&apos; into the leaf #i, and 0 otherwise.</para>
        <para>Thus, for every example x, we produce a 10000-valued vector L, with exactly 100 1s and the rest zeroes. 
        This &apos;leaf indicator&apos; vector can be considered the ensemble-induced &apos;footprint&apos; of the example.</para>
        <para>The &apos;distance&apos; between two examples in the L-space is actually a Hamming distance, and is equal to the number of trees that do not distinguish the two examples.</para>
        <para>We could repeat the same thought process for the non-leaf, or internal, nodes of the trees (we know that each tree has exactly 99 of them in our 100-leaf example), 
        and produce another indicator vector, N (size 9900), for each example, indicating the &apos;trajectory&apos; of each example through each of the trees.</para>
        <para>The distance in the combined 19900-dimensional LN-space will be equal to the number of &apos;decisions&apos; in all trees that &apos;agree&apos; on the given pair of examples.</para>
        <para>The TreeLeafFeaturizer is also producing the third vector, T, which is defined as Ti(x) = output of tree #i on example x.</para>
      </remarks>
      <example>
        <code language="csharp">
          pipeline.Add(new TreeLeafFeaturizer())
        </code>
      </example>
    </member>
        
  </members>
</doc>
