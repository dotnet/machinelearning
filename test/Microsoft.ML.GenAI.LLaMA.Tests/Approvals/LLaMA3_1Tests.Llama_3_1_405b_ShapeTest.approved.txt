0: lm_head.weight shape: [128256, 16384]
1: model.embed_tokens.weight shape: [128256, 16384]
2: model.layers.0.input_layernorm.weight shape: [16384]
3: model.layers.0.mlp.down_proj.weight shape: [16384, 53248]
4: model.layers.0.mlp.gate_proj.weight shape: [53248, 16384]
5: model.layers.0.mlp.up_proj.weight shape: [53248, 16384]
6: model.layers.0.post_attention_layernorm.weight shape: [16384]
7: model.layers.0.self_attn.k_proj.weight shape: [1024, 16384]
8: model.layers.0.self_attn.o_proj.weight shape: [16384, 16384]
9: model.layers.0.self_attn.q_proj.weight shape: [16384, 16384]
10: model.layers.0.self_attn.v_proj.weight shape: [1024, 16384]
11: model.layers.1.input_layernorm.weight shape: [16384]
12: model.layers.1.mlp.down_proj.weight shape: [16384, 53248]
13: model.layers.1.mlp.gate_proj.weight shape: [53248, 16384]
14: model.layers.1.mlp.up_proj.weight shape: [53248, 16384]
15: model.layers.1.post_attention_layernorm.weight shape: [16384]
16: model.layers.1.self_attn.k_proj.weight shape: [1024, 16384]
17: model.layers.1.self_attn.o_proj.weight shape: [16384, 16384]
18: model.layers.1.self_attn.q_proj.weight shape: [16384, 16384]
19: model.layers.1.self_attn.v_proj.weight shape: [1024, 16384]
20: model.layers.10.input_layernorm.weight shape: [16384]
21: model.layers.10.mlp.down_proj.weight shape: [16384, 53248]
22: model.layers.10.mlp.gate_proj.weight shape: [53248, 16384]
23: model.layers.10.mlp.up_proj.weight shape: [53248, 16384]
24: model.layers.10.post_attention_layernorm.weight shape: [16384]
25: model.layers.10.self_attn.k_proj.weight shape: [1024, 16384]
26: model.layers.10.self_attn.o_proj.weight shape: [16384, 16384]
27: model.layers.10.self_attn.q_proj.weight shape: [16384, 16384]
28: model.layers.10.self_attn.v_proj.weight shape: [1024, 16384]
29: model.layers.100.input_layernorm.weight shape: [16384]
30: model.layers.100.mlp.down_proj.weight shape: [16384, 53248]
31: model.layers.100.mlp.gate_proj.weight shape: [53248, 16384]
32: model.layers.100.mlp.up_proj.weight shape: [53248, 16384]
33: model.layers.100.post_attention_layernorm.weight shape: [16384]
34: model.layers.100.self_attn.k_proj.weight shape: [1024, 16384]
35: model.layers.100.self_attn.o_proj.weight shape: [16384, 16384]
36: model.layers.100.self_attn.q_proj.weight shape: [16384, 16384]
37: model.layers.100.self_attn.v_proj.weight shape: [1024, 16384]
38: model.layers.101.input_layernorm.weight shape: [16384]
39: model.layers.101.mlp.down_proj.weight shape: [16384, 53248]
40: model.layers.101.mlp.gate_proj.weight shape: [53248, 16384]
41: model.layers.101.mlp.up_proj.weight shape: [53248, 16384]
42: model.layers.101.post_attention_layernorm.weight shape: [16384]
43: model.layers.101.self_attn.k_proj.weight shape: [1024, 16384]
44: model.layers.101.self_attn.o_proj.weight shape: [16384, 16384]
45: model.layers.101.self_attn.q_proj.weight shape: [16384, 16384]
46: model.layers.101.self_attn.v_proj.weight shape: [1024, 16384]
47: model.layers.102.input_layernorm.weight shape: [16384]
48: model.layers.102.mlp.down_proj.weight shape: [16384, 53248]
49: model.layers.102.mlp.gate_proj.weight shape: [53248, 16384]
50: model.layers.102.mlp.up_proj.weight shape: [53248, 16384]
51: model.layers.102.post_attention_layernorm.weight shape: [16384]
52: model.layers.102.self_attn.k_proj.weight shape: [1024, 16384]
53: model.layers.102.self_attn.o_proj.weight shape: [16384, 16384]
54: model.layers.102.self_attn.q_proj.weight shape: [16384, 16384]
55: model.layers.102.self_attn.v_proj.weight shape: [1024, 16384]
56: model.layers.103.input_layernorm.weight shape: [16384]
57: model.layers.103.mlp.down_proj.weight shape: [16384, 53248]
58: model.layers.103.mlp.gate_proj.weight shape: [53248, 16384]
59: model.layers.103.mlp.up_proj.weight shape: [53248, 16384]
60: model.layers.103.post_attention_layernorm.weight shape: [16384]
61: model.layers.103.self_attn.k_proj.weight shape: [1024, 16384]
62: model.layers.103.self_attn.o_proj.weight shape: [16384, 16384]
63: model.layers.103.self_attn.q_proj.weight shape: [16384, 16384]
64: model.layers.103.self_attn.v_proj.weight shape: [1024, 16384]
65: model.layers.104.input_layernorm.weight shape: [16384]
66: model.layers.104.mlp.down_proj.weight shape: [16384, 53248]
67: model.layers.104.mlp.gate_proj.weight shape: [53248, 16384]
68: model.layers.104.mlp.up_proj.weight shape: [53248, 16384]
69: model.layers.104.post_attention_layernorm.weight shape: [16384]
70: model.layers.104.self_attn.k_proj.weight shape: [1024, 16384]
71: model.layers.104.self_attn.o_proj.weight shape: [16384, 16384]
72: model.layers.104.self_attn.q_proj.weight shape: [16384, 16384]
73: model.layers.104.self_attn.v_proj.weight shape: [1024, 16384]
74: model.layers.105.input_layernorm.weight shape: [16384]
75: model.layers.105.mlp.down_proj.weight shape: [16384, 53248]
76: model.layers.105.mlp.gate_proj.weight shape: [53248, 16384]
77: model.layers.105.mlp.up_proj.weight shape: [53248, 16384]
78: model.layers.105.post_attention_layernorm.weight shape: [16384]
79: model.layers.105.self_attn.k_proj.weight shape: [1024, 16384]
80: model.layers.105.self_attn.o_proj.weight shape: [16384, 16384]
81: model.layers.105.self_attn.q_proj.weight shape: [16384, 16384]
82: model.layers.105.self_attn.v_proj.weight shape: [1024, 16384]
83: model.layers.106.input_layernorm.weight shape: [16384]
84: model.layers.106.mlp.down_proj.weight shape: [16384, 53248]
85: model.layers.106.mlp.gate_proj.weight shape: [53248, 16384]
86: model.layers.106.mlp.up_proj.weight shape: [53248, 16384]
87: model.layers.106.post_attention_layernorm.weight shape: [16384]
88: model.layers.106.self_attn.k_proj.weight shape: [1024, 16384]
89: model.layers.106.self_attn.o_proj.weight shape: [16384, 16384]
90: model.layers.106.self_attn.q_proj.weight shape: [16384, 16384]
91: model.layers.106.self_attn.v_proj.weight shape: [1024, 16384]
92: model.layers.107.input_layernorm.weight shape: [16384]
93: model.layers.107.mlp.down_proj.weight shape: [16384, 53248]
94: model.layers.107.mlp.gate_proj.weight shape: [53248, 16384]
95: model.layers.107.mlp.up_proj.weight shape: [53248, 16384]
96: model.layers.107.post_attention_layernorm.weight shape: [16384]
97: model.layers.107.self_attn.k_proj.weight shape: [1024, 16384]
98: model.layers.107.self_attn.o_proj.weight shape: [16384, 16384]
99: model.layers.107.self_attn.q_proj.weight shape: [16384, 16384]
100: model.layers.107.self_attn.v_proj.weight shape: [1024, 16384]
101: model.layers.108.input_layernorm.weight shape: [16384]
102: model.layers.108.mlp.down_proj.weight shape: [16384, 53248]
103: model.layers.108.mlp.gate_proj.weight shape: [53248, 16384]
104: model.layers.108.mlp.up_proj.weight shape: [53248, 16384]
105: model.layers.108.post_attention_layernorm.weight shape: [16384]
106: model.layers.108.self_attn.k_proj.weight shape: [1024, 16384]
107: model.layers.108.self_attn.o_proj.weight shape: [16384, 16384]
108: model.layers.108.self_attn.q_proj.weight shape: [16384, 16384]
109: model.layers.108.self_attn.v_proj.weight shape: [1024, 16384]
110: model.layers.109.input_layernorm.weight shape: [16384]
111: model.layers.109.mlp.down_proj.weight shape: [16384, 53248]
112: model.layers.109.mlp.gate_proj.weight shape: [53248, 16384]
113: model.layers.109.mlp.up_proj.weight shape: [53248, 16384]
114: model.layers.109.post_attention_layernorm.weight shape: [16384]
115: model.layers.109.self_attn.k_proj.weight shape: [1024, 16384]
116: model.layers.109.self_attn.o_proj.weight shape: [16384, 16384]
117: model.layers.109.self_attn.q_proj.weight shape: [16384, 16384]
118: model.layers.109.self_attn.v_proj.weight shape: [1024, 16384]
119: model.layers.11.input_layernorm.weight shape: [16384]
120: model.layers.11.mlp.down_proj.weight shape: [16384, 53248]
121: model.layers.11.mlp.gate_proj.weight shape: [53248, 16384]
122: model.layers.11.mlp.up_proj.weight shape: [53248, 16384]
123: model.layers.11.post_attention_layernorm.weight shape: [16384]
124: model.layers.11.self_attn.k_proj.weight shape: [1024, 16384]
125: model.layers.11.self_attn.o_proj.weight shape: [16384, 16384]
126: model.layers.11.self_attn.q_proj.weight shape: [16384, 16384]
127: model.layers.11.self_attn.v_proj.weight shape: [1024, 16384]
128: model.layers.110.input_layernorm.weight shape: [16384]
129: model.layers.110.mlp.down_proj.weight shape: [16384, 53248]
130: model.layers.110.mlp.gate_proj.weight shape: [53248, 16384]
131: model.layers.110.mlp.up_proj.weight shape: [53248, 16384]
132: model.layers.110.post_attention_layernorm.weight shape: [16384]
133: model.layers.110.self_attn.k_proj.weight shape: [1024, 16384]
134: model.layers.110.self_attn.o_proj.weight shape: [16384, 16384]
135: model.layers.110.self_attn.q_proj.weight shape: [16384, 16384]
136: model.layers.110.self_attn.v_proj.weight shape: [1024, 16384]
137: model.layers.111.input_layernorm.weight shape: [16384]
138: model.layers.111.mlp.down_proj.weight shape: [16384, 53248]
139: model.layers.111.mlp.gate_proj.weight shape: [53248, 16384]
140: model.layers.111.mlp.up_proj.weight shape: [53248, 16384]
141: model.layers.111.post_attention_layernorm.weight shape: [16384]
142: model.layers.111.self_attn.k_proj.weight shape: [1024, 16384]
143: model.layers.111.self_attn.o_proj.weight shape: [16384, 16384]
144: model.layers.111.self_attn.q_proj.weight shape: [16384, 16384]
145: model.layers.111.self_attn.v_proj.weight shape: [1024, 16384]
146: model.layers.112.input_layernorm.weight shape: [16384]
147: model.layers.112.mlp.down_proj.weight shape: [16384, 53248]
148: model.layers.112.mlp.gate_proj.weight shape: [53248, 16384]
149: model.layers.112.mlp.up_proj.weight shape: [53248, 16384]
150: model.layers.112.post_attention_layernorm.weight shape: [16384]
151: model.layers.112.self_attn.k_proj.weight shape: [1024, 16384]
152: model.layers.112.self_attn.o_proj.weight shape: [16384, 16384]
153: model.layers.112.self_attn.q_proj.weight shape: [16384, 16384]
154: model.layers.112.self_attn.v_proj.weight shape: [1024, 16384]
155: model.layers.113.input_layernorm.weight shape: [16384]
156: model.layers.113.mlp.down_proj.weight shape: [16384, 53248]
157: model.layers.113.mlp.gate_proj.weight shape: [53248, 16384]
158: model.layers.113.mlp.up_proj.weight shape: [53248, 16384]
159: model.layers.113.post_attention_layernorm.weight shape: [16384]
160: model.layers.113.self_attn.k_proj.weight shape: [1024, 16384]
161: model.layers.113.self_attn.o_proj.weight shape: [16384, 16384]
162: model.layers.113.self_attn.q_proj.weight shape: [16384, 16384]
163: model.layers.113.self_attn.v_proj.weight shape: [1024, 16384]
164: model.layers.114.input_layernorm.weight shape: [16384]
165: model.layers.114.mlp.down_proj.weight shape: [16384, 53248]
166: model.layers.114.mlp.gate_proj.weight shape: [53248, 16384]
167: model.layers.114.mlp.up_proj.weight shape: [53248, 16384]
168: model.layers.114.post_attention_layernorm.weight shape: [16384]
169: model.layers.114.self_attn.k_proj.weight shape: [1024, 16384]
170: model.layers.114.self_attn.o_proj.weight shape: [16384, 16384]
171: model.layers.114.self_attn.q_proj.weight shape: [16384, 16384]
172: model.layers.114.self_attn.v_proj.weight shape: [1024, 16384]
173: model.layers.115.input_layernorm.weight shape: [16384]
174: model.layers.115.mlp.down_proj.weight shape: [16384, 53248]
175: model.layers.115.mlp.gate_proj.weight shape: [53248, 16384]
176: model.layers.115.mlp.up_proj.weight shape: [53248, 16384]
177: model.layers.115.post_attention_layernorm.weight shape: [16384]
178: model.layers.115.self_attn.k_proj.weight shape: [1024, 16384]
179: model.layers.115.self_attn.o_proj.weight shape: [16384, 16384]
180: model.layers.115.self_attn.q_proj.weight shape: [16384, 16384]
181: model.layers.115.self_attn.v_proj.weight shape: [1024, 16384]
182: model.layers.116.input_layernorm.weight shape: [16384]
183: model.layers.116.mlp.down_proj.weight shape: [16384, 53248]
184: model.layers.116.mlp.gate_proj.weight shape: [53248, 16384]
185: model.layers.116.mlp.up_proj.weight shape: [53248, 16384]
186: model.layers.116.post_attention_layernorm.weight shape: [16384]
187: model.layers.116.self_attn.k_proj.weight shape: [1024, 16384]
188: model.layers.116.self_attn.o_proj.weight shape: [16384, 16384]
189: model.layers.116.self_attn.q_proj.weight shape: [16384, 16384]
190: model.layers.116.self_attn.v_proj.weight shape: [1024, 16384]
191: model.layers.117.input_layernorm.weight shape: [16384]
192: model.layers.117.mlp.down_proj.weight shape: [16384, 53248]
193: model.layers.117.mlp.gate_proj.weight shape: [53248, 16384]
194: model.layers.117.mlp.up_proj.weight shape: [53248, 16384]
195: model.layers.117.post_attention_layernorm.weight shape: [16384]
196: model.layers.117.self_attn.k_proj.weight shape: [1024, 16384]
197: model.layers.117.self_attn.o_proj.weight shape: [16384, 16384]
198: model.layers.117.self_attn.q_proj.weight shape: [16384, 16384]
199: model.layers.117.self_attn.v_proj.weight shape: [1024, 16384]
200: model.layers.118.input_layernorm.weight shape: [16384]
201: model.layers.118.mlp.down_proj.weight shape: [16384, 53248]
202: model.layers.118.mlp.gate_proj.weight shape: [53248, 16384]
203: model.layers.118.mlp.up_proj.weight shape: [53248, 16384]
204: model.layers.118.post_attention_layernorm.weight shape: [16384]
205: model.layers.118.self_attn.k_proj.weight shape: [1024, 16384]
206: model.layers.118.self_attn.o_proj.weight shape: [16384, 16384]
207: model.layers.118.self_attn.q_proj.weight shape: [16384, 16384]
208: model.layers.118.self_attn.v_proj.weight shape: [1024, 16384]
209: model.layers.119.input_layernorm.weight shape: [16384]
210: model.layers.119.mlp.down_proj.weight shape: [16384, 53248]
211: model.layers.119.mlp.gate_proj.weight shape: [53248, 16384]
212: model.layers.119.mlp.up_proj.weight shape: [53248, 16384]
213: model.layers.119.post_attention_layernorm.weight shape: [16384]
214: model.layers.119.self_attn.k_proj.weight shape: [1024, 16384]
215: model.layers.119.self_attn.o_proj.weight shape: [16384, 16384]
216: model.layers.119.self_attn.q_proj.weight shape: [16384, 16384]
217: model.layers.119.self_attn.v_proj.weight shape: [1024, 16384]
218: model.layers.12.input_layernorm.weight shape: [16384]
219: model.layers.12.mlp.down_proj.weight shape: [16384, 53248]
220: model.layers.12.mlp.gate_proj.weight shape: [53248, 16384]
221: model.layers.12.mlp.up_proj.weight shape: [53248, 16384]
222: model.layers.12.post_attention_layernorm.weight shape: [16384]
223: model.layers.12.self_attn.k_proj.weight shape: [1024, 16384]
224: model.layers.12.self_attn.o_proj.weight shape: [16384, 16384]
225: model.layers.12.self_attn.q_proj.weight shape: [16384, 16384]
226: model.layers.12.self_attn.v_proj.weight shape: [1024, 16384]
227: model.layers.120.input_layernorm.weight shape: [16384]
228: model.layers.120.mlp.down_proj.weight shape: [16384, 53248]
229: model.layers.120.mlp.gate_proj.weight shape: [53248, 16384]
230: model.layers.120.mlp.up_proj.weight shape: [53248, 16384]
231: model.layers.120.post_attention_layernorm.weight shape: [16384]
232: model.layers.120.self_attn.k_proj.weight shape: [1024, 16384]
233: model.layers.120.self_attn.o_proj.weight shape: [16384, 16384]
234: model.layers.120.self_attn.q_proj.weight shape: [16384, 16384]
235: model.layers.120.self_attn.v_proj.weight shape: [1024, 16384]
236: model.layers.121.input_layernorm.weight shape: [16384]
237: model.layers.121.mlp.down_proj.weight shape: [16384, 53248]
238: model.layers.121.mlp.gate_proj.weight shape: [53248, 16384]
239: model.layers.121.mlp.up_proj.weight shape: [53248, 16384]
240: model.layers.121.post_attention_layernorm.weight shape: [16384]
241: model.layers.121.self_attn.k_proj.weight shape: [1024, 16384]
242: model.layers.121.self_attn.o_proj.weight shape: [16384, 16384]
243: model.layers.121.self_attn.q_proj.weight shape: [16384, 16384]
244: model.layers.121.self_attn.v_proj.weight shape: [1024, 16384]
245: model.layers.122.input_layernorm.weight shape: [16384]
246: model.layers.122.mlp.down_proj.weight shape: [16384, 53248]
247: model.layers.122.mlp.gate_proj.weight shape: [53248, 16384]
248: model.layers.122.mlp.up_proj.weight shape: [53248, 16384]
249: model.layers.122.post_attention_layernorm.weight shape: [16384]
250: model.layers.122.self_attn.k_proj.weight shape: [1024, 16384]
251: model.layers.122.self_attn.o_proj.weight shape: [16384, 16384]
252: model.layers.122.self_attn.q_proj.weight shape: [16384, 16384]
253: model.layers.122.self_attn.v_proj.weight shape: [1024, 16384]
254: model.layers.123.input_layernorm.weight shape: [16384]
255: model.layers.123.mlp.down_proj.weight shape: [16384, 53248]
256: model.layers.123.mlp.gate_proj.weight shape: [53248, 16384]
257: model.layers.123.mlp.up_proj.weight shape: [53248, 16384]
258: model.layers.123.post_attention_layernorm.weight shape: [16384]
259: model.layers.123.self_attn.k_proj.weight shape: [1024, 16384]
260: model.layers.123.self_attn.o_proj.weight shape: [16384, 16384]
261: model.layers.123.self_attn.q_proj.weight shape: [16384, 16384]
262: model.layers.123.self_attn.v_proj.weight shape: [1024, 16384]
263: model.layers.124.input_layernorm.weight shape: [16384]
264: model.layers.124.mlp.down_proj.weight shape: [16384, 53248]
265: model.layers.124.mlp.gate_proj.weight shape: [53248, 16384]
266: model.layers.124.mlp.up_proj.weight shape: [53248, 16384]
267: model.layers.124.post_attention_layernorm.weight shape: [16384]
268: model.layers.124.self_attn.k_proj.weight shape: [1024, 16384]
269: model.layers.124.self_attn.o_proj.weight shape: [16384, 16384]
270: model.layers.124.self_attn.q_proj.weight shape: [16384, 16384]
271: model.layers.124.self_attn.v_proj.weight shape: [1024, 16384]
272: model.layers.125.input_layernorm.weight shape: [16384]
273: model.layers.125.mlp.down_proj.weight shape: [16384, 53248]
274: model.layers.125.mlp.gate_proj.weight shape: [53248, 16384]
275: model.layers.125.mlp.up_proj.weight shape: [53248, 16384]
276: model.layers.125.post_attention_layernorm.weight shape: [16384]
277: model.layers.125.self_attn.k_proj.weight shape: [1024, 16384]
278: model.layers.125.self_attn.o_proj.weight shape: [16384, 16384]
279: model.layers.125.self_attn.q_proj.weight shape: [16384, 16384]
280: model.layers.125.self_attn.v_proj.weight shape: [1024, 16384]
281: model.layers.13.input_layernorm.weight shape: [16384]
282: model.layers.13.mlp.down_proj.weight shape: [16384, 53248]
283: model.layers.13.mlp.gate_proj.weight shape: [53248, 16384]
284: model.layers.13.mlp.up_proj.weight shape: [53248, 16384]
285: model.layers.13.post_attention_layernorm.weight shape: [16384]
286: model.layers.13.self_attn.k_proj.weight shape: [1024, 16384]
287: model.layers.13.self_attn.o_proj.weight shape: [16384, 16384]
288: model.layers.13.self_attn.q_proj.weight shape: [16384, 16384]
289: model.layers.13.self_attn.v_proj.weight shape: [1024, 16384]
290: model.layers.14.input_layernorm.weight shape: [16384]
291: model.layers.14.mlp.down_proj.weight shape: [16384, 53248]
292: model.layers.14.mlp.gate_proj.weight shape: [53248, 16384]
293: model.layers.14.mlp.up_proj.weight shape: [53248, 16384]
294: model.layers.14.post_attention_layernorm.weight shape: [16384]
295: model.layers.14.self_attn.k_proj.weight shape: [1024, 16384]
296: model.layers.14.self_attn.o_proj.weight shape: [16384, 16384]
297: model.layers.14.self_attn.q_proj.weight shape: [16384, 16384]
298: model.layers.14.self_attn.v_proj.weight shape: [1024, 16384]
299: model.layers.15.input_layernorm.weight shape: [16384]
300: model.layers.15.mlp.down_proj.weight shape: [16384, 53248]
301: model.layers.15.mlp.gate_proj.weight shape: [53248, 16384]
302: model.layers.15.mlp.up_proj.weight shape: [53248, 16384]
303: model.layers.15.post_attention_layernorm.weight shape: [16384]
304: model.layers.15.self_attn.k_proj.weight shape: [1024, 16384]
305: model.layers.15.self_attn.o_proj.weight shape: [16384, 16384]
306: model.layers.15.self_attn.q_proj.weight shape: [16384, 16384]
307: model.layers.15.self_attn.v_proj.weight shape: [1024, 16384]
308: model.layers.16.input_layernorm.weight shape: [16384]
309: model.layers.16.mlp.down_proj.weight shape: [16384, 53248]
310: model.layers.16.mlp.gate_proj.weight shape: [53248, 16384]
311: model.layers.16.mlp.up_proj.weight shape: [53248, 16384]
312: model.layers.16.post_attention_layernorm.weight shape: [16384]
313: model.layers.16.self_attn.k_proj.weight shape: [1024, 16384]
314: model.layers.16.self_attn.o_proj.weight shape: [16384, 16384]
315: model.layers.16.self_attn.q_proj.weight shape: [16384, 16384]
316: model.layers.16.self_attn.v_proj.weight shape: [1024, 16384]
317: model.layers.17.input_layernorm.weight shape: [16384]
318: model.layers.17.mlp.down_proj.weight shape: [16384, 53248]
319: model.layers.17.mlp.gate_proj.weight shape: [53248, 16384]
320: model.layers.17.mlp.up_proj.weight shape: [53248, 16384]
321: model.layers.17.post_attention_layernorm.weight shape: [16384]
322: model.layers.17.self_attn.k_proj.weight shape: [1024, 16384]
323: model.layers.17.self_attn.o_proj.weight shape: [16384, 16384]
324: model.layers.17.self_attn.q_proj.weight shape: [16384, 16384]
325: model.layers.17.self_attn.v_proj.weight shape: [1024, 16384]
326: model.layers.18.input_layernorm.weight shape: [16384]
327: model.layers.18.mlp.down_proj.weight shape: [16384, 53248]
328: model.layers.18.mlp.gate_proj.weight shape: [53248, 16384]
329: model.layers.18.mlp.up_proj.weight shape: [53248, 16384]
330: model.layers.18.post_attention_layernorm.weight shape: [16384]
331: model.layers.18.self_attn.k_proj.weight shape: [1024, 16384]
332: model.layers.18.self_attn.o_proj.weight shape: [16384, 16384]
333: model.layers.18.self_attn.q_proj.weight shape: [16384, 16384]
334: model.layers.18.self_attn.v_proj.weight shape: [1024, 16384]
335: model.layers.19.input_layernorm.weight shape: [16384]
336: model.layers.19.mlp.down_proj.weight shape: [16384, 53248]
337: model.layers.19.mlp.gate_proj.weight shape: [53248, 16384]
338: model.layers.19.mlp.up_proj.weight shape: [53248, 16384]
339: model.layers.19.post_attention_layernorm.weight shape: [16384]
340: model.layers.19.self_attn.k_proj.weight shape: [1024, 16384]
341: model.layers.19.self_attn.o_proj.weight shape: [16384, 16384]
342: model.layers.19.self_attn.q_proj.weight shape: [16384, 16384]
343: model.layers.19.self_attn.v_proj.weight shape: [1024, 16384]
344: model.layers.2.input_layernorm.weight shape: [16384]
345: model.layers.2.mlp.down_proj.weight shape: [16384, 53248]
346: model.layers.2.mlp.gate_proj.weight shape: [53248, 16384]
347: model.layers.2.mlp.up_proj.weight shape: [53248, 16384]
348: model.layers.2.post_attention_layernorm.weight shape: [16384]
349: model.layers.2.self_attn.k_proj.weight shape: [1024, 16384]
350: model.layers.2.self_attn.o_proj.weight shape: [16384, 16384]
351: model.layers.2.self_attn.q_proj.weight shape: [16384, 16384]
352: model.layers.2.self_attn.v_proj.weight shape: [1024, 16384]
353: model.layers.20.input_layernorm.weight shape: [16384]
354: model.layers.20.mlp.down_proj.weight shape: [16384, 53248]
355: model.layers.20.mlp.gate_proj.weight shape: [53248, 16384]
356: model.layers.20.mlp.up_proj.weight shape: [53248, 16384]
357: model.layers.20.post_attention_layernorm.weight shape: [16384]
358: model.layers.20.self_attn.k_proj.weight shape: [1024, 16384]
359: model.layers.20.self_attn.o_proj.weight shape: [16384, 16384]
360: model.layers.20.self_attn.q_proj.weight shape: [16384, 16384]
361: model.layers.20.self_attn.v_proj.weight shape: [1024, 16384]
362: model.layers.21.input_layernorm.weight shape: [16384]
363: model.layers.21.mlp.down_proj.weight shape: [16384, 53248]
364: model.layers.21.mlp.gate_proj.weight shape: [53248, 16384]
365: model.layers.21.mlp.up_proj.weight shape: [53248, 16384]
366: model.layers.21.post_attention_layernorm.weight shape: [16384]
367: model.layers.21.self_attn.k_proj.weight shape: [1024, 16384]
368: model.layers.21.self_attn.o_proj.weight shape: [16384, 16384]
369: model.layers.21.self_attn.q_proj.weight shape: [16384, 16384]
370: model.layers.21.self_attn.v_proj.weight shape: [1024, 16384]
371: model.layers.22.input_layernorm.weight shape: [16384]
372: model.layers.22.mlp.down_proj.weight shape: [16384, 53248]
373: model.layers.22.mlp.gate_proj.weight shape: [53248, 16384]
374: model.layers.22.mlp.up_proj.weight shape: [53248, 16384]
375: model.layers.22.post_attention_layernorm.weight shape: [16384]
376: model.layers.22.self_attn.k_proj.weight shape: [1024, 16384]
377: model.layers.22.self_attn.o_proj.weight shape: [16384, 16384]
378: model.layers.22.self_attn.q_proj.weight shape: [16384, 16384]
379: model.layers.22.self_attn.v_proj.weight shape: [1024, 16384]
380: model.layers.23.input_layernorm.weight shape: [16384]
381: model.layers.23.mlp.down_proj.weight shape: [16384, 53248]
382: model.layers.23.mlp.gate_proj.weight shape: [53248, 16384]
383: model.layers.23.mlp.up_proj.weight shape: [53248, 16384]
384: model.layers.23.post_attention_layernorm.weight shape: [16384]
385: model.layers.23.self_attn.k_proj.weight shape: [1024, 16384]
386: model.layers.23.self_attn.o_proj.weight shape: [16384, 16384]
387: model.layers.23.self_attn.q_proj.weight shape: [16384, 16384]
388: model.layers.23.self_attn.v_proj.weight shape: [1024, 16384]
389: model.layers.24.input_layernorm.weight shape: [16384]
390: model.layers.24.mlp.down_proj.weight shape: [16384, 53248]
391: model.layers.24.mlp.gate_proj.weight shape: [53248, 16384]
392: model.layers.24.mlp.up_proj.weight shape: [53248, 16384]
393: model.layers.24.post_attention_layernorm.weight shape: [16384]
394: model.layers.24.self_attn.k_proj.weight shape: [1024, 16384]
395: model.layers.24.self_attn.o_proj.weight shape: [16384, 16384]
396: model.layers.24.self_attn.q_proj.weight shape: [16384, 16384]
397: model.layers.24.self_attn.v_proj.weight shape: [1024, 16384]
398: model.layers.25.input_layernorm.weight shape: [16384]
399: model.layers.25.mlp.down_proj.weight shape: [16384, 53248]
400: model.layers.25.mlp.gate_proj.weight shape: [53248, 16384]
401: model.layers.25.mlp.up_proj.weight shape: [53248, 16384]
402: model.layers.25.post_attention_layernorm.weight shape: [16384]
403: model.layers.25.self_attn.k_proj.weight shape: [1024, 16384]
404: model.layers.25.self_attn.o_proj.weight shape: [16384, 16384]
405: model.layers.25.self_attn.q_proj.weight shape: [16384, 16384]
406: model.layers.25.self_attn.v_proj.weight shape: [1024, 16384]
407: model.layers.26.input_layernorm.weight shape: [16384]
408: model.layers.26.mlp.down_proj.weight shape: [16384, 53248]
409: model.layers.26.mlp.gate_proj.weight shape: [53248, 16384]
410: model.layers.26.mlp.up_proj.weight shape: [53248, 16384]
411: model.layers.26.post_attention_layernorm.weight shape: [16384]
412: model.layers.26.self_attn.k_proj.weight shape: [1024, 16384]
413: model.layers.26.self_attn.o_proj.weight shape: [16384, 16384]
414: model.layers.26.self_attn.q_proj.weight shape: [16384, 16384]
415: model.layers.26.self_attn.v_proj.weight shape: [1024, 16384]
416: model.layers.27.input_layernorm.weight shape: [16384]
417: model.layers.27.mlp.down_proj.weight shape: [16384, 53248]
418: model.layers.27.mlp.gate_proj.weight shape: [53248, 16384]
419: model.layers.27.mlp.up_proj.weight shape: [53248, 16384]
420: model.layers.27.post_attention_layernorm.weight shape: [16384]
421: model.layers.27.self_attn.k_proj.weight shape: [1024, 16384]
422: model.layers.27.self_attn.o_proj.weight shape: [16384, 16384]
423: model.layers.27.self_attn.q_proj.weight shape: [16384, 16384]
424: model.layers.27.self_attn.v_proj.weight shape: [1024, 16384]
425: model.layers.28.input_layernorm.weight shape: [16384]
426: model.layers.28.mlp.down_proj.weight shape: [16384, 53248]
427: model.layers.28.mlp.gate_proj.weight shape: [53248, 16384]
428: model.layers.28.mlp.up_proj.weight shape: [53248, 16384]
429: model.layers.28.post_attention_layernorm.weight shape: [16384]
430: model.layers.28.self_attn.k_proj.weight shape: [1024, 16384]
431: model.layers.28.self_attn.o_proj.weight shape: [16384, 16384]
432: model.layers.28.self_attn.q_proj.weight shape: [16384, 16384]
433: model.layers.28.self_attn.v_proj.weight shape: [1024, 16384]
434: model.layers.29.input_layernorm.weight shape: [16384]
435: model.layers.29.mlp.down_proj.weight shape: [16384, 53248]
436: model.layers.29.mlp.gate_proj.weight shape: [53248, 16384]
437: model.layers.29.mlp.up_proj.weight shape: [53248, 16384]
438: model.layers.29.post_attention_layernorm.weight shape: [16384]
439: model.layers.29.self_attn.k_proj.weight shape: [1024, 16384]
440: model.layers.29.self_attn.o_proj.weight shape: [16384, 16384]
441: model.layers.29.self_attn.q_proj.weight shape: [16384, 16384]
442: model.layers.29.self_attn.v_proj.weight shape: [1024, 16384]
443: model.layers.3.input_layernorm.weight shape: [16384]
444: model.layers.3.mlp.down_proj.weight shape: [16384, 53248]
445: model.layers.3.mlp.gate_proj.weight shape: [53248, 16384]
446: model.layers.3.mlp.up_proj.weight shape: [53248, 16384]
447: model.layers.3.post_attention_layernorm.weight shape: [16384]
448: model.layers.3.self_attn.k_proj.weight shape: [1024, 16384]
449: model.layers.3.self_attn.o_proj.weight shape: [16384, 16384]
450: model.layers.3.self_attn.q_proj.weight shape: [16384, 16384]
451: model.layers.3.self_attn.v_proj.weight shape: [1024, 16384]
452: model.layers.30.input_layernorm.weight shape: [16384]
453: model.layers.30.mlp.down_proj.weight shape: [16384, 53248]
454: model.layers.30.mlp.gate_proj.weight shape: [53248, 16384]
455: model.layers.30.mlp.up_proj.weight shape: [53248, 16384]
456: model.layers.30.post_attention_layernorm.weight shape: [16384]
457: model.layers.30.self_attn.k_proj.weight shape: [1024, 16384]
458: model.layers.30.self_attn.o_proj.weight shape: [16384, 16384]
459: model.layers.30.self_attn.q_proj.weight shape: [16384, 16384]
460: model.layers.30.self_attn.v_proj.weight shape: [1024, 16384]
461: model.layers.31.input_layernorm.weight shape: [16384]
462: model.layers.31.mlp.down_proj.weight shape: [16384, 53248]
463: model.layers.31.mlp.gate_proj.weight shape: [53248, 16384]
464: model.layers.31.mlp.up_proj.weight shape: [53248, 16384]
465: model.layers.31.post_attention_layernorm.weight shape: [16384]
466: model.layers.31.self_attn.k_proj.weight shape: [1024, 16384]
467: model.layers.31.self_attn.o_proj.weight shape: [16384, 16384]
468: model.layers.31.self_attn.q_proj.weight shape: [16384, 16384]
469: model.layers.31.self_attn.v_proj.weight shape: [1024, 16384]
470: model.layers.32.input_layernorm.weight shape: [16384]
471: model.layers.32.mlp.down_proj.weight shape: [16384, 53248]
472: model.layers.32.mlp.gate_proj.weight shape: [53248, 16384]
473: model.layers.32.mlp.up_proj.weight shape: [53248, 16384]
474: model.layers.32.post_attention_layernorm.weight shape: [16384]
475: model.layers.32.self_attn.k_proj.weight shape: [1024, 16384]
476: model.layers.32.self_attn.o_proj.weight shape: [16384, 16384]
477: model.layers.32.self_attn.q_proj.weight shape: [16384, 16384]
478: model.layers.32.self_attn.v_proj.weight shape: [1024, 16384]
479: model.layers.33.input_layernorm.weight shape: [16384]
480: model.layers.33.mlp.down_proj.weight shape: [16384, 53248]
481: model.layers.33.mlp.gate_proj.weight shape: [53248, 16384]
482: model.layers.33.mlp.up_proj.weight shape: [53248, 16384]
483: model.layers.33.post_attention_layernorm.weight shape: [16384]
484: model.layers.33.self_attn.k_proj.weight shape: [1024, 16384]
485: model.layers.33.self_attn.o_proj.weight shape: [16384, 16384]
486: model.layers.33.self_attn.q_proj.weight shape: [16384, 16384]
487: model.layers.33.self_attn.v_proj.weight shape: [1024, 16384]
488: model.layers.34.input_layernorm.weight shape: [16384]
489: model.layers.34.mlp.down_proj.weight shape: [16384, 53248]
490: model.layers.34.mlp.gate_proj.weight shape: [53248, 16384]
491: model.layers.34.mlp.up_proj.weight shape: [53248, 16384]
492: model.layers.34.post_attention_layernorm.weight shape: [16384]
493: model.layers.34.self_attn.k_proj.weight shape: [1024, 16384]
494: model.layers.34.self_attn.o_proj.weight shape: [16384, 16384]
495: model.layers.34.self_attn.q_proj.weight shape: [16384, 16384]
496: model.layers.34.self_attn.v_proj.weight shape: [1024, 16384]
497: model.layers.35.input_layernorm.weight shape: [16384]
498: model.layers.35.mlp.down_proj.weight shape: [16384, 53248]
499: model.layers.35.mlp.gate_proj.weight shape: [53248, 16384]
500: model.layers.35.mlp.up_proj.weight shape: [53248, 16384]
501: model.layers.35.post_attention_layernorm.weight shape: [16384]
502: model.layers.35.self_attn.k_proj.weight shape: [1024, 16384]
503: model.layers.35.self_attn.o_proj.weight shape: [16384, 16384]
504: model.layers.35.self_attn.q_proj.weight shape: [16384, 16384]
505: model.layers.35.self_attn.v_proj.weight shape: [1024, 16384]
506: model.layers.36.input_layernorm.weight shape: [16384]
507: model.layers.36.mlp.down_proj.weight shape: [16384, 53248]
508: model.layers.36.mlp.gate_proj.weight shape: [53248, 16384]
509: model.layers.36.mlp.up_proj.weight shape: [53248, 16384]
510: model.layers.36.post_attention_layernorm.weight shape: [16384]
511: model.layers.36.self_attn.k_proj.weight shape: [1024, 16384]
512: model.layers.36.self_attn.o_proj.weight shape: [16384, 16384]
513: model.layers.36.self_attn.q_proj.weight shape: [16384, 16384]
514: model.layers.36.self_attn.v_proj.weight shape: [1024, 16384]
515: model.layers.37.input_layernorm.weight shape: [16384]
516: model.layers.37.mlp.down_proj.weight shape: [16384, 53248]
517: model.layers.37.mlp.gate_proj.weight shape: [53248, 16384]
518: model.layers.37.mlp.up_proj.weight shape: [53248, 16384]
519: model.layers.37.post_attention_layernorm.weight shape: [16384]
520: model.layers.37.self_attn.k_proj.weight shape: [1024, 16384]
521: model.layers.37.self_attn.o_proj.weight shape: [16384, 16384]
522: model.layers.37.self_attn.q_proj.weight shape: [16384, 16384]
523: model.layers.37.self_attn.v_proj.weight shape: [1024, 16384]
524: model.layers.38.input_layernorm.weight shape: [16384]
525: model.layers.38.mlp.down_proj.weight shape: [16384, 53248]
526: model.layers.38.mlp.gate_proj.weight shape: [53248, 16384]
527: model.layers.38.mlp.up_proj.weight shape: [53248, 16384]
528: model.layers.38.post_attention_layernorm.weight shape: [16384]
529: model.layers.38.self_attn.k_proj.weight shape: [1024, 16384]
530: model.layers.38.self_attn.o_proj.weight shape: [16384, 16384]
531: model.layers.38.self_attn.q_proj.weight shape: [16384, 16384]
532: model.layers.38.self_attn.v_proj.weight shape: [1024, 16384]
533: model.layers.39.input_layernorm.weight shape: [16384]
534: model.layers.39.mlp.down_proj.weight shape: [16384, 53248]
535: model.layers.39.mlp.gate_proj.weight shape: [53248, 16384]
536: model.layers.39.mlp.up_proj.weight shape: [53248, 16384]
537: model.layers.39.post_attention_layernorm.weight shape: [16384]
538: model.layers.39.self_attn.k_proj.weight shape: [1024, 16384]
539: model.layers.39.self_attn.o_proj.weight shape: [16384, 16384]
540: model.layers.39.self_attn.q_proj.weight shape: [16384, 16384]
541: model.layers.39.self_attn.v_proj.weight shape: [1024, 16384]
542: model.layers.4.input_layernorm.weight shape: [16384]
543: model.layers.4.mlp.down_proj.weight shape: [16384, 53248]
544: model.layers.4.mlp.gate_proj.weight shape: [53248, 16384]
545: model.layers.4.mlp.up_proj.weight shape: [53248, 16384]
546: model.layers.4.post_attention_layernorm.weight shape: [16384]
547: model.layers.4.self_attn.k_proj.weight shape: [1024, 16384]
548: model.layers.4.self_attn.o_proj.weight shape: [16384, 16384]
549: model.layers.4.self_attn.q_proj.weight shape: [16384, 16384]
550: model.layers.4.self_attn.v_proj.weight shape: [1024, 16384]
551: model.layers.40.input_layernorm.weight shape: [16384]
552: model.layers.40.mlp.down_proj.weight shape: [16384, 53248]
553: model.layers.40.mlp.gate_proj.weight shape: [53248, 16384]
554: model.layers.40.mlp.up_proj.weight shape: [53248, 16384]
555: model.layers.40.post_attention_layernorm.weight shape: [16384]
556: model.layers.40.self_attn.k_proj.weight shape: [1024, 16384]
557: model.layers.40.self_attn.o_proj.weight shape: [16384, 16384]
558: model.layers.40.self_attn.q_proj.weight shape: [16384, 16384]
559: model.layers.40.self_attn.v_proj.weight shape: [1024, 16384]
560: model.layers.41.input_layernorm.weight shape: [16384]
561: model.layers.41.mlp.down_proj.weight shape: [16384, 53248]
562: model.layers.41.mlp.gate_proj.weight shape: [53248, 16384]
563: model.layers.41.mlp.up_proj.weight shape: [53248, 16384]
564: model.layers.41.post_attention_layernorm.weight shape: [16384]
565: model.layers.41.self_attn.k_proj.weight shape: [1024, 16384]
566: model.layers.41.self_attn.o_proj.weight shape: [16384, 16384]
567: model.layers.41.self_attn.q_proj.weight shape: [16384, 16384]
568: model.layers.41.self_attn.v_proj.weight shape: [1024, 16384]
569: model.layers.42.input_layernorm.weight shape: [16384]
570: model.layers.42.mlp.down_proj.weight shape: [16384, 53248]
571: model.layers.42.mlp.gate_proj.weight shape: [53248, 16384]
572: model.layers.42.mlp.up_proj.weight shape: [53248, 16384]
573: model.layers.42.post_attention_layernorm.weight shape: [16384]
574: model.layers.42.self_attn.k_proj.weight shape: [1024, 16384]
575: model.layers.42.self_attn.o_proj.weight shape: [16384, 16384]
576: model.layers.42.self_attn.q_proj.weight shape: [16384, 16384]
577: model.layers.42.self_attn.v_proj.weight shape: [1024, 16384]
578: model.layers.43.input_layernorm.weight shape: [16384]
579: model.layers.43.mlp.down_proj.weight shape: [16384, 53248]
580: model.layers.43.mlp.gate_proj.weight shape: [53248, 16384]
581: model.layers.43.mlp.up_proj.weight shape: [53248, 16384]
582: model.layers.43.post_attention_layernorm.weight shape: [16384]
583: model.layers.43.self_attn.k_proj.weight shape: [1024, 16384]
584: model.layers.43.self_attn.o_proj.weight shape: [16384, 16384]
585: model.layers.43.self_attn.q_proj.weight shape: [16384, 16384]
586: model.layers.43.self_attn.v_proj.weight shape: [1024, 16384]
587: model.layers.44.input_layernorm.weight shape: [16384]
588: model.layers.44.mlp.down_proj.weight shape: [16384, 53248]
589: model.layers.44.mlp.gate_proj.weight shape: [53248, 16384]
590: model.layers.44.mlp.up_proj.weight shape: [53248, 16384]
591: model.layers.44.post_attention_layernorm.weight shape: [16384]
592: model.layers.44.self_attn.k_proj.weight shape: [1024, 16384]
593: model.layers.44.self_attn.o_proj.weight shape: [16384, 16384]
594: model.layers.44.self_attn.q_proj.weight shape: [16384, 16384]
595: model.layers.44.self_attn.v_proj.weight shape: [1024, 16384]
596: model.layers.45.input_layernorm.weight shape: [16384]
597: model.layers.45.mlp.down_proj.weight shape: [16384, 53248]
598: model.layers.45.mlp.gate_proj.weight shape: [53248, 16384]
599: model.layers.45.mlp.up_proj.weight shape: [53248, 16384]
600: model.layers.45.post_attention_layernorm.weight shape: [16384]
601: model.layers.45.self_attn.k_proj.weight shape: [1024, 16384]
602: model.layers.45.self_attn.o_proj.weight shape: [16384, 16384]
603: model.layers.45.self_attn.q_proj.weight shape: [16384, 16384]
604: model.layers.45.self_attn.v_proj.weight shape: [1024, 16384]
605: model.layers.46.input_layernorm.weight shape: [16384]
606: model.layers.46.mlp.down_proj.weight shape: [16384, 53248]
607: model.layers.46.mlp.gate_proj.weight shape: [53248, 16384]
608: model.layers.46.mlp.up_proj.weight shape: [53248, 16384]
609: model.layers.46.post_attention_layernorm.weight shape: [16384]
610: model.layers.46.self_attn.k_proj.weight shape: [1024, 16384]
611: model.layers.46.self_attn.o_proj.weight shape: [16384, 16384]
612: model.layers.46.self_attn.q_proj.weight shape: [16384, 16384]
613: model.layers.46.self_attn.v_proj.weight shape: [1024, 16384]
614: model.layers.47.input_layernorm.weight shape: [16384]
615: model.layers.47.mlp.down_proj.weight shape: [16384, 53248]
616: model.layers.47.mlp.gate_proj.weight shape: [53248, 16384]
617: model.layers.47.mlp.up_proj.weight shape: [53248, 16384]
618: model.layers.47.post_attention_layernorm.weight shape: [16384]
619: model.layers.47.self_attn.k_proj.weight shape: [1024, 16384]
620: model.layers.47.self_attn.o_proj.weight shape: [16384, 16384]
621: model.layers.47.self_attn.q_proj.weight shape: [16384, 16384]
622: model.layers.47.self_attn.v_proj.weight shape: [1024, 16384]
623: model.layers.48.input_layernorm.weight shape: [16384]
624: model.layers.48.mlp.down_proj.weight shape: [16384, 53248]
625: model.layers.48.mlp.gate_proj.weight shape: [53248, 16384]
626: model.layers.48.mlp.up_proj.weight shape: [53248, 16384]
627: model.layers.48.post_attention_layernorm.weight shape: [16384]
628: model.layers.48.self_attn.k_proj.weight shape: [1024, 16384]
629: model.layers.48.self_attn.o_proj.weight shape: [16384, 16384]
630: model.layers.48.self_attn.q_proj.weight shape: [16384, 16384]
631: model.layers.48.self_attn.v_proj.weight shape: [1024, 16384]
632: model.layers.49.input_layernorm.weight shape: [16384]
633: model.layers.49.mlp.down_proj.weight shape: [16384, 53248]
634: model.layers.49.mlp.gate_proj.weight shape: [53248, 16384]
635: model.layers.49.mlp.up_proj.weight shape: [53248, 16384]
636: model.layers.49.post_attention_layernorm.weight shape: [16384]
637: model.layers.49.self_attn.k_proj.weight shape: [1024, 16384]
638: model.layers.49.self_attn.o_proj.weight shape: [16384, 16384]
639: model.layers.49.self_attn.q_proj.weight shape: [16384, 16384]
640: model.layers.49.self_attn.v_proj.weight shape: [1024, 16384]
641: model.layers.5.input_layernorm.weight shape: [16384]
642: model.layers.5.mlp.down_proj.weight shape: [16384, 53248]
643: model.layers.5.mlp.gate_proj.weight shape: [53248, 16384]
644: model.layers.5.mlp.up_proj.weight shape: [53248, 16384]
645: model.layers.5.post_attention_layernorm.weight shape: [16384]
646: model.layers.5.self_attn.k_proj.weight shape: [1024, 16384]
647: model.layers.5.self_attn.o_proj.weight shape: [16384, 16384]
648: model.layers.5.self_attn.q_proj.weight shape: [16384, 16384]
649: model.layers.5.self_attn.v_proj.weight shape: [1024, 16384]
650: model.layers.50.input_layernorm.weight shape: [16384]
651: model.layers.50.mlp.down_proj.weight shape: [16384, 53248]
652: model.layers.50.mlp.gate_proj.weight shape: [53248, 16384]
653: model.layers.50.mlp.up_proj.weight shape: [53248, 16384]
654: model.layers.50.post_attention_layernorm.weight shape: [16384]
655: model.layers.50.self_attn.k_proj.weight shape: [1024, 16384]
656: model.layers.50.self_attn.o_proj.weight shape: [16384, 16384]
657: model.layers.50.self_attn.q_proj.weight shape: [16384, 16384]
658: model.layers.50.self_attn.v_proj.weight shape: [1024, 16384]
659: model.layers.51.input_layernorm.weight shape: [16384]
660: model.layers.51.mlp.down_proj.weight shape: [16384, 53248]
661: model.layers.51.mlp.gate_proj.weight shape: [53248, 16384]
662: model.layers.51.mlp.up_proj.weight shape: [53248, 16384]
663: model.layers.51.post_attention_layernorm.weight shape: [16384]
664: model.layers.51.self_attn.k_proj.weight shape: [1024, 16384]
665: model.layers.51.self_attn.o_proj.weight shape: [16384, 16384]
666: model.layers.51.self_attn.q_proj.weight shape: [16384, 16384]
667: model.layers.51.self_attn.v_proj.weight shape: [1024, 16384]
668: model.layers.52.input_layernorm.weight shape: [16384]
669: model.layers.52.mlp.down_proj.weight shape: [16384, 53248]
670: model.layers.52.mlp.gate_proj.weight shape: [53248, 16384]
671: model.layers.52.mlp.up_proj.weight shape: [53248, 16384]
672: model.layers.52.post_attention_layernorm.weight shape: [16384]
673: model.layers.52.self_attn.k_proj.weight shape: [1024, 16384]
674: model.layers.52.self_attn.o_proj.weight shape: [16384, 16384]
675: model.layers.52.self_attn.q_proj.weight shape: [16384, 16384]
676: model.layers.52.self_attn.v_proj.weight shape: [1024, 16384]
677: model.layers.53.input_layernorm.weight shape: [16384]
678: model.layers.53.mlp.down_proj.weight shape: [16384, 53248]
679: model.layers.53.mlp.gate_proj.weight shape: [53248, 16384]
680: model.layers.53.mlp.up_proj.weight shape: [53248, 16384]
681: model.layers.53.post_attention_layernorm.weight shape: [16384]
682: model.layers.53.self_attn.k_proj.weight shape: [1024, 16384]
683: model.layers.53.self_attn.o_proj.weight shape: [16384, 16384]
684: model.layers.53.self_attn.q_proj.weight shape: [16384, 16384]
685: model.layers.53.self_attn.v_proj.weight shape: [1024, 16384]
686: model.layers.54.input_layernorm.weight shape: [16384]
687: model.layers.54.mlp.down_proj.weight shape: [16384, 53248]
688: model.layers.54.mlp.gate_proj.weight shape: [53248, 16384]
689: model.layers.54.mlp.up_proj.weight shape: [53248, 16384]
690: model.layers.54.post_attention_layernorm.weight shape: [16384]
691: model.layers.54.self_attn.k_proj.weight shape: [1024, 16384]
692: model.layers.54.self_attn.o_proj.weight shape: [16384, 16384]
693: model.layers.54.self_attn.q_proj.weight shape: [16384, 16384]
694: model.layers.54.self_attn.v_proj.weight shape: [1024, 16384]
695: model.layers.55.input_layernorm.weight shape: [16384]
696: model.layers.55.mlp.down_proj.weight shape: [16384, 53248]
697: model.layers.55.mlp.gate_proj.weight shape: [53248, 16384]
698: model.layers.55.mlp.up_proj.weight shape: [53248, 16384]
699: model.layers.55.post_attention_layernorm.weight shape: [16384]
700: model.layers.55.self_attn.k_proj.weight shape: [1024, 16384]
701: model.layers.55.self_attn.o_proj.weight shape: [16384, 16384]
702: model.layers.55.self_attn.q_proj.weight shape: [16384, 16384]
703: model.layers.55.self_attn.v_proj.weight shape: [1024, 16384]
704: model.layers.56.input_layernorm.weight shape: [16384]
705: model.layers.56.mlp.down_proj.weight shape: [16384, 53248]
706: model.layers.56.mlp.gate_proj.weight shape: [53248, 16384]
707: model.layers.56.mlp.up_proj.weight shape: [53248, 16384]
708: model.layers.56.post_attention_layernorm.weight shape: [16384]
709: model.layers.56.self_attn.k_proj.weight shape: [1024, 16384]
710: model.layers.56.self_attn.o_proj.weight shape: [16384, 16384]
711: model.layers.56.self_attn.q_proj.weight shape: [16384, 16384]
712: model.layers.56.self_attn.v_proj.weight shape: [1024, 16384]
713: model.layers.57.input_layernorm.weight shape: [16384]
714: model.layers.57.mlp.down_proj.weight shape: [16384, 53248]
715: model.layers.57.mlp.gate_proj.weight shape: [53248, 16384]
716: model.layers.57.mlp.up_proj.weight shape: [53248, 16384]
717: model.layers.57.post_attention_layernorm.weight shape: [16384]
718: model.layers.57.self_attn.k_proj.weight shape: [1024, 16384]
719: model.layers.57.self_attn.o_proj.weight shape: [16384, 16384]
720: model.layers.57.self_attn.q_proj.weight shape: [16384, 16384]
721: model.layers.57.self_attn.v_proj.weight shape: [1024, 16384]
722: model.layers.58.input_layernorm.weight shape: [16384]
723: model.layers.58.mlp.down_proj.weight shape: [16384, 53248]
724: model.layers.58.mlp.gate_proj.weight shape: [53248, 16384]
725: model.layers.58.mlp.up_proj.weight shape: [53248, 16384]
726: model.layers.58.post_attention_layernorm.weight shape: [16384]
727: model.layers.58.self_attn.k_proj.weight shape: [1024, 16384]
728: model.layers.58.self_attn.o_proj.weight shape: [16384, 16384]
729: model.layers.58.self_attn.q_proj.weight shape: [16384, 16384]
730: model.layers.58.self_attn.v_proj.weight shape: [1024, 16384]
731: model.layers.59.input_layernorm.weight shape: [16384]
732: model.layers.59.mlp.down_proj.weight shape: [16384, 53248]
733: model.layers.59.mlp.gate_proj.weight shape: [53248, 16384]
734: model.layers.59.mlp.up_proj.weight shape: [53248, 16384]
735: model.layers.59.post_attention_layernorm.weight shape: [16384]
736: model.layers.59.self_attn.k_proj.weight shape: [1024, 16384]
737: model.layers.59.self_attn.o_proj.weight shape: [16384, 16384]
738: model.layers.59.self_attn.q_proj.weight shape: [16384, 16384]
739: model.layers.59.self_attn.v_proj.weight shape: [1024, 16384]
740: model.layers.6.input_layernorm.weight shape: [16384]
741: model.layers.6.mlp.down_proj.weight shape: [16384, 53248]
742: model.layers.6.mlp.gate_proj.weight shape: [53248, 16384]
743: model.layers.6.mlp.up_proj.weight shape: [53248, 16384]
744: model.layers.6.post_attention_layernorm.weight shape: [16384]
745: model.layers.6.self_attn.k_proj.weight shape: [1024, 16384]
746: model.layers.6.self_attn.o_proj.weight shape: [16384, 16384]
747: model.layers.6.self_attn.q_proj.weight shape: [16384, 16384]
748: model.layers.6.self_attn.v_proj.weight shape: [1024, 16384]
749: model.layers.60.input_layernorm.weight shape: [16384]
750: model.layers.60.mlp.down_proj.weight shape: [16384, 53248]
751: model.layers.60.mlp.gate_proj.weight shape: [53248, 16384]
752: model.layers.60.mlp.up_proj.weight shape: [53248, 16384]
753: model.layers.60.post_attention_layernorm.weight shape: [16384]
754: model.layers.60.self_attn.k_proj.weight shape: [1024, 16384]
755: model.layers.60.self_attn.o_proj.weight shape: [16384, 16384]
756: model.layers.60.self_attn.q_proj.weight shape: [16384, 16384]
757: model.layers.60.self_attn.v_proj.weight shape: [1024, 16384]
758: model.layers.61.input_layernorm.weight shape: [16384]
759: model.layers.61.mlp.down_proj.weight shape: [16384, 53248]
760: model.layers.61.mlp.gate_proj.weight shape: [53248, 16384]
761: model.layers.61.mlp.up_proj.weight shape: [53248, 16384]
762: model.layers.61.post_attention_layernorm.weight shape: [16384]
763: model.layers.61.self_attn.k_proj.weight shape: [1024, 16384]
764: model.layers.61.self_attn.o_proj.weight shape: [16384, 16384]
765: model.layers.61.self_attn.q_proj.weight shape: [16384, 16384]
766: model.layers.61.self_attn.v_proj.weight shape: [1024, 16384]
767: model.layers.62.input_layernorm.weight shape: [16384]
768: model.layers.62.mlp.down_proj.weight shape: [16384, 53248]
769: model.layers.62.mlp.gate_proj.weight shape: [53248, 16384]
770: model.layers.62.mlp.up_proj.weight shape: [53248, 16384]
771: model.layers.62.post_attention_layernorm.weight shape: [16384]
772: model.layers.62.self_attn.k_proj.weight shape: [1024, 16384]
773: model.layers.62.self_attn.o_proj.weight shape: [16384, 16384]
774: model.layers.62.self_attn.q_proj.weight shape: [16384, 16384]
775: model.layers.62.self_attn.v_proj.weight shape: [1024, 16384]
776: model.layers.63.input_layernorm.weight shape: [16384]
777: model.layers.63.mlp.down_proj.weight shape: [16384, 53248]
778: model.layers.63.mlp.gate_proj.weight shape: [53248, 16384]
779: model.layers.63.mlp.up_proj.weight shape: [53248, 16384]
780: model.layers.63.post_attention_layernorm.weight shape: [16384]
781: model.layers.63.self_attn.k_proj.weight shape: [1024, 16384]
782: model.layers.63.self_attn.o_proj.weight shape: [16384, 16384]
783: model.layers.63.self_attn.q_proj.weight shape: [16384, 16384]
784: model.layers.63.self_attn.v_proj.weight shape: [1024, 16384]
785: model.layers.64.input_layernorm.weight shape: [16384]
786: model.layers.64.mlp.down_proj.weight shape: [16384, 53248]
787: model.layers.64.mlp.gate_proj.weight shape: [53248, 16384]
788: model.layers.64.mlp.up_proj.weight shape: [53248, 16384]
789: model.layers.64.post_attention_layernorm.weight shape: [16384]
790: model.layers.64.self_attn.k_proj.weight shape: [1024, 16384]
791: model.layers.64.self_attn.o_proj.weight shape: [16384, 16384]
792: model.layers.64.self_attn.q_proj.weight shape: [16384, 16384]
793: model.layers.64.self_attn.v_proj.weight shape: [1024, 16384]
794: model.layers.65.input_layernorm.weight shape: [16384]
795: model.layers.65.mlp.down_proj.weight shape: [16384, 53248]
796: model.layers.65.mlp.gate_proj.weight shape: [53248, 16384]
797: model.layers.65.mlp.up_proj.weight shape: [53248, 16384]
798: model.layers.65.post_attention_layernorm.weight shape: [16384]
799: model.layers.65.self_attn.k_proj.weight shape: [1024, 16384]
800: model.layers.65.self_attn.o_proj.weight shape: [16384, 16384]
801: model.layers.65.self_attn.q_proj.weight shape: [16384, 16384]
802: model.layers.65.self_attn.v_proj.weight shape: [1024, 16384]
803: model.layers.66.input_layernorm.weight shape: [16384]
804: model.layers.66.mlp.down_proj.weight shape: [16384, 53248]
805: model.layers.66.mlp.gate_proj.weight shape: [53248, 16384]
806: model.layers.66.mlp.up_proj.weight shape: [53248, 16384]
807: model.layers.66.post_attention_layernorm.weight shape: [16384]
808: model.layers.66.self_attn.k_proj.weight shape: [1024, 16384]
809: model.layers.66.self_attn.o_proj.weight shape: [16384, 16384]
810: model.layers.66.self_attn.q_proj.weight shape: [16384, 16384]
811: model.layers.66.self_attn.v_proj.weight shape: [1024, 16384]
812: model.layers.67.input_layernorm.weight shape: [16384]
813: model.layers.67.mlp.down_proj.weight shape: [16384, 53248]
814: model.layers.67.mlp.gate_proj.weight shape: [53248, 16384]
815: model.layers.67.mlp.up_proj.weight shape: [53248, 16384]
816: model.layers.67.post_attention_layernorm.weight shape: [16384]
817: model.layers.67.self_attn.k_proj.weight shape: [1024, 16384]
818: model.layers.67.self_attn.o_proj.weight shape: [16384, 16384]
819: model.layers.67.self_attn.q_proj.weight shape: [16384, 16384]
820: model.layers.67.self_attn.v_proj.weight shape: [1024, 16384]
821: model.layers.68.input_layernorm.weight shape: [16384]
822: model.layers.68.mlp.down_proj.weight shape: [16384, 53248]
823: model.layers.68.mlp.gate_proj.weight shape: [53248, 16384]
824: model.layers.68.mlp.up_proj.weight shape: [53248, 16384]
825: model.layers.68.post_attention_layernorm.weight shape: [16384]
826: model.layers.68.self_attn.k_proj.weight shape: [1024, 16384]
827: model.layers.68.self_attn.o_proj.weight shape: [16384, 16384]
828: model.layers.68.self_attn.q_proj.weight shape: [16384, 16384]
829: model.layers.68.self_attn.v_proj.weight shape: [1024, 16384]
830: model.layers.69.input_layernorm.weight shape: [16384]
831: model.layers.69.mlp.down_proj.weight shape: [16384, 53248]
832: model.layers.69.mlp.gate_proj.weight shape: [53248, 16384]
833: model.layers.69.mlp.up_proj.weight shape: [53248, 16384]
834: model.layers.69.post_attention_layernorm.weight shape: [16384]
835: model.layers.69.self_attn.k_proj.weight shape: [1024, 16384]
836: model.layers.69.self_attn.o_proj.weight shape: [16384, 16384]
837: model.layers.69.self_attn.q_proj.weight shape: [16384, 16384]
838: model.layers.69.self_attn.v_proj.weight shape: [1024, 16384]
839: model.layers.7.input_layernorm.weight shape: [16384]
840: model.layers.7.mlp.down_proj.weight shape: [16384, 53248]
841: model.layers.7.mlp.gate_proj.weight shape: [53248, 16384]
842: model.layers.7.mlp.up_proj.weight shape: [53248, 16384]
843: model.layers.7.post_attention_layernorm.weight shape: [16384]
844: model.layers.7.self_attn.k_proj.weight shape: [1024, 16384]
845: model.layers.7.self_attn.o_proj.weight shape: [16384, 16384]
846: model.layers.7.self_attn.q_proj.weight shape: [16384, 16384]
847: model.layers.7.self_attn.v_proj.weight shape: [1024, 16384]
848: model.layers.70.input_layernorm.weight shape: [16384]
849: model.layers.70.mlp.down_proj.weight shape: [16384, 53248]
850: model.layers.70.mlp.gate_proj.weight shape: [53248, 16384]
851: model.layers.70.mlp.up_proj.weight shape: [53248, 16384]
852: model.layers.70.post_attention_layernorm.weight shape: [16384]
853: model.layers.70.self_attn.k_proj.weight shape: [1024, 16384]
854: model.layers.70.self_attn.o_proj.weight shape: [16384, 16384]
855: model.layers.70.self_attn.q_proj.weight shape: [16384, 16384]
856: model.layers.70.self_attn.v_proj.weight shape: [1024, 16384]
857: model.layers.71.input_layernorm.weight shape: [16384]
858: model.layers.71.mlp.down_proj.weight shape: [16384, 53248]
859: model.layers.71.mlp.gate_proj.weight shape: [53248, 16384]
860: model.layers.71.mlp.up_proj.weight shape: [53248, 16384]
861: model.layers.71.post_attention_layernorm.weight shape: [16384]
862: model.layers.71.self_attn.k_proj.weight shape: [1024, 16384]
863: model.layers.71.self_attn.o_proj.weight shape: [16384, 16384]
864: model.layers.71.self_attn.q_proj.weight shape: [16384, 16384]
865: model.layers.71.self_attn.v_proj.weight shape: [1024, 16384]
866: model.layers.72.input_layernorm.weight shape: [16384]
867: model.layers.72.mlp.down_proj.weight shape: [16384, 53248]
868: model.layers.72.mlp.gate_proj.weight shape: [53248, 16384]
869: model.layers.72.mlp.up_proj.weight shape: [53248, 16384]
870: model.layers.72.post_attention_layernorm.weight shape: [16384]
871: model.layers.72.self_attn.k_proj.weight shape: [1024, 16384]
872: model.layers.72.self_attn.o_proj.weight shape: [16384, 16384]
873: model.layers.72.self_attn.q_proj.weight shape: [16384, 16384]
874: model.layers.72.self_attn.v_proj.weight shape: [1024, 16384]
875: model.layers.73.input_layernorm.weight shape: [16384]
876: model.layers.73.mlp.down_proj.weight shape: [16384, 53248]
877: model.layers.73.mlp.gate_proj.weight shape: [53248, 16384]
878: model.layers.73.mlp.up_proj.weight shape: [53248, 16384]
879: model.layers.73.post_attention_layernorm.weight shape: [16384]
880: model.layers.73.self_attn.k_proj.weight shape: [1024, 16384]
881: model.layers.73.self_attn.o_proj.weight shape: [16384, 16384]
882: model.layers.73.self_attn.q_proj.weight shape: [16384, 16384]
883: model.layers.73.self_attn.v_proj.weight shape: [1024, 16384]
884: model.layers.74.input_layernorm.weight shape: [16384]
885: model.layers.74.mlp.down_proj.weight shape: [16384, 53248]
886: model.layers.74.mlp.gate_proj.weight shape: [53248, 16384]
887: model.layers.74.mlp.up_proj.weight shape: [53248, 16384]
888: model.layers.74.post_attention_layernorm.weight shape: [16384]
889: model.layers.74.self_attn.k_proj.weight shape: [1024, 16384]
890: model.layers.74.self_attn.o_proj.weight shape: [16384, 16384]
891: model.layers.74.self_attn.q_proj.weight shape: [16384, 16384]
892: model.layers.74.self_attn.v_proj.weight shape: [1024, 16384]
893: model.layers.75.input_layernorm.weight shape: [16384]
894: model.layers.75.mlp.down_proj.weight shape: [16384, 53248]
895: model.layers.75.mlp.gate_proj.weight shape: [53248, 16384]
896: model.layers.75.mlp.up_proj.weight shape: [53248, 16384]
897: model.layers.75.post_attention_layernorm.weight shape: [16384]
898: model.layers.75.self_attn.k_proj.weight shape: [1024, 16384]
899: model.layers.75.self_attn.o_proj.weight shape: [16384, 16384]
900: model.layers.75.self_attn.q_proj.weight shape: [16384, 16384]
901: model.layers.75.self_attn.v_proj.weight shape: [1024, 16384]
902: model.layers.76.input_layernorm.weight shape: [16384]
903: model.layers.76.mlp.down_proj.weight shape: [16384, 53248]
904: model.layers.76.mlp.gate_proj.weight shape: [53248, 16384]
905: model.layers.76.mlp.up_proj.weight shape: [53248, 16384]
906: model.layers.76.post_attention_layernorm.weight shape: [16384]
907: model.layers.76.self_attn.k_proj.weight shape: [1024, 16384]
908: model.layers.76.self_attn.o_proj.weight shape: [16384, 16384]
909: model.layers.76.self_attn.q_proj.weight shape: [16384, 16384]
910: model.layers.76.self_attn.v_proj.weight shape: [1024, 16384]
911: model.layers.77.input_layernorm.weight shape: [16384]
912: model.layers.77.mlp.down_proj.weight shape: [16384, 53248]
913: model.layers.77.mlp.gate_proj.weight shape: [53248, 16384]
914: model.layers.77.mlp.up_proj.weight shape: [53248, 16384]
915: model.layers.77.post_attention_layernorm.weight shape: [16384]
916: model.layers.77.self_attn.k_proj.weight shape: [1024, 16384]
917: model.layers.77.self_attn.o_proj.weight shape: [16384, 16384]
918: model.layers.77.self_attn.q_proj.weight shape: [16384, 16384]
919: model.layers.77.self_attn.v_proj.weight shape: [1024, 16384]
920: model.layers.78.input_layernorm.weight shape: [16384]
921: model.layers.78.mlp.down_proj.weight shape: [16384, 53248]
922: model.layers.78.mlp.gate_proj.weight shape: [53248, 16384]
923: model.layers.78.mlp.up_proj.weight shape: [53248, 16384]
924: model.layers.78.post_attention_layernorm.weight shape: [16384]
925: model.layers.78.self_attn.k_proj.weight shape: [1024, 16384]
926: model.layers.78.self_attn.o_proj.weight shape: [16384, 16384]
927: model.layers.78.self_attn.q_proj.weight shape: [16384, 16384]
928: model.layers.78.self_attn.v_proj.weight shape: [1024, 16384]
929: model.layers.79.input_layernorm.weight shape: [16384]
930: model.layers.79.mlp.down_proj.weight shape: [16384, 53248]
931: model.layers.79.mlp.gate_proj.weight shape: [53248, 16384]
932: model.layers.79.mlp.up_proj.weight shape: [53248, 16384]
933: model.layers.79.post_attention_layernorm.weight shape: [16384]
934: model.layers.79.self_attn.k_proj.weight shape: [1024, 16384]
935: model.layers.79.self_attn.o_proj.weight shape: [16384, 16384]
936: model.layers.79.self_attn.q_proj.weight shape: [16384, 16384]
937: model.layers.79.self_attn.v_proj.weight shape: [1024, 16384]
938: model.layers.8.input_layernorm.weight shape: [16384]
939: model.layers.8.mlp.down_proj.weight shape: [16384, 53248]
940: model.layers.8.mlp.gate_proj.weight shape: [53248, 16384]
941: model.layers.8.mlp.up_proj.weight shape: [53248, 16384]
942: model.layers.8.post_attention_layernorm.weight shape: [16384]
943: model.layers.8.self_attn.k_proj.weight shape: [1024, 16384]
944: model.layers.8.self_attn.o_proj.weight shape: [16384, 16384]
945: model.layers.8.self_attn.q_proj.weight shape: [16384, 16384]
946: model.layers.8.self_attn.v_proj.weight shape: [1024, 16384]
947: model.layers.80.input_layernorm.weight shape: [16384]
948: model.layers.80.mlp.down_proj.weight shape: [16384, 53248]
949: model.layers.80.mlp.gate_proj.weight shape: [53248, 16384]
950: model.layers.80.mlp.up_proj.weight shape: [53248, 16384]
951: model.layers.80.post_attention_layernorm.weight shape: [16384]
952: model.layers.80.self_attn.k_proj.weight shape: [1024, 16384]
953: model.layers.80.self_attn.o_proj.weight shape: [16384, 16384]
954: model.layers.80.self_attn.q_proj.weight shape: [16384, 16384]
955: model.layers.80.self_attn.v_proj.weight shape: [1024, 16384]
956: model.layers.81.input_layernorm.weight shape: [16384]
957: model.layers.81.mlp.down_proj.weight shape: [16384, 53248]
958: model.layers.81.mlp.gate_proj.weight shape: [53248, 16384]
959: model.layers.81.mlp.up_proj.weight shape: [53248, 16384]
960: model.layers.81.post_attention_layernorm.weight shape: [16384]
961: model.layers.81.self_attn.k_proj.weight shape: [1024, 16384]
962: model.layers.81.self_attn.o_proj.weight shape: [16384, 16384]
963: model.layers.81.self_attn.q_proj.weight shape: [16384, 16384]
964: model.layers.81.self_attn.v_proj.weight shape: [1024, 16384]
965: model.layers.82.input_layernorm.weight shape: [16384]
966: model.layers.82.mlp.down_proj.weight shape: [16384, 53248]
967: model.layers.82.mlp.gate_proj.weight shape: [53248, 16384]
968: model.layers.82.mlp.up_proj.weight shape: [53248, 16384]
969: model.layers.82.post_attention_layernorm.weight shape: [16384]
970: model.layers.82.self_attn.k_proj.weight shape: [1024, 16384]
971: model.layers.82.self_attn.o_proj.weight shape: [16384, 16384]
972: model.layers.82.self_attn.q_proj.weight shape: [16384, 16384]
973: model.layers.82.self_attn.v_proj.weight shape: [1024, 16384]
974: model.layers.83.input_layernorm.weight shape: [16384]
975: model.layers.83.mlp.down_proj.weight shape: [16384, 53248]
976: model.layers.83.mlp.gate_proj.weight shape: [53248, 16384]
977: model.layers.83.mlp.up_proj.weight shape: [53248, 16384]
978: model.layers.83.post_attention_layernorm.weight shape: [16384]
979: model.layers.83.self_attn.k_proj.weight shape: [1024, 16384]
980: model.layers.83.self_attn.o_proj.weight shape: [16384, 16384]
981: model.layers.83.self_attn.q_proj.weight shape: [16384, 16384]
982: model.layers.83.self_attn.v_proj.weight shape: [1024, 16384]
983: model.layers.84.input_layernorm.weight shape: [16384]
984: model.layers.84.mlp.down_proj.weight shape: [16384, 53248]
985: model.layers.84.mlp.gate_proj.weight shape: [53248, 16384]
986: model.layers.84.mlp.up_proj.weight shape: [53248, 16384]
987: model.layers.84.post_attention_layernorm.weight shape: [16384]
988: model.layers.84.self_attn.k_proj.weight shape: [1024, 16384]
989: model.layers.84.self_attn.o_proj.weight shape: [16384, 16384]
990: model.layers.84.self_attn.q_proj.weight shape: [16384, 16384]
991: model.layers.84.self_attn.v_proj.weight shape: [1024, 16384]
992: model.layers.85.input_layernorm.weight shape: [16384]
993: model.layers.85.mlp.down_proj.weight shape: [16384, 53248]
994: model.layers.85.mlp.gate_proj.weight shape: [53248, 16384]
995: model.layers.85.mlp.up_proj.weight shape: [53248, 16384]
996: model.layers.85.post_attention_layernorm.weight shape: [16384]
997: model.layers.85.self_attn.k_proj.weight shape: [1024, 16384]
998: model.layers.85.self_attn.o_proj.weight shape: [16384, 16384]
999: model.layers.85.self_attn.q_proj.weight shape: [16384, 16384]
1000: model.layers.85.self_attn.v_proj.weight shape: [1024, 16384]
1001: model.layers.86.input_layernorm.weight shape: [16384]
1002: model.layers.86.mlp.down_proj.weight shape: [16384, 53248]
1003: model.layers.86.mlp.gate_proj.weight shape: [53248, 16384]
1004: model.layers.86.mlp.up_proj.weight shape: [53248, 16384]
1005: model.layers.86.post_attention_layernorm.weight shape: [16384]
1006: model.layers.86.self_attn.k_proj.weight shape: [1024, 16384]
1007: model.layers.86.self_attn.o_proj.weight shape: [16384, 16384]
1008: model.layers.86.self_attn.q_proj.weight shape: [16384, 16384]
1009: model.layers.86.self_attn.v_proj.weight shape: [1024, 16384]
1010: model.layers.87.input_layernorm.weight shape: [16384]
1011: model.layers.87.mlp.down_proj.weight shape: [16384, 53248]
1012: model.layers.87.mlp.gate_proj.weight shape: [53248, 16384]
1013: model.layers.87.mlp.up_proj.weight shape: [53248, 16384]
1014: model.layers.87.post_attention_layernorm.weight shape: [16384]
1015: model.layers.87.self_attn.k_proj.weight shape: [1024, 16384]
1016: model.layers.87.self_attn.o_proj.weight shape: [16384, 16384]
1017: model.layers.87.self_attn.q_proj.weight shape: [16384, 16384]
1018: model.layers.87.self_attn.v_proj.weight shape: [1024, 16384]
1019: model.layers.88.input_layernorm.weight shape: [16384]
1020: model.layers.88.mlp.down_proj.weight shape: [16384, 53248]
1021: model.layers.88.mlp.gate_proj.weight shape: [53248, 16384]
1022: model.layers.88.mlp.up_proj.weight shape: [53248, 16384]
1023: model.layers.88.post_attention_layernorm.weight shape: [16384]
1024: model.layers.88.self_attn.k_proj.weight shape: [1024, 16384]
1025: model.layers.88.self_attn.o_proj.weight shape: [16384, 16384]
1026: model.layers.88.self_attn.q_proj.weight shape: [16384, 16384]
1027: model.layers.88.self_attn.v_proj.weight shape: [1024, 16384]
1028: model.layers.89.input_layernorm.weight shape: [16384]
1029: model.layers.89.mlp.down_proj.weight shape: [16384, 53248]
1030: model.layers.89.mlp.gate_proj.weight shape: [53248, 16384]
1031: model.layers.89.mlp.up_proj.weight shape: [53248, 16384]
1032: model.layers.89.post_attention_layernorm.weight shape: [16384]
1033: model.layers.89.self_attn.k_proj.weight shape: [1024, 16384]
1034: model.layers.89.self_attn.o_proj.weight shape: [16384, 16384]
1035: model.layers.89.self_attn.q_proj.weight shape: [16384, 16384]
1036: model.layers.89.self_attn.v_proj.weight shape: [1024, 16384]
1037: model.layers.9.input_layernorm.weight shape: [16384]
1038: model.layers.9.mlp.down_proj.weight shape: [16384, 53248]
1039: model.layers.9.mlp.gate_proj.weight shape: [53248, 16384]
1040: model.layers.9.mlp.up_proj.weight shape: [53248, 16384]
1041: model.layers.9.post_attention_layernorm.weight shape: [16384]
1042: model.layers.9.self_attn.k_proj.weight shape: [1024, 16384]
1043: model.layers.9.self_attn.o_proj.weight shape: [16384, 16384]
1044: model.layers.9.self_attn.q_proj.weight shape: [16384, 16384]
1045: model.layers.9.self_attn.v_proj.weight shape: [1024, 16384]
1046: model.layers.90.input_layernorm.weight shape: [16384]
1047: model.layers.90.mlp.down_proj.weight shape: [16384, 53248]
1048: model.layers.90.mlp.gate_proj.weight shape: [53248, 16384]
1049: model.layers.90.mlp.up_proj.weight shape: [53248, 16384]
1050: model.layers.90.post_attention_layernorm.weight shape: [16384]
1051: model.layers.90.self_attn.k_proj.weight shape: [1024, 16384]
1052: model.layers.90.self_attn.o_proj.weight shape: [16384, 16384]
1053: model.layers.90.self_attn.q_proj.weight shape: [16384, 16384]
1054: model.layers.90.self_attn.v_proj.weight shape: [1024, 16384]
1055: model.layers.91.input_layernorm.weight shape: [16384]
1056: model.layers.91.mlp.down_proj.weight shape: [16384, 53248]
1057: model.layers.91.mlp.gate_proj.weight shape: [53248, 16384]
1058: model.layers.91.mlp.up_proj.weight shape: [53248, 16384]
1059: model.layers.91.post_attention_layernorm.weight shape: [16384]
1060: model.layers.91.self_attn.k_proj.weight shape: [1024, 16384]
1061: model.layers.91.self_attn.o_proj.weight shape: [16384, 16384]
1062: model.layers.91.self_attn.q_proj.weight shape: [16384, 16384]
1063: model.layers.91.self_attn.v_proj.weight shape: [1024, 16384]
1064: model.layers.92.input_layernorm.weight shape: [16384]
1065: model.layers.92.mlp.down_proj.weight shape: [16384, 53248]
1066: model.layers.92.mlp.gate_proj.weight shape: [53248, 16384]
1067: model.layers.92.mlp.up_proj.weight shape: [53248, 16384]
1068: model.layers.92.post_attention_layernorm.weight shape: [16384]
1069: model.layers.92.self_attn.k_proj.weight shape: [1024, 16384]
1070: model.layers.92.self_attn.o_proj.weight shape: [16384, 16384]
1071: model.layers.92.self_attn.q_proj.weight shape: [16384, 16384]
1072: model.layers.92.self_attn.v_proj.weight shape: [1024, 16384]
1073: model.layers.93.input_layernorm.weight shape: [16384]
1074: model.layers.93.mlp.down_proj.weight shape: [16384, 53248]
1075: model.layers.93.mlp.gate_proj.weight shape: [53248, 16384]
1076: model.layers.93.mlp.up_proj.weight shape: [53248, 16384]
1077: model.layers.93.post_attention_layernorm.weight shape: [16384]
1078: model.layers.93.self_attn.k_proj.weight shape: [1024, 16384]
1079: model.layers.93.self_attn.o_proj.weight shape: [16384, 16384]
1080: model.layers.93.self_attn.q_proj.weight shape: [16384, 16384]
1081: model.layers.93.self_attn.v_proj.weight shape: [1024, 16384]
1082: model.layers.94.input_layernorm.weight shape: [16384]
1083: model.layers.94.mlp.down_proj.weight shape: [16384, 53248]
1084: model.layers.94.mlp.gate_proj.weight shape: [53248, 16384]
1085: model.layers.94.mlp.up_proj.weight shape: [53248, 16384]
1086: model.layers.94.post_attention_layernorm.weight shape: [16384]
1087: model.layers.94.self_attn.k_proj.weight shape: [1024, 16384]
1088: model.layers.94.self_attn.o_proj.weight shape: [16384, 16384]
1089: model.layers.94.self_attn.q_proj.weight shape: [16384, 16384]
1090: model.layers.94.self_attn.v_proj.weight shape: [1024, 16384]
1091: model.layers.95.input_layernorm.weight shape: [16384]
1092: model.layers.95.mlp.down_proj.weight shape: [16384, 53248]
1093: model.layers.95.mlp.gate_proj.weight shape: [53248, 16384]
1094: model.layers.95.mlp.up_proj.weight shape: [53248, 16384]
1095: model.layers.95.post_attention_layernorm.weight shape: [16384]
1096: model.layers.95.self_attn.k_proj.weight shape: [1024, 16384]
1097: model.layers.95.self_attn.o_proj.weight shape: [16384, 16384]
1098: model.layers.95.self_attn.q_proj.weight shape: [16384, 16384]
1099: model.layers.95.self_attn.v_proj.weight shape: [1024, 16384]
1100: model.layers.96.input_layernorm.weight shape: [16384]
1101: model.layers.96.mlp.down_proj.weight shape: [16384, 53248]
1102: model.layers.96.mlp.gate_proj.weight shape: [53248, 16384]
1103: model.layers.96.mlp.up_proj.weight shape: [53248, 16384]
1104: model.layers.96.post_attention_layernorm.weight shape: [16384]
1105: model.layers.96.self_attn.k_proj.weight shape: [1024, 16384]
1106: model.layers.96.self_attn.o_proj.weight shape: [16384, 16384]
1107: model.layers.96.self_attn.q_proj.weight shape: [16384, 16384]
1108: model.layers.96.self_attn.v_proj.weight shape: [1024, 16384]
1109: model.layers.97.input_layernorm.weight shape: [16384]
1110: model.layers.97.mlp.down_proj.weight shape: [16384, 53248]
1111: model.layers.97.mlp.gate_proj.weight shape: [53248, 16384]
1112: model.layers.97.mlp.up_proj.weight shape: [53248, 16384]
1113: model.layers.97.post_attention_layernorm.weight shape: [16384]
1114: model.layers.97.self_attn.k_proj.weight shape: [1024, 16384]
1115: model.layers.97.self_attn.o_proj.weight shape: [16384, 16384]
1116: model.layers.97.self_attn.q_proj.weight shape: [16384, 16384]
1117: model.layers.97.self_attn.v_proj.weight shape: [1024, 16384]
1118: model.layers.98.input_layernorm.weight shape: [16384]
1119: model.layers.98.mlp.down_proj.weight shape: [16384, 53248]
1120: model.layers.98.mlp.gate_proj.weight shape: [53248, 16384]
1121: model.layers.98.mlp.up_proj.weight shape: [53248, 16384]
1122: model.layers.98.post_attention_layernorm.weight shape: [16384]
1123: model.layers.98.self_attn.k_proj.weight shape: [1024, 16384]
1124: model.layers.98.self_attn.o_proj.weight shape: [16384, 16384]
1125: model.layers.98.self_attn.q_proj.weight shape: [16384, 16384]
1126: model.layers.98.self_attn.v_proj.weight shape: [1024, 16384]
1127: model.layers.99.input_layernorm.weight shape: [16384]
1128: model.layers.99.mlp.down_proj.weight shape: [16384, 53248]
1129: model.layers.99.mlp.gate_proj.weight shape: [53248, 16384]
1130: model.layers.99.mlp.up_proj.weight shape: [53248, 16384]
1131: model.layers.99.post_attention_layernorm.weight shape: [16384]
1132: model.layers.99.self_attn.k_proj.weight shape: [1024, 16384]
1133: model.layers.99.self_attn.o_proj.weight shape: [16384, 16384]
1134: model.layers.99.self_attn.q_proj.weight shape: [16384, 16384]
1135: model.layers.99.self_attn.v_proj.weight shape: [1024, 16384]
1136: model.norm.weight shape: [16384]
