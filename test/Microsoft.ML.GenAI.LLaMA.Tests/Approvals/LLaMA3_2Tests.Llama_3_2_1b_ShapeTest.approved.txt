0: model.embed_tokens.weight shape: [128256, 2048]
1: model.layers.0.input_layernorm.weight shape: [2048]
2: model.layers.0.mlp.down_proj.weight shape: [2048, 8192]
3: model.layers.0.mlp.gate_proj.weight shape: [8192, 2048]
4: model.layers.0.mlp.up_proj.weight shape: [8192, 2048]
5: model.layers.0.post_attention_layernorm.weight shape: [2048]
6: model.layers.0.self_attn.k_proj.weight shape: [512, 2048]
7: model.layers.0.self_attn.o_proj.weight shape: [2048, 2048]
8: model.layers.0.self_attn.q_proj.weight shape: [2048, 2048]
9: model.layers.0.self_attn.v_proj.weight shape: [512, 2048]
10: model.layers.1.input_layernorm.weight shape: [2048]
11: model.layers.1.mlp.down_proj.weight shape: [2048, 8192]
12: model.layers.1.mlp.gate_proj.weight shape: [8192, 2048]
13: model.layers.1.mlp.up_proj.weight shape: [8192, 2048]
14: model.layers.1.post_attention_layernorm.weight shape: [2048]
15: model.layers.1.self_attn.k_proj.weight shape: [512, 2048]
16: model.layers.1.self_attn.o_proj.weight shape: [2048, 2048]
17: model.layers.1.self_attn.q_proj.weight shape: [2048, 2048]
18: model.layers.1.self_attn.v_proj.weight shape: [512, 2048]
19: model.layers.10.input_layernorm.weight shape: [2048]
20: model.layers.10.mlp.down_proj.weight shape: [2048, 8192]
21: model.layers.10.mlp.gate_proj.weight shape: [8192, 2048]
22: model.layers.10.mlp.up_proj.weight shape: [8192, 2048]
23: model.layers.10.post_attention_layernorm.weight shape: [2048]
24: model.layers.10.self_attn.k_proj.weight shape: [512, 2048]
25: model.layers.10.self_attn.o_proj.weight shape: [2048, 2048]
26: model.layers.10.self_attn.q_proj.weight shape: [2048, 2048]
27: model.layers.10.self_attn.v_proj.weight shape: [512, 2048]
28: model.layers.11.input_layernorm.weight shape: [2048]
29: model.layers.11.mlp.down_proj.weight shape: [2048, 8192]
30: model.layers.11.mlp.gate_proj.weight shape: [8192, 2048]
31: model.layers.11.mlp.up_proj.weight shape: [8192, 2048]
32: model.layers.11.post_attention_layernorm.weight shape: [2048]
33: model.layers.11.self_attn.k_proj.weight shape: [512, 2048]
34: model.layers.11.self_attn.o_proj.weight shape: [2048, 2048]
35: model.layers.11.self_attn.q_proj.weight shape: [2048, 2048]
36: model.layers.11.self_attn.v_proj.weight shape: [512, 2048]
37: model.layers.12.input_layernorm.weight shape: [2048]
38: model.layers.12.mlp.down_proj.weight shape: [2048, 8192]
39: model.layers.12.mlp.gate_proj.weight shape: [8192, 2048]
40: model.layers.12.mlp.up_proj.weight shape: [8192, 2048]
41: model.layers.12.post_attention_layernorm.weight shape: [2048]
42: model.layers.12.self_attn.k_proj.weight shape: [512, 2048]
43: model.layers.12.self_attn.o_proj.weight shape: [2048, 2048]
44: model.layers.12.self_attn.q_proj.weight shape: [2048, 2048]
45: model.layers.12.self_attn.v_proj.weight shape: [512, 2048]
46: model.layers.13.input_layernorm.weight shape: [2048]
47: model.layers.13.mlp.down_proj.weight shape: [2048, 8192]
48: model.layers.13.mlp.gate_proj.weight shape: [8192, 2048]
49: model.layers.13.mlp.up_proj.weight shape: [8192, 2048]
50: model.layers.13.post_attention_layernorm.weight shape: [2048]
51: model.layers.13.self_attn.k_proj.weight shape: [512, 2048]
52: model.layers.13.self_attn.o_proj.weight shape: [2048, 2048]
53: model.layers.13.self_attn.q_proj.weight shape: [2048, 2048]
54: model.layers.13.self_attn.v_proj.weight shape: [512, 2048]
55: model.layers.14.input_layernorm.weight shape: [2048]
56: model.layers.14.mlp.down_proj.weight shape: [2048, 8192]
57: model.layers.14.mlp.gate_proj.weight shape: [8192, 2048]
58: model.layers.14.mlp.up_proj.weight shape: [8192, 2048]
59: model.layers.14.post_attention_layernorm.weight shape: [2048]
60: model.layers.14.self_attn.k_proj.weight shape: [512, 2048]
61: model.layers.14.self_attn.o_proj.weight shape: [2048, 2048]
62: model.layers.14.self_attn.q_proj.weight shape: [2048, 2048]
63: model.layers.14.self_attn.v_proj.weight shape: [512, 2048]
64: model.layers.15.input_layernorm.weight shape: [2048]
65: model.layers.15.mlp.down_proj.weight shape: [2048, 8192]
66: model.layers.15.mlp.gate_proj.weight shape: [8192, 2048]
67: model.layers.15.mlp.up_proj.weight shape: [8192, 2048]
68: model.layers.15.post_attention_layernorm.weight shape: [2048]
69: model.layers.15.self_attn.k_proj.weight shape: [512, 2048]
70: model.layers.15.self_attn.o_proj.weight shape: [2048, 2048]
71: model.layers.15.self_attn.q_proj.weight shape: [2048, 2048]
72: model.layers.15.self_attn.v_proj.weight shape: [512, 2048]
73: model.layers.2.input_layernorm.weight shape: [2048]
74: model.layers.2.mlp.down_proj.weight shape: [2048, 8192]
75: model.layers.2.mlp.gate_proj.weight shape: [8192, 2048]
76: model.layers.2.mlp.up_proj.weight shape: [8192, 2048]
77: model.layers.2.post_attention_layernorm.weight shape: [2048]
78: model.layers.2.self_attn.k_proj.weight shape: [512, 2048]
79: model.layers.2.self_attn.o_proj.weight shape: [2048, 2048]
80: model.layers.2.self_attn.q_proj.weight shape: [2048, 2048]
81: model.layers.2.self_attn.v_proj.weight shape: [512, 2048]
82: model.layers.3.input_layernorm.weight shape: [2048]
83: model.layers.3.mlp.down_proj.weight shape: [2048, 8192]
84: model.layers.3.mlp.gate_proj.weight shape: [8192, 2048]
85: model.layers.3.mlp.up_proj.weight shape: [8192, 2048]
86: model.layers.3.post_attention_layernorm.weight shape: [2048]
87: model.layers.3.self_attn.k_proj.weight shape: [512, 2048]
88: model.layers.3.self_attn.o_proj.weight shape: [2048, 2048]
89: model.layers.3.self_attn.q_proj.weight shape: [2048, 2048]
90: model.layers.3.self_attn.v_proj.weight shape: [512, 2048]
91: model.layers.4.input_layernorm.weight shape: [2048]
92: model.layers.4.mlp.down_proj.weight shape: [2048, 8192]
93: model.layers.4.mlp.gate_proj.weight shape: [8192, 2048]
94: model.layers.4.mlp.up_proj.weight shape: [8192, 2048]
95: model.layers.4.post_attention_layernorm.weight shape: [2048]
96: model.layers.4.self_attn.k_proj.weight shape: [512, 2048]
97: model.layers.4.self_attn.o_proj.weight shape: [2048, 2048]
98: model.layers.4.self_attn.q_proj.weight shape: [2048, 2048]
99: model.layers.4.self_attn.v_proj.weight shape: [512, 2048]
100: model.layers.5.input_layernorm.weight shape: [2048]
101: model.layers.5.mlp.down_proj.weight shape: [2048, 8192]
102: model.layers.5.mlp.gate_proj.weight shape: [8192, 2048]
103: model.layers.5.mlp.up_proj.weight shape: [8192, 2048]
104: model.layers.5.post_attention_layernorm.weight shape: [2048]
105: model.layers.5.self_attn.k_proj.weight shape: [512, 2048]
106: model.layers.5.self_attn.o_proj.weight shape: [2048, 2048]
107: model.layers.5.self_attn.q_proj.weight shape: [2048, 2048]
108: model.layers.5.self_attn.v_proj.weight shape: [512, 2048]
109: model.layers.6.input_layernorm.weight shape: [2048]
110: model.layers.6.mlp.down_proj.weight shape: [2048, 8192]
111: model.layers.6.mlp.gate_proj.weight shape: [8192, 2048]
112: model.layers.6.mlp.up_proj.weight shape: [8192, 2048]
113: model.layers.6.post_attention_layernorm.weight shape: [2048]
114: model.layers.6.self_attn.k_proj.weight shape: [512, 2048]
115: model.layers.6.self_attn.o_proj.weight shape: [2048, 2048]
116: model.layers.6.self_attn.q_proj.weight shape: [2048, 2048]
117: model.layers.6.self_attn.v_proj.weight shape: [512, 2048]
118: model.layers.7.input_layernorm.weight shape: [2048]
119: model.layers.7.mlp.down_proj.weight shape: [2048, 8192]
120: model.layers.7.mlp.gate_proj.weight shape: [8192, 2048]
121: model.layers.7.mlp.up_proj.weight shape: [8192, 2048]
122: model.layers.7.post_attention_layernorm.weight shape: [2048]
123: model.layers.7.self_attn.k_proj.weight shape: [512, 2048]
124: model.layers.7.self_attn.o_proj.weight shape: [2048, 2048]
125: model.layers.7.self_attn.q_proj.weight shape: [2048, 2048]
126: model.layers.7.self_attn.v_proj.weight shape: [512, 2048]
127: model.layers.8.input_layernorm.weight shape: [2048]
128: model.layers.8.mlp.down_proj.weight shape: [2048, 8192]
129: model.layers.8.mlp.gate_proj.weight shape: [8192, 2048]
130: model.layers.8.mlp.up_proj.weight shape: [8192, 2048]
131: model.layers.8.post_attention_layernorm.weight shape: [2048]
132: model.layers.8.self_attn.k_proj.weight shape: [512, 2048]
133: model.layers.8.self_attn.o_proj.weight shape: [2048, 2048]
134: model.layers.8.self_attn.q_proj.weight shape: [2048, 2048]
135: model.layers.8.self_attn.v_proj.weight shape: [512, 2048]
136: model.layers.9.input_layernorm.weight shape: [2048]
137: model.layers.9.mlp.down_proj.weight shape: [2048, 8192]
138: model.layers.9.mlp.gate_proj.weight shape: [8192, 2048]
139: model.layers.9.mlp.up_proj.weight shape: [8192, 2048]
140: model.layers.9.post_attention_layernorm.weight shape: [2048]
141: model.layers.9.self_attn.k_proj.weight shape: [512, 2048]
142: model.layers.9.self_attn.o_proj.weight shape: [2048, 2048]
143: model.layers.9.self_attn.q_proj.weight shape: [2048, 2048]
144: model.layers.9.self_attn.v_proj.weight shape: [512, 2048]
145: model.norm.weight shape: [2048]
