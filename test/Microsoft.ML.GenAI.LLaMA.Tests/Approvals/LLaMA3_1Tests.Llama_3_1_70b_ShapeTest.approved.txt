0: lm_head.weight shape: [128256, 8192]
1: model.embed_tokens.weight shape: [128256, 8192]
2: model.layers.0.input_layernorm.weight shape: [8192]
3: model.layers.0.mlp.down_proj.weight shape: [8192, 28672]
4: model.layers.0.mlp.gate_proj.weight shape: [28672, 8192]
5: model.layers.0.mlp.up_proj.weight shape: [28672, 8192]
6: model.layers.0.post_attention_layernorm.weight shape: [8192]
7: model.layers.0.self_attn.k_proj.weight shape: [1024, 8192]
8: model.layers.0.self_attn.o_proj.weight shape: [8192, 8192]
9: model.layers.0.self_attn.q_proj.weight shape: [8192, 8192]
10: model.layers.0.self_attn.v_proj.weight shape: [1024, 8192]
11: model.layers.1.input_layernorm.weight shape: [8192]
12: model.layers.1.mlp.down_proj.weight shape: [8192, 28672]
13: model.layers.1.mlp.gate_proj.weight shape: [28672, 8192]
14: model.layers.1.mlp.up_proj.weight shape: [28672, 8192]
15: model.layers.1.post_attention_layernorm.weight shape: [8192]
16: model.layers.1.self_attn.k_proj.weight shape: [1024, 8192]
17: model.layers.1.self_attn.o_proj.weight shape: [8192, 8192]
18: model.layers.1.self_attn.q_proj.weight shape: [8192, 8192]
19: model.layers.1.self_attn.v_proj.weight shape: [1024, 8192]
20: model.layers.10.input_layernorm.weight shape: [8192]
21: model.layers.10.mlp.down_proj.weight shape: [8192, 28672]
22: model.layers.10.mlp.gate_proj.weight shape: [28672, 8192]
23: model.layers.10.mlp.up_proj.weight shape: [28672, 8192]
24: model.layers.10.post_attention_layernorm.weight shape: [8192]
25: model.layers.10.self_attn.k_proj.weight shape: [1024, 8192]
26: model.layers.10.self_attn.o_proj.weight shape: [8192, 8192]
27: model.layers.10.self_attn.q_proj.weight shape: [8192, 8192]
28: model.layers.10.self_attn.v_proj.weight shape: [1024, 8192]
29: model.layers.11.input_layernorm.weight shape: [8192]
30: model.layers.11.mlp.down_proj.weight shape: [8192, 28672]
31: model.layers.11.mlp.gate_proj.weight shape: [28672, 8192]
32: model.layers.11.mlp.up_proj.weight shape: [28672, 8192]
33: model.layers.11.post_attention_layernorm.weight shape: [8192]
34: model.layers.11.self_attn.k_proj.weight shape: [1024, 8192]
35: model.layers.11.self_attn.o_proj.weight shape: [8192, 8192]
36: model.layers.11.self_attn.q_proj.weight shape: [8192, 8192]
37: model.layers.11.self_attn.v_proj.weight shape: [1024, 8192]
38: model.layers.12.input_layernorm.weight shape: [8192]
39: model.layers.12.mlp.down_proj.weight shape: [8192, 28672]
40: model.layers.12.mlp.gate_proj.weight shape: [28672, 8192]
41: model.layers.12.mlp.up_proj.weight shape: [28672, 8192]
42: model.layers.12.post_attention_layernorm.weight shape: [8192]
43: model.layers.12.self_attn.k_proj.weight shape: [1024, 8192]
44: model.layers.12.self_attn.o_proj.weight shape: [8192, 8192]
45: model.layers.12.self_attn.q_proj.weight shape: [8192, 8192]
46: model.layers.12.self_attn.v_proj.weight shape: [1024, 8192]
47: model.layers.13.input_layernorm.weight shape: [8192]
48: model.layers.13.mlp.down_proj.weight shape: [8192, 28672]
49: model.layers.13.mlp.gate_proj.weight shape: [28672, 8192]
50: model.layers.13.mlp.up_proj.weight shape: [28672, 8192]
51: model.layers.13.post_attention_layernorm.weight shape: [8192]
52: model.layers.13.self_attn.k_proj.weight shape: [1024, 8192]
53: model.layers.13.self_attn.o_proj.weight shape: [8192, 8192]
54: model.layers.13.self_attn.q_proj.weight shape: [8192, 8192]
55: model.layers.13.self_attn.v_proj.weight shape: [1024, 8192]
56: model.layers.14.input_layernorm.weight shape: [8192]
57: model.layers.14.mlp.down_proj.weight shape: [8192, 28672]
58: model.layers.14.mlp.gate_proj.weight shape: [28672, 8192]
59: model.layers.14.mlp.up_proj.weight shape: [28672, 8192]
60: model.layers.14.post_attention_layernorm.weight shape: [8192]
61: model.layers.14.self_attn.k_proj.weight shape: [1024, 8192]
62: model.layers.14.self_attn.o_proj.weight shape: [8192, 8192]
63: model.layers.14.self_attn.q_proj.weight shape: [8192, 8192]
64: model.layers.14.self_attn.v_proj.weight shape: [1024, 8192]
65: model.layers.15.input_layernorm.weight shape: [8192]
66: model.layers.15.mlp.down_proj.weight shape: [8192, 28672]
67: model.layers.15.mlp.gate_proj.weight shape: [28672, 8192]
68: model.layers.15.mlp.up_proj.weight shape: [28672, 8192]
69: model.layers.15.post_attention_layernorm.weight shape: [8192]
70: model.layers.15.self_attn.k_proj.weight shape: [1024, 8192]
71: model.layers.15.self_attn.o_proj.weight shape: [8192, 8192]
72: model.layers.15.self_attn.q_proj.weight shape: [8192, 8192]
73: model.layers.15.self_attn.v_proj.weight shape: [1024, 8192]
74: model.layers.16.input_layernorm.weight shape: [8192]
75: model.layers.16.mlp.down_proj.weight shape: [8192, 28672]
76: model.layers.16.mlp.gate_proj.weight shape: [28672, 8192]
77: model.layers.16.mlp.up_proj.weight shape: [28672, 8192]
78: model.layers.16.post_attention_layernorm.weight shape: [8192]
79: model.layers.16.self_attn.k_proj.weight shape: [1024, 8192]
80: model.layers.16.self_attn.o_proj.weight shape: [8192, 8192]
81: model.layers.16.self_attn.q_proj.weight shape: [8192, 8192]
82: model.layers.16.self_attn.v_proj.weight shape: [1024, 8192]
83: model.layers.17.input_layernorm.weight shape: [8192]
84: model.layers.17.mlp.down_proj.weight shape: [8192, 28672]
85: model.layers.17.mlp.gate_proj.weight shape: [28672, 8192]
86: model.layers.17.mlp.up_proj.weight shape: [28672, 8192]
87: model.layers.17.post_attention_layernorm.weight shape: [8192]
88: model.layers.17.self_attn.k_proj.weight shape: [1024, 8192]
89: model.layers.17.self_attn.o_proj.weight shape: [8192, 8192]
90: model.layers.17.self_attn.q_proj.weight shape: [8192, 8192]
91: model.layers.17.self_attn.v_proj.weight shape: [1024, 8192]
92: model.layers.18.input_layernorm.weight shape: [8192]
93: model.layers.18.mlp.down_proj.weight shape: [8192, 28672]
94: model.layers.18.mlp.gate_proj.weight shape: [28672, 8192]
95: model.layers.18.mlp.up_proj.weight shape: [28672, 8192]
96: model.layers.18.post_attention_layernorm.weight shape: [8192]
97: model.layers.18.self_attn.k_proj.weight shape: [1024, 8192]
98: model.layers.18.self_attn.o_proj.weight shape: [8192, 8192]
99: model.layers.18.self_attn.q_proj.weight shape: [8192, 8192]
100: model.layers.18.self_attn.v_proj.weight shape: [1024, 8192]
101: model.layers.19.input_layernorm.weight shape: [8192]
102: model.layers.19.mlp.down_proj.weight shape: [8192, 28672]
103: model.layers.19.mlp.gate_proj.weight shape: [28672, 8192]
104: model.layers.19.mlp.up_proj.weight shape: [28672, 8192]
105: model.layers.19.post_attention_layernorm.weight shape: [8192]
106: model.layers.19.self_attn.k_proj.weight shape: [1024, 8192]
107: model.layers.19.self_attn.o_proj.weight shape: [8192, 8192]
108: model.layers.19.self_attn.q_proj.weight shape: [8192, 8192]
109: model.layers.19.self_attn.v_proj.weight shape: [1024, 8192]
110: model.layers.2.input_layernorm.weight shape: [8192]
111: model.layers.2.mlp.down_proj.weight shape: [8192, 28672]
112: model.layers.2.mlp.gate_proj.weight shape: [28672, 8192]
113: model.layers.2.mlp.up_proj.weight shape: [28672, 8192]
114: model.layers.2.post_attention_layernorm.weight shape: [8192]
115: model.layers.2.self_attn.k_proj.weight shape: [1024, 8192]
116: model.layers.2.self_attn.o_proj.weight shape: [8192, 8192]
117: model.layers.2.self_attn.q_proj.weight shape: [8192, 8192]
118: model.layers.2.self_attn.v_proj.weight shape: [1024, 8192]
119: model.layers.20.input_layernorm.weight shape: [8192]
120: model.layers.20.mlp.down_proj.weight shape: [8192, 28672]
121: model.layers.20.mlp.gate_proj.weight shape: [28672, 8192]
122: model.layers.20.mlp.up_proj.weight shape: [28672, 8192]
123: model.layers.20.post_attention_layernorm.weight shape: [8192]
124: model.layers.20.self_attn.k_proj.weight shape: [1024, 8192]
125: model.layers.20.self_attn.o_proj.weight shape: [8192, 8192]
126: model.layers.20.self_attn.q_proj.weight shape: [8192, 8192]
127: model.layers.20.self_attn.v_proj.weight shape: [1024, 8192]
128: model.layers.21.input_layernorm.weight shape: [8192]
129: model.layers.21.mlp.down_proj.weight shape: [8192, 28672]
130: model.layers.21.mlp.gate_proj.weight shape: [28672, 8192]
131: model.layers.21.mlp.up_proj.weight shape: [28672, 8192]
132: model.layers.21.post_attention_layernorm.weight shape: [8192]
133: model.layers.21.self_attn.k_proj.weight shape: [1024, 8192]
134: model.layers.21.self_attn.o_proj.weight shape: [8192, 8192]
135: model.layers.21.self_attn.q_proj.weight shape: [8192, 8192]
136: model.layers.21.self_attn.v_proj.weight shape: [1024, 8192]
137: model.layers.22.input_layernorm.weight shape: [8192]
138: model.layers.22.mlp.down_proj.weight shape: [8192, 28672]
139: model.layers.22.mlp.gate_proj.weight shape: [28672, 8192]
140: model.layers.22.mlp.up_proj.weight shape: [28672, 8192]
141: model.layers.22.post_attention_layernorm.weight shape: [8192]
142: model.layers.22.self_attn.k_proj.weight shape: [1024, 8192]
143: model.layers.22.self_attn.o_proj.weight shape: [8192, 8192]
144: model.layers.22.self_attn.q_proj.weight shape: [8192, 8192]
145: model.layers.22.self_attn.v_proj.weight shape: [1024, 8192]
146: model.layers.23.input_layernorm.weight shape: [8192]
147: model.layers.23.mlp.down_proj.weight shape: [8192, 28672]
148: model.layers.23.mlp.gate_proj.weight shape: [28672, 8192]
149: model.layers.23.mlp.up_proj.weight shape: [28672, 8192]
150: model.layers.23.post_attention_layernorm.weight shape: [8192]
151: model.layers.23.self_attn.k_proj.weight shape: [1024, 8192]
152: model.layers.23.self_attn.o_proj.weight shape: [8192, 8192]
153: model.layers.23.self_attn.q_proj.weight shape: [8192, 8192]
154: model.layers.23.self_attn.v_proj.weight shape: [1024, 8192]
155: model.layers.24.input_layernorm.weight shape: [8192]
156: model.layers.24.mlp.down_proj.weight shape: [8192, 28672]
157: model.layers.24.mlp.gate_proj.weight shape: [28672, 8192]
158: model.layers.24.mlp.up_proj.weight shape: [28672, 8192]
159: model.layers.24.post_attention_layernorm.weight shape: [8192]
160: model.layers.24.self_attn.k_proj.weight shape: [1024, 8192]
161: model.layers.24.self_attn.o_proj.weight shape: [8192, 8192]
162: model.layers.24.self_attn.q_proj.weight shape: [8192, 8192]
163: model.layers.24.self_attn.v_proj.weight shape: [1024, 8192]
164: model.layers.25.input_layernorm.weight shape: [8192]
165: model.layers.25.mlp.down_proj.weight shape: [8192, 28672]
166: model.layers.25.mlp.gate_proj.weight shape: [28672, 8192]
167: model.layers.25.mlp.up_proj.weight shape: [28672, 8192]
168: model.layers.25.post_attention_layernorm.weight shape: [8192]
169: model.layers.25.self_attn.k_proj.weight shape: [1024, 8192]
170: model.layers.25.self_attn.o_proj.weight shape: [8192, 8192]
171: model.layers.25.self_attn.q_proj.weight shape: [8192, 8192]
172: model.layers.25.self_attn.v_proj.weight shape: [1024, 8192]
173: model.layers.26.input_layernorm.weight shape: [8192]
174: model.layers.26.mlp.down_proj.weight shape: [8192, 28672]
175: model.layers.26.mlp.gate_proj.weight shape: [28672, 8192]
176: model.layers.26.mlp.up_proj.weight shape: [28672, 8192]
177: model.layers.26.post_attention_layernorm.weight shape: [8192]
178: model.layers.26.self_attn.k_proj.weight shape: [1024, 8192]
179: model.layers.26.self_attn.o_proj.weight shape: [8192, 8192]
180: model.layers.26.self_attn.q_proj.weight shape: [8192, 8192]
181: model.layers.26.self_attn.v_proj.weight shape: [1024, 8192]
182: model.layers.27.input_layernorm.weight shape: [8192]
183: model.layers.27.mlp.down_proj.weight shape: [8192, 28672]
184: model.layers.27.mlp.gate_proj.weight shape: [28672, 8192]
185: model.layers.27.mlp.up_proj.weight shape: [28672, 8192]
186: model.layers.27.post_attention_layernorm.weight shape: [8192]
187: model.layers.27.self_attn.k_proj.weight shape: [1024, 8192]
188: model.layers.27.self_attn.o_proj.weight shape: [8192, 8192]
189: model.layers.27.self_attn.q_proj.weight shape: [8192, 8192]
190: model.layers.27.self_attn.v_proj.weight shape: [1024, 8192]
191: model.layers.28.input_layernorm.weight shape: [8192]
192: model.layers.28.mlp.down_proj.weight shape: [8192, 28672]
193: model.layers.28.mlp.gate_proj.weight shape: [28672, 8192]
194: model.layers.28.mlp.up_proj.weight shape: [28672, 8192]
195: model.layers.28.post_attention_layernorm.weight shape: [8192]
196: model.layers.28.self_attn.k_proj.weight shape: [1024, 8192]
197: model.layers.28.self_attn.o_proj.weight shape: [8192, 8192]
198: model.layers.28.self_attn.q_proj.weight shape: [8192, 8192]
199: model.layers.28.self_attn.v_proj.weight shape: [1024, 8192]
200: model.layers.29.input_layernorm.weight shape: [8192]
201: model.layers.29.mlp.down_proj.weight shape: [8192, 28672]
202: model.layers.29.mlp.gate_proj.weight shape: [28672, 8192]
203: model.layers.29.mlp.up_proj.weight shape: [28672, 8192]
204: model.layers.29.post_attention_layernorm.weight shape: [8192]
205: model.layers.29.self_attn.k_proj.weight shape: [1024, 8192]
206: model.layers.29.self_attn.o_proj.weight shape: [8192, 8192]
207: model.layers.29.self_attn.q_proj.weight shape: [8192, 8192]
208: model.layers.29.self_attn.v_proj.weight shape: [1024, 8192]
209: model.layers.3.input_layernorm.weight shape: [8192]
210: model.layers.3.mlp.down_proj.weight shape: [8192, 28672]
211: model.layers.3.mlp.gate_proj.weight shape: [28672, 8192]
212: model.layers.3.mlp.up_proj.weight shape: [28672, 8192]
213: model.layers.3.post_attention_layernorm.weight shape: [8192]
214: model.layers.3.self_attn.k_proj.weight shape: [1024, 8192]
215: model.layers.3.self_attn.o_proj.weight shape: [8192, 8192]
216: model.layers.3.self_attn.q_proj.weight shape: [8192, 8192]
217: model.layers.3.self_attn.v_proj.weight shape: [1024, 8192]
218: model.layers.30.input_layernorm.weight shape: [8192]
219: model.layers.30.mlp.down_proj.weight shape: [8192, 28672]
220: model.layers.30.mlp.gate_proj.weight shape: [28672, 8192]
221: model.layers.30.mlp.up_proj.weight shape: [28672, 8192]
222: model.layers.30.post_attention_layernorm.weight shape: [8192]
223: model.layers.30.self_attn.k_proj.weight shape: [1024, 8192]
224: model.layers.30.self_attn.o_proj.weight shape: [8192, 8192]
225: model.layers.30.self_attn.q_proj.weight shape: [8192, 8192]
226: model.layers.30.self_attn.v_proj.weight shape: [1024, 8192]
227: model.layers.31.input_layernorm.weight shape: [8192]
228: model.layers.31.mlp.down_proj.weight shape: [8192, 28672]
229: model.layers.31.mlp.gate_proj.weight shape: [28672, 8192]
230: model.layers.31.mlp.up_proj.weight shape: [28672, 8192]
231: model.layers.31.post_attention_layernorm.weight shape: [8192]
232: model.layers.31.self_attn.k_proj.weight shape: [1024, 8192]
233: model.layers.31.self_attn.o_proj.weight shape: [8192, 8192]
234: model.layers.31.self_attn.q_proj.weight shape: [8192, 8192]
235: model.layers.31.self_attn.v_proj.weight shape: [1024, 8192]
236: model.layers.32.input_layernorm.weight shape: [8192]
237: model.layers.32.mlp.down_proj.weight shape: [8192, 28672]
238: model.layers.32.mlp.gate_proj.weight shape: [28672, 8192]
239: model.layers.32.mlp.up_proj.weight shape: [28672, 8192]
240: model.layers.32.post_attention_layernorm.weight shape: [8192]
241: model.layers.32.self_attn.k_proj.weight shape: [1024, 8192]
242: model.layers.32.self_attn.o_proj.weight shape: [8192, 8192]
243: model.layers.32.self_attn.q_proj.weight shape: [8192, 8192]
244: model.layers.32.self_attn.v_proj.weight shape: [1024, 8192]
245: model.layers.33.input_layernorm.weight shape: [8192]
246: model.layers.33.mlp.down_proj.weight shape: [8192, 28672]
247: model.layers.33.mlp.gate_proj.weight shape: [28672, 8192]
248: model.layers.33.mlp.up_proj.weight shape: [28672, 8192]
249: model.layers.33.post_attention_layernorm.weight shape: [8192]
250: model.layers.33.self_attn.k_proj.weight shape: [1024, 8192]
251: model.layers.33.self_attn.o_proj.weight shape: [8192, 8192]
252: model.layers.33.self_attn.q_proj.weight shape: [8192, 8192]
253: model.layers.33.self_attn.v_proj.weight shape: [1024, 8192]
254: model.layers.34.input_layernorm.weight shape: [8192]
255: model.layers.34.mlp.down_proj.weight shape: [8192, 28672]
256: model.layers.34.mlp.gate_proj.weight shape: [28672, 8192]
257: model.layers.34.mlp.up_proj.weight shape: [28672, 8192]
258: model.layers.34.post_attention_layernorm.weight shape: [8192]
259: model.layers.34.self_attn.k_proj.weight shape: [1024, 8192]
260: model.layers.34.self_attn.o_proj.weight shape: [8192, 8192]
261: model.layers.34.self_attn.q_proj.weight shape: [8192, 8192]
262: model.layers.34.self_attn.v_proj.weight shape: [1024, 8192]
263: model.layers.35.input_layernorm.weight shape: [8192]
264: model.layers.35.mlp.down_proj.weight shape: [8192, 28672]
265: model.layers.35.mlp.gate_proj.weight shape: [28672, 8192]
266: model.layers.35.mlp.up_proj.weight shape: [28672, 8192]
267: model.layers.35.post_attention_layernorm.weight shape: [8192]
268: model.layers.35.self_attn.k_proj.weight shape: [1024, 8192]
269: model.layers.35.self_attn.o_proj.weight shape: [8192, 8192]
270: model.layers.35.self_attn.q_proj.weight shape: [8192, 8192]
271: model.layers.35.self_attn.v_proj.weight shape: [1024, 8192]
272: model.layers.36.input_layernorm.weight shape: [8192]
273: model.layers.36.mlp.down_proj.weight shape: [8192, 28672]
274: model.layers.36.mlp.gate_proj.weight shape: [28672, 8192]
275: model.layers.36.mlp.up_proj.weight shape: [28672, 8192]
276: model.layers.36.post_attention_layernorm.weight shape: [8192]
277: model.layers.36.self_attn.k_proj.weight shape: [1024, 8192]
278: model.layers.36.self_attn.o_proj.weight shape: [8192, 8192]
279: model.layers.36.self_attn.q_proj.weight shape: [8192, 8192]
280: model.layers.36.self_attn.v_proj.weight shape: [1024, 8192]
281: model.layers.37.input_layernorm.weight shape: [8192]
282: model.layers.37.mlp.down_proj.weight shape: [8192, 28672]
283: model.layers.37.mlp.gate_proj.weight shape: [28672, 8192]
284: model.layers.37.mlp.up_proj.weight shape: [28672, 8192]
285: model.layers.37.post_attention_layernorm.weight shape: [8192]
286: model.layers.37.self_attn.k_proj.weight shape: [1024, 8192]
287: model.layers.37.self_attn.o_proj.weight shape: [8192, 8192]
288: model.layers.37.self_attn.q_proj.weight shape: [8192, 8192]
289: model.layers.37.self_attn.v_proj.weight shape: [1024, 8192]
290: model.layers.38.input_layernorm.weight shape: [8192]
291: model.layers.38.mlp.down_proj.weight shape: [8192, 28672]
292: model.layers.38.mlp.gate_proj.weight shape: [28672, 8192]
293: model.layers.38.mlp.up_proj.weight shape: [28672, 8192]
294: model.layers.38.post_attention_layernorm.weight shape: [8192]
295: model.layers.38.self_attn.k_proj.weight shape: [1024, 8192]
296: model.layers.38.self_attn.o_proj.weight shape: [8192, 8192]
297: model.layers.38.self_attn.q_proj.weight shape: [8192, 8192]
298: model.layers.38.self_attn.v_proj.weight shape: [1024, 8192]
299: model.layers.39.input_layernorm.weight shape: [8192]
300: model.layers.39.mlp.down_proj.weight shape: [8192, 28672]
301: model.layers.39.mlp.gate_proj.weight shape: [28672, 8192]
302: model.layers.39.mlp.up_proj.weight shape: [28672, 8192]
303: model.layers.39.post_attention_layernorm.weight shape: [8192]
304: model.layers.39.self_attn.k_proj.weight shape: [1024, 8192]
305: model.layers.39.self_attn.o_proj.weight shape: [8192, 8192]
306: model.layers.39.self_attn.q_proj.weight shape: [8192, 8192]
307: model.layers.39.self_attn.v_proj.weight shape: [1024, 8192]
308: model.layers.4.input_layernorm.weight shape: [8192]
309: model.layers.4.mlp.down_proj.weight shape: [8192, 28672]
310: model.layers.4.mlp.gate_proj.weight shape: [28672, 8192]
311: model.layers.4.mlp.up_proj.weight shape: [28672, 8192]
312: model.layers.4.post_attention_layernorm.weight shape: [8192]
313: model.layers.4.self_attn.k_proj.weight shape: [1024, 8192]
314: model.layers.4.self_attn.o_proj.weight shape: [8192, 8192]
315: model.layers.4.self_attn.q_proj.weight shape: [8192, 8192]
316: model.layers.4.self_attn.v_proj.weight shape: [1024, 8192]
317: model.layers.40.input_layernorm.weight shape: [8192]
318: model.layers.40.mlp.down_proj.weight shape: [8192, 28672]
319: model.layers.40.mlp.gate_proj.weight shape: [28672, 8192]
320: model.layers.40.mlp.up_proj.weight shape: [28672, 8192]
321: model.layers.40.post_attention_layernorm.weight shape: [8192]
322: model.layers.40.self_attn.k_proj.weight shape: [1024, 8192]
323: model.layers.40.self_attn.o_proj.weight shape: [8192, 8192]
324: model.layers.40.self_attn.q_proj.weight shape: [8192, 8192]
325: model.layers.40.self_attn.v_proj.weight shape: [1024, 8192]
326: model.layers.41.input_layernorm.weight shape: [8192]
327: model.layers.41.mlp.down_proj.weight shape: [8192, 28672]
328: model.layers.41.mlp.gate_proj.weight shape: [28672, 8192]
329: model.layers.41.mlp.up_proj.weight shape: [28672, 8192]
330: model.layers.41.post_attention_layernorm.weight shape: [8192]
331: model.layers.41.self_attn.k_proj.weight shape: [1024, 8192]
332: model.layers.41.self_attn.o_proj.weight shape: [8192, 8192]
333: model.layers.41.self_attn.q_proj.weight shape: [8192, 8192]
334: model.layers.41.self_attn.v_proj.weight shape: [1024, 8192]
335: model.layers.42.input_layernorm.weight shape: [8192]
336: model.layers.42.mlp.down_proj.weight shape: [8192, 28672]
337: model.layers.42.mlp.gate_proj.weight shape: [28672, 8192]
338: model.layers.42.mlp.up_proj.weight shape: [28672, 8192]
339: model.layers.42.post_attention_layernorm.weight shape: [8192]
340: model.layers.42.self_attn.k_proj.weight shape: [1024, 8192]
341: model.layers.42.self_attn.o_proj.weight shape: [8192, 8192]
342: model.layers.42.self_attn.q_proj.weight shape: [8192, 8192]
343: model.layers.42.self_attn.v_proj.weight shape: [1024, 8192]
344: model.layers.43.input_layernorm.weight shape: [8192]
345: model.layers.43.mlp.down_proj.weight shape: [8192, 28672]
346: model.layers.43.mlp.gate_proj.weight shape: [28672, 8192]
347: model.layers.43.mlp.up_proj.weight shape: [28672, 8192]
348: model.layers.43.post_attention_layernorm.weight shape: [8192]
349: model.layers.43.self_attn.k_proj.weight shape: [1024, 8192]
350: model.layers.43.self_attn.o_proj.weight shape: [8192, 8192]
351: model.layers.43.self_attn.q_proj.weight shape: [8192, 8192]
352: model.layers.43.self_attn.v_proj.weight shape: [1024, 8192]
353: model.layers.44.input_layernorm.weight shape: [8192]
354: model.layers.44.mlp.down_proj.weight shape: [8192, 28672]
355: model.layers.44.mlp.gate_proj.weight shape: [28672, 8192]
356: model.layers.44.mlp.up_proj.weight shape: [28672, 8192]
357: model.layers.44.post_attention_layernorm.weight shape: [8192]
358: model.layers.44.self_attn.k_proj.weight shape: [1024, 8192]
359: model.layers.44.self_attn.o_proj.weight shape: [8192, 8192]
360: model.layers.44.self_attn.q_proj.weight shape: [8192, 8192]
361: model.layers.44.self_attn.v_proj.weight shape: [1024, 8192]
362: model.layers.45.input_layernorm.weight shape: [8192]
363: model.layers.45.mlp.down_proj.weight shape: [8192, 28672]
364: model.layers.45.mlp.gate_proj.weight shape: [28672, 8192]
365: model.layers.45.mlp.up_proj.weight shape: [28672, 8192]
366: model.layers.45.post_attention_layernorm.weight shape: [8192]
367: model.layers.45.self_attn.k_proj.weight shape: [1024, 8192]
368: model.layers.45.self_attn.o_proj.weight shape: [8192, 8192]
369: model.layers.45.self_attn.q_proj.weight shape: [8192, 8192]
370: model.layers.45.self_attn.v_proj.weight shape: [1024, 8192]
371: model.layers.46.input_layernorm.weight shape: [8192]
372: model.layers.46.mlp.down_proj.weight shape: [8192, 28672]
373: model.layers.46.mlp.gate_proj.weight shape: [28672, 8192]
374: model.layers.46.mlp.up_proj.weight shape: [28672, 8192]
375: model.layers.46.post_attention_layernorm.weight shape: [8192]
376: model.layers.46.self_attn.k_proj.weight shape: [1024, 8192]
377: model.layers.46.self_attn.o_proj.weight shape: [8192, 8192]
378: model.layers.46.self_attn.q_proj.weight shape: [8192, 8192]
379: model.layers.46.self_attn.v_proj.weight shape: [1024, 8192]
380: model.layers.47.input_layernorm.weight shape: [8192]
381: model.layers.47.mlp.down_proj.weight shape: [8192, 28672]
382: model.layers.47.mlp.gate_proj.weight shape: [28672, 8192]
383: model.layers.47.mlp.up_proj.weight shape: [28672, 8192]
384: model.layers.47.post_attention_layernorm.weight shape: [8192]
385: model.layers.47.self_attn.k_proj.weight shape: [1024, 8192]
386: model.layers.47.self_attn.o_proj.weight shape: [8192, 8192]
387: model.layers.47.self_attn.q_proj.weight shape: [8192, 8192]
388: model.layers.47.self_attn.v_proj.weight shape: [1024, 8192]
389: model.layers.48.input_layernorm.weight shape: [8192]
390: model.layers.48.mlp.down_proj.weight shape: [8192, 28672]
391: model.layers.48.mlp.gate_proj.weight shape: [28672, 8192]
392: model.layers.48.mlp.up_proj.weight shape: [28672, 8192]
393: model.layers.48.post_attention_layernorm.weight shape: [8192]
394: model.layers.48.self_attn.k_proj.weight shape: [1024, 8192]
395: model.layers.48.self_attn.o_proj.weight shape: [8192, 8192]
396: model.layers.48.self_attn.q_proj.weight shape: [8192, 8192]
397: model.layers.48.self_attn.v_proj.weight shape: [1024, 8192]
398: model.layers.49.input_layernorm.weight shape: [8192]
399: model.layers.49.mlp.down_proj.weight shape: [8192, 28672]
400: model.layers.49.mlp.gate_proj.weight shape: [28672, 8192]
401: model.layers.49.mlp.up_proj.weight shape: [28672, 8192]
402: model.layers.49.post_attention_layernorm.weight shape: [8192]
403: model.layers.49.self_attn.k_proj.weight shape: [1024, 8192]
404: model.layers.49.self_attn.o_proj.weight shape: [8192, 8192]
405: model.layers.49.self_attn.q_proj.weight shape: [8192, 8192]
406: model.layers.49.self_attn.v_proj.weight shape: [1024, 8192]
407: model.layers.5.input_layernorm.weight shape: [8192]
408: model.layers.5.mlp.down_proj.weight shape: [8192, 28672]
409: model.layers.5.mlp.gate_proj.weight shape: [28672, 8192]
410: model.layers.5.mlp.up_proj.weight shape: [28672, 8192]
411: model.layers.5.post_attention_layernorm.weight shape: [8192]
412: model.layers.5.self_attn.k_proj.weight shape: [1024, 8192]
413: model.layers.5.self_attn.o_proj.weight shape: [8192, 8192]
414: model.layers.5.self_attn.q_proj.weight shape: [8192, 8192]
415: model.layers.5.self_attn.v_proj.weight shape: [1024, 8192]
416: model.layers.50.input_layernorm.weight shape: [8192]
417: model.layers.50.mlp.down_proj.weight shape: [8192, 28672]
418: model.layers.50.mlp.gate_proj.weight shape: [28672, 8192]
419: model.layers.50.mlp.up_proj.weight shape: [28672, 8192]
420: model.layers.50.post_attention_layernorm.weight shape: [8192]
421: model.layers.50.self_attn.k_proj.weight shape: [1024, 8192]
422: model.layers.50.self_attn.o_proj.weight shape: [8192, 8192]
423: model.layers.50.self_attn.q_proj.weight shape: [8192, 8192]
424: model.layers.50.self_attn.v_proj.weight shape: [1024, 8192]
425: model.layers.51.input_layernorm.weight shape: [8192]
426: model.layers.51.mlp.down_proj.weight shape: [8192, 28672]
427: model.layers.51.mlp.gate_proj.weight shape: [28672, 8192]
428: model.layers.51.mlp.up_proj.weight shape: [28672, 8192]
429: model.layers.51.post_attention_layernorm.weight shape: [8192]
430: model.layers.51.self_attn.k_proj.weight shape: [1024, 8192]
431: model.layers.51.self_attn.o_proj.weight shape: [8192, 8192]
432: model.layers.51.self_attn.q_proj.weight shape: [8192, 8192]
433: model.layers.51.self_attn.v_proj.weight shape: [1024, 8192]
434: model.layers.52.input_layernorm.weight shape: [8192]
435: model.layers.52.mlp.down_proj.weight shape: [8192, 28672]
436: model.layers.52.mlp.gate_proj.weight shape: [28672, 8192]
437: model.layers.52.mlp.up_proj.weight shape: [28672, 8192]
438: model.layers.52.post_attention_layernorm.weight shape: [8192]
439: model.layers.52.self_attn.k_proj.weight shape: [1024, 8192]
440: model.layers.52.self_attn.o_proj.weight shape: [8192, 8192]
441: model.layers.52.self_attn.q_proj.weight shape: [8192, 8192]
442: model.layers.52.self_attn.v_proj.weight shape: [1024, 8192]
443: model.layers.53.input_layernorm.weight shape: [8192]
444: model.layers.53.mlp.down_proj.weight shape: [8192, 28672]
445: model.layers.53.mlp.gate_proj.weight shape: [28672, 8192]
446: model.layers.53.mlp.up_proj.weight shape: [28672, 8192]
447: model.layers.53.post_attention_layernorm.weight shape: [8192]
448: model.layers.53.self_attn.k_proj.weight shape: [1024, 8192]
449: model.layers.53.self_attn.o_proj.weight shape: [8192, 8192]
450: model.layers.53.self_attn.q_proj.weight shape: [8192, 8192]
451: model.layers.53.self_attn.v_proj.weight shape: [1024, 8192]
452: model.layers.54.input_layernorm.weight shape: [8192]
453: model.layers.54.mlp.down_proj.weight shape: [8192, 28672]
454: model.layers.54.mlp.gate_proj.weight shape: [28672, 8192]
455: model.layers.54.mlp.up_proj.weight shape: [28672, 8192]
456: model.layers.54.post_attention_layernorm.weight shape: [8192]
457: model.layers.54.self_attn.k_proj.weight shape: [1024, 8192]
458: model.layers.54.self_attn.o_proj.weight shape: [8192, 8192]
459: model.layers.54.self_attn.q_proj.weight shape: [8192, 8192]
460: model.layers.54.self_attn.v_proj.weight shape: [1024, 8192]
461: model.layers.55.input_layernorm.weight shape: [8192]
462: model.layers.55.mlp.down_proj.weight shape: [8192, 28672]
463: model.layers.55.mlp.gate_proj.weight shape: [28672, 8192]
464: model.layers.55.mlp.up_proj.weight shape: [28672, 8192]
465: model.layers.55.post_attention_layernorm.weight shape: [8192]
466: model.layers.55.self_attn.k_proj.weight shape: [1024, 8192]
467: model.layers.55.self_attn.o_proj.weight shape: [8192, 8192]
468: model.layers.55.self_attn.q_proj.weight shape: [8192, 8192]
469: model.layers.55.self_attn.v_proj.weight shape: [1024, 8192]
470: model.layers.56.input_layernorm.weight shape: [8192]
471: model.layers.56.mlp.down_proj.weight shape: [8192, 28672]
472: model.layers.56.mlp.gate_proj.weight shape: [28672, 8192]
473: model.layers.56.mlp.up_proj.weight shape: [28672, 8192]
474: model.layers.56.post_attention_layernorm.weight shape: [8192]
475: model.layers.56.self_attn.k_proj.weight shape: [1024, 8192]
476: model.layers.56.self_attn.o_proj.weight shape: [8192, 8192]
477: model.layers.56.self_attn.q_proj.weight shape: [8192, 8192]
478: model.layers.56.self_attn.v_proj.weight shape: [1024, 8192]
479: model.layers.57.input_layernorm.weight shape: [8192]
480: model.layers.57.mlp.down_proj.weight shape: [8192, 28672]
481: model.layers.57.mlp.gate_proj.weight shape: [28672, 8192]
482: model.layers.57.mlp.up_proj.weight shape: [28672, 8192]
483: model.layers.57.post_attention_layernorm.weight shape: [8192]
484: model.layers.57.self_attn.k_proj.weight shape: [1024, 8192]
485: model.layers.57.self_attn.o_proj.weight shape: [8192, 8192]
486: model.layers.57.self_attn.q_proj.weight shape: [8192, 8192]
487: model.layers.57.self_attn.v_proj.weight shape: [1024, 8192]
488: model.layers.58.input_layernorm.weight shape: [8192]
489: model.layers.58.mlp.down_proj.weight shape: [8192, 28672]
490: model.layers.58.mlp.gate_proj.weight shape: [28672, 8192]
491: model.layers.58.mlp.up_proj.weight shape: [28672, 8192]
492: model.layers.58.post_attention_layernorm.weight shape: [8192]
493: model.layers.58.self_attn.k_proj.weight shape: [1024, 8192]
494: model.layers.58.self_attn.o_proj.weight shape: [8192, 8192]
495: model.layers.58.self_attn.q_proj.weight shape: [8192, 8192]
496: model.layers.58.self_attn.v_proj.weight shape: [1024, 8192]
497: model.layers.59.input_layernorm.weight shape: [8192]
498: model.layers.59.mlp.down_proj.weight shape: [8192, 28672]
499: model.layers.59.mlp.gate_proj.weight shape: [28672, 8192]
500: model.layers.59.mlp.up_proj.weight shape: [28672, 8192]
501: model.layers.59.post_attention_layernorm.weight shape: [8192]
502: model.layers.59.self_attn.k_proj.weight shape: [1024, 8192]
503: model.layers.59.self_attn.o_proj.weight shape: [8192, 8192]
504: model.layers.59.self_attn.q_proj.weight shape: [8192, 8192]
505: model.layers.59.self_attn.v_proj.weight shape: [1024, 8192]
506: model.layers.6.input_layernorm.weight shape: [8192]
507: model.layers.6.mlp.down_proj.weight shape: [8192, 28672]
508: model.layers.6.mlp.gate_proj.weight shape: [28672, 8192]
509: model.layers.6.mlp.up_proj.weight shape: [28672, 8192]
510: model.layers.6.post_attention_layernorm.weight shape: [8192]
511: model.layers.6.self_attn.k_proj.weight shape: [1024, 8192]
512: model.layers.6.self_attn.o_proj.weight shape: [8192, 8192]
513: model.layers.6.self_attn.q_proj.weight shape: [8192, 8192]
514: model.layers.6.self_attn.v_proj.weight shape: [1024, 8192]
515: model.layers.60.input_layernorm.weight shape: [8192]
516: model.layers.60.mlp.down_proj.weight shape: [8192, 28672]
517: model.layers.60.mlp.gate_proj.weight shape: [28672, 8192]
518: model.layers.60.mlp.up_proj.weight shape: [28672, 8192]
519: model.layers.60.post_attention_layernorm.weight shape: [8192]
520: model.layers.60.self_attn.k_proj.weight shape: [1024, 8192]
521: model.layers.60.self_attn.o_proj.weight shape: [8192, 8192]
522: model.layers.60.self_attn.q_proj.weight shape: [8192, 8192]
523: model.layers.60.self_attn.v_proj.weight shape: [1024, 8192]
524: model.layers.61.input_layernorm.weight shape: [8192]
525: model.layers.61.mlp.down_proj.weight shape: [8192, 28672]
526: model.layers.61.mlp.gate_proj.weight shape: [28672, 8192]
527: model.layers.61.mlp.up_proj.weight shape: [28672, 8192]
528: model.layers.61.post_attention_layernorm.weight shape: [8192]
529: model.layers.61.self_attn.k_proj.weight shape: [1024, 8192]
530: model.layers.61.self_attn.o_proj.weight shape: [8192, 8192]
531: model.layers.61.self_attn.q_proj.weight shape: [8192, 8192]
532: model.layers.61.self_attn.v_proj.weight shape: [1024, 8192]
533: model.layers.62.input_layernorm.weight shape: [8192]
534: model.layers.62.mlp.down_proj.weight shape: [8192, 28672]
535: model.layers.62.mlp.gate_proj.weight shape: [28672, 8192]
536: model.layers.62.mlp.up_proj.weight shape: [28672, 8192]
537: model.layers.62.post_attention_layernorm.weight shape: [8192]
538: model.layers.62.self_attn.k_proj.weight shape: [1024, 8192]
539: model.layers.62.self_attn.o_proj.weight shape: [8192, 8192]
540: model.layers.62.self_attn.q_proj.weight shape: [8192, 8192]
541: model.layers.62.self_attn.v_proj.weight shape: [1024, 8192]
542: model.layers.63.input_layernorm.weight shape: [8192]
543: model.layers.63.mlp.down_proj.weight shape: [8192, 28672]
544: model.layers.63.mlp.gate_proj.weight shape: [28672, 8192]
545: model.layers.63.mlp.up_proj.weight shape: [28672, 8192]
546: model.layers.63.post_attention_layernorm.weight shape: [8192]
547: model.layers.63.self_attn.k_proj.weight shape: [1024, 8192]
548: model.layers.63.self_attn.o_proj.weight shape: [8192, 8192]
549: model.layers.63.self_attn.q_proj.weight shape: [8192, 8192]
550: model.layers.63.self_attn.v_proj.weight shape: [1024, 8192]
551: model.layers.64.input_layernorm.weight shape: [8192]
552: model.layers.64.mlp.down_proj.weight shape: [8192, 28672]
553: model.layers.64.mlp.gate_proj.weight shape: [28672, 8192]
554: model.layers.64.mlp.up_proj.weight shape: [28672, 8192]
555: model.layers.64.post_attention_layernorm.weight shape: [8192]
556: model.layers.64.self_attn.k_proj.weight shape: [1024, 8192]
557: model.layers.64.self_attn.o_proj.weight shape: [8192, 8192]
558: model.layers.64.self_attn.q_proj.weight shape: [8192, 8192]
559: model.layers.64.self_attn.v_proj.weight shape: [1024, 8192]
560: model.layers.65.input_layernorm.weight shape: [8192]
561: model.layers.65.mlp.down_proj.weight shape: [8192, 28672]
562: model.layers.65.mlp.gate_proj.weight shape: [28672, 8192]
563: model.layers.65.mlp.up_proj.weight shape: [28672, 8192]
564: model.layers.65.post_attention_layernorm.weight shape: [8192]
565: model.layers.65.self_attn.k_proj.weight shape: [1024, 8192]
566: model.layers.65.self_attn.o_proj.weight shape: [8192, 8192]
567: model.layers.65.self_attn.q_proj.weight shape: [8192, 8192]
568: model.layers.65.self_attn.v_proj.weight shape: [1024, 8192]
569: model.layers.66.input_layernorm.weight shape: [8192]
570: model.layers.66.mlp.down_proj.weight shape: [8192, 28672]
571: model.layers.66.mlp.gate_proj.weight shape: [28672, 8192]
572: model.layers.66.mlp.up_proj.weight shape: [28672, 8192]
573: model.layers.66.post_attention_layernorm.weight shape: [8192]
574: model.layers.66.self_attn.k_proj.weight shape: [1024, 8192]
575: model.layers.66.self_attn.o_proj.weight shape: [8192, 8192]
576: model.layers.66.self_attn.q_proj.weight shape: [8192, 8192]
577: model.layers.66.self_attn.v_proj.weight shape: [1024, 8192]
578: model.layers.67.input_layernorm.weight shape: [8192]
579: model.layers.67.mlp.down_proj.weight shape: [8192, 28672]
580: model.layers.67.mlp.gate_proj.weight shape: [28672, 8192]
581: model.layers.67.mlp.up_proj.weight shape: [28672, 8192]
582: model.layers.67.post_attention_layernorm.weight shape: [8192]
583: model.layers.67.self_attn.k_proj.weight shape: [1024, 8192]
584: model.layers.67.self_attn.o_proj.weight shape: [8192, 8192]
585: model.layers.67.self_attn.q_proj.weight shape: [8192, 8192]
586: model.layers.67.self_attn.v_proj.weight shape: [1024, 8192]
587: model.layers.68.input_layernorm.weight shape: [8192]
588: model.layers.68.mlp.down_proj.weight shape: [8192, 28672]
589: model.layers.68.mlp.gate_proj.weight shape: [28672, 8192]
590: model.layers.68.mlp.up_proj.weight shape: [28672, 8192]
591: model.layers.68.post_attention_layernorm.weight shape: [8192]
592: model.layers.68.self_attn.k_proj.weight shape: [1024, 8192]
593: model.layers.68.self_attn.o_proj.weight shape: [8192, 8192]
594: model.layers.68.self_attn.q_proj.weight shape: [8192, 8192]
595: model.layers.68.self_attn.v_proj.weight shape: [1024, 8192]
596: model.layers.69.input_layernorm.weight shape: [8192]
597: model.layers.69.mlp.down_proj.weight shape: [8192, 28672]
598: model.layers.69.mlp.gate_proj.weight shape: [28672, 8192]
599: model.layers.69.mlp.up_proj.weight shape: [28672, 8192]
600: model.layers.69.post_attention_layernorm.weight shape: [8192]
601: model.layers.69.self_attn.k_proj.weight shape: [1024, 8192]
602: model.layers.69.self_attn.o_proj.weight shape: [8192, 8192]
603: model.layers.69.self_attn.q_proj.weight shape: [8192, 8192]
604: model.layers.69.self_attn.v_proj.weight shape: [1024, 8192]
605: model.layers.7.input_layernorm.weight shape: [8192]
606: model.layers.7.mlp.down_proj.weight shape: [8192, 28672]
607: model.layers.7.mlp.gate_proj.weight shape: [28672, 8192]
608: model.layers.7.mlp.up_proj.weight shape: [28672, 8192]
609: model.layers.7.post_attention_layernorm.weight shape: [8192]
610: model.layers.7.self_attn.k_proj.weight shape: [1024, 8192]
611: model.layers.7.self_attn.o_proj.weight shape: [8192, 8192]
612: model.layers.7.self_attn.q_proj.weight shape: [8192, 8192]
613: model.layers.7.self_attn.v_proj.weight shape: [1024, 8192]
614: model.layers.70.input_layernorm.weight shape: [8192]
615: model.layers.70.mlp.down_proj.weight shape: [8192, 28672]
616: model.layers.70.mlp.gate_proj.weight shape: [28672, 8192]
617: model.layers.70.mlp.up_proj.weight shape: [28672, 8192]
618: model.layers.70.post_attention_layernorm.weight shape: [8192]
619: model.layers.70.self_attn.k_proj.weight shape: [1024, 8192]
620: model.layers.70.self_attn.o_proj.weight shape: [8192, 8192]
621: model.layers.70.self_attn.q_proj.weight shape: [8192, 8192]
622: model.layers.70.self_attn.v_proj.weight shape: [1024, 8192]
623: model.layers.71.input_layernorm.weight shape: [8192]
624: model.layers.71.mlp.down_proj.weight shape: [8192, 28672]
625: model.layers.71.mlp.gate_proj.weight shape: [28672, 8192]
626: model.layers.71.mlp.up_proj.weight shape: [28672, 8192]
627: model.layers.71.post_attention_layernorm.weight shape: [8192]
628: model.layers.71.self_attn.k_proj.weight shape: [1024, 8192]
629: model.layers.71.self_attn.o_proj.weight shape: [8192, 8192]
630: model.layers.71.self_attn.q_proj.weight shape: [8192, 8192]
631: model.layers.71.self_attn.v_proj.weight shape: [1024, 8192]
632: model.layers.72.input_layernorm.weight shape: [8192]
633: model.layers.72.mlp.down_proj.weight shape: [8192, 28672]
634: model.layers.72.mlp.gate_proj.weight shape: [28672, 8192]
635: model.layers.72.mlp.up_proj.weight shape: [28672, 8192]
636: model.layers.72.post_attention_layernorm.weight shape: [8192]
637: model.layers.72.self_attn.k_proj.weight shape: [1024, 8192]
638: model.layers.72.self_attn.o_proj.weight shape: [8192, 8192]
639: model.layers.72.self_attn.q_proj.weight shape: [8192, 8192]
640: model.layers.72.self_attn.v_proj.weight shape: [1024, 8192]
641: model.layers.73.input_layernorm.weight shape: [8192]
642: model.layers.73.mlp.down_proj.weight shape: [8192, 28672]
643: model.layers.73.mlp.gate_proj.weight shape: [28672, 8192]
644: model.layers.73.mlp.up_proj.weight shape: [28672, 8192]
645: model.layers.73.post_attention_layernorm.weight shape: [8192]
646: model.layers.73.self_attn.k_proj.weight shape: [1024, 8192]
647: model.layers.73.self_attn.o_proj.weight shape: [8192, 8192]
648: model.layers.73.self_attn.q_proj.weight shape: [8192, 8192]
649: model.layers.73.self_attn.v_proj.weight shape: [1024, 8192]
650: model.layers.74.input_layernorm.weight shape: [8192]
651: model.layers.74.mlp.down_proj.weight shape: [8192, 28672]
652: model.layers.74.mlp.gate_proj.weight shape: [28672, 8192]
653: model.layers.74.mlp.up_proj.weight shape: [28672, 8192]
654: model.layers.74.post_attention_layernorm.weight shape: [8192]
655: model.layers.74.self_attn.k_proj.weight shape: [1024, 8192]
656: model.layers.74.self_attn.o_proj.weight shape: [8192, 8192]
657: model.layers.74.self_attn.q_proj.weight shape: [8192, 8192]
658: model.layers.74.self_attn.v_proj.weight shape: [1024, 8192]
659: model.layers.75.input_layernorm.weight shape: [8192]
660: model.layers.75.mlp.down_proj.weight shape: [8192, 28672]
661: model.layers.75.mlp.gate_proj.weight shape: [28672, 8192]
662: model.layers.75.mlp.up_proj.weight shape: [28672, 8192]
663: model.layers.75.post_attention_layernorm.weight shape: [8192]
664: model.layers.75.self_attn.k_proj.weight shape: [1024, 8192]
665: model.layers.75.self_attn.o_proj.weight shape: [8192, 8192]
666: model.layers.75.self_attn.q_proj.weight shape: [8192, 8192]
667: model.layers.75.self_attn.v_proj.weight shape: [1024, 8192]
668: model.layers.76.input_layernorm.weight shape: [8192]
669: model.layers.76.mlp.down_proj.weight shape: [8192, 28672]
670: model.layers.76.mlp.gate_proj.weight shape: [28672, 8192]
671: model.layers.76.mlp.up_proj.weight shape: [28672, 8192]
672: model.layers.76.post_attention_layernorm.weight shape: [8192]
673: model.layers.76.self_attn.k_proj.weight shape: [1024, 8192]
674: model.layers.76.self_attn.o_proj.weight shape: [8192, 8192]
675: model.layers.76.self_attn.q_proj.weight shape: [8192, 8192]
676: model.layers.76.self_attn.v_proj.weight shape: [1024, 8192]
677: model.layers.77.input_layernorm.weight shape: [8192]
678: model.layers.77.mlp.down_proj.weight shape: [8192, 28672]
679: model.layers.77.mlp.gate_proj.weight shape: [28672, 8192]
680: model.layers.77.mlp.up_proj.weight shape: [28672, 8192]
681: model.layers.77.post_attention_layernorm.weight shape: [8192]
682: model.layers.77.self_attn.k_proj.weight shape: [1024, 8192]
683: model.layers.77.self_attn.o_proj.weight shape: [8192, 8192]
684: model.layers.77.self_attn.q_proj.weight shape: [8192, 8192]
685: model.layers.77.self_attn.v_proj.weight shape: [1024, 8192]
686: model.layers.78.input_layernorm.weight shape: [8192]
687: model.layers.78.mlp.down_proj.weight shape: [8192, 28672]
688: model.layers.78.mlp.gate_proj.weight shape: [28672, 8192]
689: model.layers.78.mlp.up_proj.weight shape: [28672, 8192]
690: model.layers.78.post_attention_layernorm.weight shape: [8192]
691: model.layers.78.self_attn.k_proj.weight shape: [1024, 8192]
692: model.layers.78.self_attn.o_proj.weight shape: [8192, 8192]
693: model.layers.78.self_attn.q_proj.weight shape: [8192, 8192]
694: model.layers.78.self_attn.v_proj.weight shape: [1024, 8192]
695: model.layers.79.input_layernorm.weight shape: [8192]
696: model.layers.79.mlp.down_proj.weight shape: [8192, 28672]
697: model.layers.79.mlp.gate_proj.weight shape: [28672, 8192]
698: model.layers.79.mlp.up_proj.weight shape: [28672, 8192]
699: model.layers.79.post_attention_layernorm.weight shape: [8192]
700: model.layers.79.self_attn.k_proj.weight shape: [1024, 8192]
701: model.layers.79.self_attn.o_proj.weight shape: [8192, 8192]
702: model.layers.79.self_attn.q_proj.weight shape: [8192, 8192]
703: model.layers.79.self_attn.v_proj.weight shape: [1024, 8192]
704: model.layers.8.input_layernorm.weight shape: [8192]
705: model.layers.8.mlp.down_proj.weight shape: [8192, 28672]
706: model.layers.8.mlp.gate_proj.weight shape: [28672, 8192]
707: model.layers.8.mlp.up_proj.weight shape: [28672, 8192]
708: model.layers.8.post_attention_layernorm.weight shape: [8192]
709: model.layers.8.self_attn.k_proj.weight shape: [1024, 8192]
710: model.layers.8.self_attn.o_proj.weight shape: [8192, 8192]
711: model.layers.8.self_attn.q_proj.weight shape: [8192, 8192]
712: model.layers.8.self_attn.v_proj.weight shape: [1024, 8192]
713: model.layers.9.input_layernorm.weight shape: [8192]
714: model.layers.9.mlp.down_proj.weight shape: [8192, 28672]
715: model.layers.9.mlp.gate_proj.weight shape: [28672, 8192]
716: model.layers.9.mlp.up_proj.weight shape: [28672, 8192]
717: model.layers.9.post_attention_layernorm.weight shape: [8192]
718: model.layers.9.self_attn.k_proj.weight shape: [1024, 8192]
719: model.layers.9.self_attn.o_proj.weight shape: [8192, 8192]
720: model.layers.9.self_attn.q_proj.weight shape: [8192, 8192]
721: model.layers.9.self_attn.v_proj.weight shape: [1024, 8192]
722: model.norm.weight shape: [8192]
