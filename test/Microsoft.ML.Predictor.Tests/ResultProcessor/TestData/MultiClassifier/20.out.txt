maml.exe TrainTest test=F:\data\MNIST\Train-28x28.txt tr=MultiClassLogisticRegression{l2=0.1 ot=0.0001 m=5 initwts=1} loader=TextLoader{col=Label:R4:0 col=Features:R4:1-784} data=F:\data\MNIST\Test-28x28.txt
Automatically adding a MinMax normalization transform, use 'norm=Warn' or 'norm=No' to turn this behavior off.
Beginning optimization
   num vars: 7850
   term criterion: Mean Improvement

Iter n: new_value (term_crit)
-------------------------------------------------
Iter 0: 4.1740e0 (**********) 
Iter 1: 3.1533e0 (1.021e0) 
Iter 2: 2.4973e0 (7.289e-1) 
Iter 3: 1.4418e0 (9.777e-1) 
Iter 4: 1.0662e0 (5.243e-1) 
Iter 5: 9.1288e-1 (2.458e-1) 
Iter 6: 8.5584e-1 (1.042e-1) 
Iter 7: 7.9153e-1 (7.428e-2) 
Iter 8: 7.4611e-1 (5.264e-2) 
Iter 9: 6.9562e-1 (5.103e-2) 
Iter 10: 6.6097e-1 (3.874e-2) 
Iter 11: 6.2395e-1 (3.745e-2) 
Iter 12: 5.9911e-1 (2.799e-2) 
Iter 13: 5.6443e-1 (3.301e-2) 
Iter 14: 5.6369e-1 (8.806e-3) 
Iter 15: 5.3883e-1 (2.085e-2) 
Iter 16: 5.3190e-1 (1.041e-2) 
Iter 17: 5.2088e-1 (1.087e-2) .
Iter 18: 5.1579e-1 (6.533e-3) 
Iter 19: 5.0533e-1 (9.480e-3) 
Iter 20: 4.9239e-1 (1.207e-2) 
Iter 21: 4.8074e-1 (1.176e-2) 
Iter 22: 4.7058e-1 (1.056e-2) 
Iter 23: 4.6574e-1 (6.272e-3) 
Iter 24: 4.5728e-1 (7.907e-3) 
Iter 25: 4.4637e-1 (1.017e-2) .
Iter 26: 4.4133e-1 (6.319e-3) 
Iter 27: 4.3278e-1 (7.989e-3) 
Iter 28: 4.2411e-1 (8.499e-3) 
Iter 29: 4.1368e-1 (9.950e-3) 
Iter 30: 4.0673e-1 (7.699e-3) 
Iter 31: 4.0060e-1 (6.523e-3) 
Iter 32: 3.9612e-1 (4.992e-3) 
Iter 33: 3.9070e-1 (5.311e-3) 
Iter 34: 3.8299e-1 (7.110e-3) 
Iter 35: 3.7758e-1 (5.834e-3) 
Iter 36: 3.7515e-1 (3.287e-3) 
Iter 37: 3.6826e-1 (5.989e-3) 
Iter 38: 3.6329e-1 (5.223e-3) 
Iter 39: 3.5782e-1 (5.406e-3) 
Iter 40: 3.5272e-1 (5.179e-3) 
Iter 41: 3.4733e-1 (5.336e-3) 
Iter 42: 3.4448e-1 (3.474e-3) 
Iter 43: 3.4096e-1 (3.503e-3) 
Iter 44: 3.3786e-1 (3.207e-3) 
Iter 45: 3.3519e-1 (2.802e-3) 
Iter 46: 3.2940e-1 (5.046e-3) 
Iter 47: 3.2593e-1 (3.858e-3) 
Iter 48: 3.2187e-1 (4.009e-3) 
Iter 49: 3.1848e-1 (3.544e-3) 
Iter 50: 3.1469e-1 (3.733e-3) 
Iter 51: 3.1215e-1 (2.839e-3) 
Iter 52: 3.0810e-1 (3.742e-3) 
Iter 53: 3.0715e-1 (1.654e-3) 
Iter 54: 3.0464e-1 (2.294e-3) 
Iter 55: 3.0265e-1 (2.068e-3) 
Iter 56: 2.9920e-1 (3.101e-3) 
Iter 57: 2.9565e-1 (3.439e-3) .
Iter 58: 2.9453e-1 (1.696e-3) 
Iter 59: 2.9263e-1 (1.849e-3) 
Iter 60: 2.9093e-1 (1.744e-3) 
Iter 61: 2.8898e-1 (1.897e-3) 
Iter 62: 2.8677e-1 (2.130e-3) 
Iter 63: 2.8432e-1 (2.367e-3) 
Iter 64: 2.8220e-1 (2.182e-3) 
Iter 65: 2.8024e-1 (2.022e-3) 
Iter 66: 2.7832e-1 (1.942e-3) 
Iter 67: 2.7678e-1 (1.642e-3) 
Iter 68: 2.7487e-1 (1.840e-3) 
Iter 69: 2.7327e-1 (1.662e-3) 
Iter 70: 2.7133e-1 (1.868e-3) 
Iter 71: 2.6895e-1 (2.255e-3) .
Iter 72: 2.6823e-1 (1.107e-3) 
Iter 73: 2.6692e-1 (1.255e-3) 
Iter 74: 2.6592e-1 (1.065e-3) 
Iter 75: 2.6466e-1 (1.211e-3) 
Iter 76: 2.6358e-1 (1.116e-3) 
Iter 77: 2.6255e-1 (1.048e-3) 
Iter 78: 2.6139e-1 (1.135e-3) 
Iter 79: 2.6013e-1 (1.228e-3) 
Iter 80: 2.5897e-1 (1.178e-3) 
Iter 81: 2.5846e-1 (6.721e-4) 
Iter 82: 2.5716e-1 (1.146e-3) 
Iter 83: 2.5705e-1 (3.661e-4) 
Iter 84: 2.5551e-1 (1.250e-3) 
Iter 85: 2.5522e-1 (5.290e-4) 
Iter 86: 2.5406e-1 (1.003e-3) 
Iter 87: 2.5349e-1 (6.790e-4) 
Iter 88: 2.5263e-1 (8.115e-4) 
Iter 89: 2.5228e-1 (4.652e-4) 
Iter 90: 2.5164e-1 (5.949e-4) 
Iter 91: 2.5119e-1 (4.900e-4) 
Iter 92: 2.5036e-1 (7.442e-4) 
Iter 93: 2.4977e-1 (6.256e-4) 
Iter 94: 2.4920e-1 (5.850e-4) .
Iter 95: 2.4896e-1 (3.289e-4) 
Iter 96: 2.4857e-1 (3.710e-4) 
Iter 97: 2.4794e-1 (5.706e-4) 
Iter 98: 2.4741e-1 (5.398e-4) 
Iter 99: 2.4681e-1 (5.791e-4) 
Iter 100: 2.4654e-1 (3.502e-4) 
Iter 101: 2.4632e-1 (2.506e-4) 
Iter 102: 2.4601e-1 (3.008e-4) 
Iter 103: 2.4562e-1 (3.653e-4) 
Iter 104: 2.4515e-1 (4.406e-4) 
Iter 105: 2.4462e-1 (5.073e-4) 
Iter 106: 2.4424e-1 (4.120e-4) .
Iter 107: 2.4408e-1 (2.289e-4) 
Iter 108: 2.4377e-1 (2.880e-4) 
Iter 109: 2.4335e-1 (3.895e-4) .
Iter 110: 2.4316e-1 (2.325e-4) 
Iter 111: 2.4286e-1 (2.897e-4) 
Iter 112: 2.4260e-1 (2.665e-4) .
Iter 113: 2.4243e-1 (1.950e-4) 
Iter 114: 2.4211e-1 (2.873e-4) 
Iter 115: 2.4181e-1 (2.948e-4) 
Iter 116: 2.4150e-1 (3.085e-4) 
Iter 117: 2.4120e-1 (3.014e-4) 
Iter 118: 2.4105e-1 (1.874e-4) 
Iter 119: 2.4092e-1 (1.424e-4) 
Iter 120: 2.4059e-1 (2.823e-4) 
Iter 121: 2.4035e-1 (2.557e-4) 
Iter 122: 2.4020e-1 (1.718e-4) 
Iter 123: 2.3991e-1 (2.602e-4) 
Iter 124: 2.3983e-1 (1.287e-4) 
Iter 125: 2.3967e-1 (1.538e-4) 
Iter 126: 2.3951e-1 (1.526e-4) 
Iter 127: 2.3932e-1 (1.833e-4) 
Iter 128: 2.3908e-1 (2.241e-4) 
Iter 129: 2.3896e-1 (1.482e-4) 
Iter 130: 2.3880e-1 (1.532e-4) 
Iter 131: 2.3863e-1 (1.655e-4) 
Iter 132: 2.3852e-1 (1.264e-4) 
Iter 133: 2.3836e-1 (1.495e-4) 
Iter 134: 2.3820e-1 (1.640e-4) 
Iter 135: 2.3802e-1 (1.734e-4) 
Iter 136: 2.3789e-1 (1.370e-4) 
Iter 137: 2.3780e-1 (1.073e-4) 
Iter 138: 2.3768e-1 (1.130e-4) 
Iter 139: 2.3750e-1 (1.625e-4) 
Iter 140: 2.3734e-1 (1.642e-4) 
Iter 141: 2.3720e-1 (1.467e-4) 
Iter 142: 2.3711e-1 (1.033e-4) 
Iter 143: 2.3700e-1 (1.103e-4) 
Iter 144: 2.3689e-1 (1.087e-4) 
Iter 145: 2.3672e-1 (1.562e-4) 
Iter 146: 2.3662e-1 (1.101e-4) 
Iter 147: 2.3654e-1 (9.139e-5)
L1 regularization selected 3217 of 7850 weights.
Not training a calibrator because it is not needed.

 Confusion table (sampled)
          ||================================================================================
PREDICTED ||     0 |     1 |     2 |     3 |     4 |     5 |     6 |     7 |     8 |     9 | Recall
TRUTH     ||========================================================================================
       0  ||  5634 |     1 |    34 |    24 |    24 |    59 |    53 |     6 |    67 |    21 | 0.951
       1  ||     2 |  6529 |    35 |    18 |     8 |    49 |     5 |    25 |    57 |    14 | 0.968
       2  ||    26 |    65 |  5282 |    91 |   113 |    33 |    87 |   103 |   122 |    36 | 0.887
       3  ||    18 |    42 |   196 |  5229 |     6 |   297 |    32 |    80 |   128 |   103 | 0.853
       4  ||    18 |    25 |    64 |    12 |  5319 |     7 |    57 |    43 |    55 |   242 | 0.910
       5  ||    60 |    56 |    54 |   162 |    84 |  4624 |   136 |    18 |   149 |    78 | 0.853
       6  ||    43 |    30 |    81 |     3 |    47 |    75 |  5572 |     9 |    42 |    16 | 0.942
       7  ||    45 |    34 |    53 |    22 |    61 |    26 |     7 |  5771 |    23 |   223 | 0.921
       8  ||    34 |   188 |   132 |   153 |    25 |   212 |    75 |    24 |  4895 |   113 | 0.837
       9  ||    48 |    34 |    38 |    99 |   187 |    48 |     2 |   265 |    53 |  5175 | 0.870
      ======================================================================================
Precision || 0.950 | 0.932 | 0.885 | 0.900 | 0.906 | 0.852 | 0.925 | 0.910 | 0.876 | 0.859 |

ACCURACY(micro-avg):     0.900500
ACCURACY(macro-avg):     0.899167
LOG-LOSS:                0.358861
LOG-LOSS REDUCTION:      84.405208

OVERALL RESULTS
---------------------------------------
ACCURACY(micro-avg): 0.9005 (0.0000)
ACCURACY(macro-avg): 0.8992 (0.0000)
LOG-LOSS:            0.3589 (0.0000)
LOG-LOSS REDUCTION: 84.4052 (0.0000)

---------------------------------------
2/1/2016 4:37:51 PM	 Time elapsed(s): 2.378

