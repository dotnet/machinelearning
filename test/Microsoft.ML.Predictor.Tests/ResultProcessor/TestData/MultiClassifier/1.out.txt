maml.exe TrainTest test=F:\data\MNIST\Train-28x28.txt tr=MulticlassLogisticRegression{l2=0 ot=0.0001 m=5 initwts=0.5} loader=TextLoader{col=Label:R4:0 col=Features:R4:1-784} data=F:\data\MNIST\Test-28x28.txt
Automatically adding a MinMax normalization transform, use 'norm=Warn' or 'norm=No' to turn this behavior off.
Beginning optimization
   num vars: 7850
   term criterion: Mean Improvement

Iter n: new_value (term_crit)
-------------------------------------------------
Iter 0: 3.2847e0 (**********) 
Iter 1: 2.6089e0 (6.759e-1) 
Iter 2: 2.0796e0 (5.586e-1) 
Iter 3: 9.4274e-1 (9.992e-1) 
Iter 4: 9.0948e-1 (2.719e-1) 
Iter 5: 7.1213e-1 (2.159e-1) 
Iter 6: 6.6704e-1 (8.777e-2) 
Iter 7: 6.3741e-1 (4.417e-2) 
Iter 8: 5.7394e-1 (5.864e-2) 
Iter 9: 5.5432e-1 (2.938e-2) 
Iter 10: 5.0481e-1 (4.447e-2) 
Iter 11: 4.9127e-1 (2.127e-2) 
Iter 12: 4.7640e-1 (1.647e-2) 
Iter 13: 4.5983e-1 (1.654e-2) 
Iter 14: 4.5822e-1 (5.343e-3) 
Iter 15: 4.2810e-1 (2.393e-2) 
Iter 16: 4.1890e-1 (1.288e-2) 
Iter 17: 4.1011e-1 (9.818e-3) 
Iter 18: 4.0065e-1 (9.547e-3) 
Iter 19: 3.9903e-1 (3.598e-3) 
Iter 20: 3.8359e-1 (1.248e-2) 
Iter 21: 3.7951e-1 (6.181e-3) 
Iter 22: 3.7400e-1 (5.682e-3) 
Iter 23: 3.7193e-1 (2.973e-3) 
Iter 24: 3.6536e-1 (5.668e-3) 
Iter 25: 3.6149e-1 (4.317e-3) 
Iter 26: 3.5634e-1 (4.946e-3) 
Iter 27: 3.5106e-1 (5.196e-3) 
Iter 28: 3.4808e-1 (3.534e-3) 
Iter 29: 3.4435e-1 (3.678e-3) 
Iter 30: 3.4073e-1 (3.635e-3) 
Iter 31: 3.3699e-1 (3.714e-3) 
Iter 32: 3.3204e-1 (4.640e-3) 
Iter 33: 3.2858e-1 (3.761e-3) 
Iter 34: 3.2474e-1 (3.819e-3) 
Iter 35: 3.2150e-1 (3.386e-3) 
Iter 36: 3.1769e-1 (3.702e-3) 
Iter 37: 3.1342e-1 (4.128e-3) 
Iter 38: 3.0865e-1 (4.610e-3) 
Iter 39: 3.0458e-1 (4.203e-3) 
Iter 40: 2.9966e-1 (4.739e-3) 
Iter 41: 2.9727e-1 (2.982e-3) 
Iter 42: 2.9529e-1 (2.225e-3) 
Iter 43: 2.9119e-1 (3.633e-3) 
Iter 44: 2.8849e-1 (2.934e-3) 
Iter 45: 2.8608e-1 (2.540e-3) 
Iter 46: 2.8472e-1 (1.652e-3) 
Iter 47: 2.8259e-1 (2.014e-3) 
Iter 48: 2.8070e-1 (1.920e-3) 
Iter 49: 2.7859e-1 (2.061e-3) 
Iter 50: 2.7722e-1 (1.542e-3) 
Iter 51: 2.7615e-1 (1.193e-3) 
Iter 52: 2.7346e-1 (2.309e-3) .
Iter 53: 2.7234e-1 (1.419e-3) 
Iter 54: 2.7048e-1 (1.748e-3) 
Iter 55: 2.6865e-1 (1.813e-3) 
Iter 56: 2.6690e-1 (1.768e-3) .
Iter 57: 2.6621e-1 (9.567e-4) 
Iter 58: 2.6543e-1 (8.222e-4) 
Iter 59: 2.6308e-1 (1.969e-3) .
Iter 60: 2.6213e-1 (1.203e-3) 
Iter 61: 2.6032e-1 (1.663e-3) 
Iter 62: 2.5906e-1 (1.356e-3) 
Iter 63: 2.5741e-1 (1.581e-3) 
Iter 64: 2.5667e-1 (9.520e-4) 
Iter 65: 2.5607e-1 (6.872e-4) 
Iter 66: 2.5511e-1 (8.904e-4) 
Iter 67: 2.5433e-1 (8.092e-4) 
Iter 68: 2.5382e-1 (5.840e-4) 
Iter 69: 2.5282e-1 (8.943e-4) 
Iter 70: 2.5212e-1 (7.475e-4) 
Iter 71: 2.5168e-1 (5.187e-4) 
Iter 72: 2.5089e-1 (7.247e-4) 
Iter 73: 2.5024e-1 (6.638e-4) 
Iter 74: 2.4953e-1 (6.976e-4) 
Iter 75: 2.4870e-1 (7.981e-4) 
Iter 76: 2.4827e-1 (5.196e-4) 
Iter 77: 2.4778e-1 (5.017e-4) 
Iter 78: 2.4717e-1 (5.853e-4) 
Iter 79: 2.4682e-1 (4.033e-4) 
Iter 80: 2.4653e-1 (3.231e-4) 
Iter 81: 2.4590e-1 (5.530e-4) 
Iter 82: 2.4569e-1 (2.919e-4) 
Iter 83: 2.4530e-1 (3.693e-4) 
Iter 84: 2.4481e-1 (4.562e-4) 
Iter 85: 2.4473e-1 (1.755e-4) 
Iter 86: 2.4398e-1 (6.032e-4) 
Iter 87: 2.4376e-1 (3.167e-4) 
Iter 88: 2.4336e-1 (3.831e-4) 
Iter 89: 2.4292e-1 (4.216e-4) 
Iter 90: 2.4256e-1 (3.793e-4) 
Iter 91: 2.4222e-1 (3.448e-4) 
Iter 92: 2.4190e-1 (3.278e-4) 
Iter 93: 2.4160e-1 (3.073e-4) 
Iter 94: 2.4129e-1 (3.081e-4) 
Iter 95: 2.4084e-1 (4.174e-4) 
Iter 96: 2.4062e-1 (2.680e-4) 
Iter 97: 2.4036e-1 (2.668e-4) 
Iter 98: 2.4004e-1 (3.042e-4) 
Iter 99: 2.3977e-1 (2.796e-4) 
Iter 100: 2.3951e-1 (2.632e-4) 
Iter 101: 2.3920e-1 (3.013e-4) 
Iter 102: 2.3894e-1 (2.659e-4) 
Iter 103: 2.3882e-1 (1.542e-4) 
Iter 104: 2.3851e-1 (2.712e-4) 
Iter 105: 2.3838e-1 (1.655e-4) 
Iter 106: 2.3811e-1 (2.430e-4) 
Iter 107: 2.3788e-1 (2.337e-4) 
Iter 108: 2.3762e-1 (2.587e-4) 
Iter 109: 2.3741e-1 (2.234e-4) 
Iter 110: 2.3722e-1 (1.920e-4) 
Iter 111: 2.3711e-1 (1.299e-4) 
Iter 112: 2.3694e-1 (1.603e-4) 
Iter 113: 2.3679e-1 (1.528e-4) 
Iter 114: 2.3663e-1 (1.621e-4) 
Iter 115: 2.3638e-1 (2.237e-4) 
Iter 116: 2.3625e-1 (1.607e-4) 
Iter 117: 2.3598e-1 (2.356e-4) 
Iter 118: 2.3586e-1 (1.546e-4) 
Iter 119: 2.3572e-1 (1.446e-4) 
Iter 120: 2.3551e-1 (1.922e-4) 
Iter 121: 2.3536e-1 (1.606e-4) 
Iter 122: 2.3523e-1 (1.329e-4) 
Iter 123: 2.3511e-1 (1.272e-4) 
Iter 124: 2.3495e-1 (1.533e-4) 
Iter 125: 2.3482e-1 (1.365e-4) 
Iter 126: 2.3456e-1 (2.222e-4) 
Iter 127: 2.3446e-1 (1.312e-4) 
Iter 128: 2.3436e-1 (1.100e-4) 
Iter 129: 2.3435e-1 (3.403e-5)
L1 regularization selected 3094 of 7850 weights.
Not training a calibrator because it is not needed.

 Confusion table (sampled)
          ||================================================================================
PREDICTED ||     0 |     1 |     2 |     3 |     4 |     5 |     6 |     7 |     8 |     9 | Recall
TRUTH     ||========================================================================================
       0  ||  5617 |     1 |    35 |    30 |    27 |    63 |    52 |     5 |    67 |    26 | 0.948
       1  ||     2 |  6534 |    35 |    19 |     9 |    46 |     4 |    22 |    55 |    16 | 0.969
       2  ||    26 |    75 |  5252 |    94 |   115 |    36 |    91 |   111 |   120 |    38 | 0.882
       3  ||    17 |    41 |   187 |  5264 |     7 |   269 |    32 |    79 |   129 |   106 | 0.859
       4  ||    17 |    24 |    68 |    15 |  5306 |     8 |    60 |    46 |    62 |   236 | 0.908
       5  ||    58 |    56 |    55 |   182 |    96 |  4569 |   139 |    23 |   167 |    76 | 0.843
       6  ||    42 |    31 |    83 |     4 |    46 |    72 |  5565 |    10 |    52 |    13 | 0.940
       7  ||    44 |    36 |    51 |    21 |    62 |    21 |     8 |  5774 |    25 |   223 | 0.922
       8  ||    32 |   180 |   130 |   168 |    25 |   184 |    72 |    24 |  4922 |   114 | 0.841
       9  ||    49 |    34 |    38 |   101 |   191 |    42 |     3 |   260 |    53 |  5178 | 0.870
      ======================================================================================
Precision || 0.951 | 0.932 | 0.885 | 0.893 | 0.902 | 0.860 | 0.923 | 0.909 | 0.871 | 0.859 |

ACCURACY(micro-avg):     0.899683
ACCURACY(macro-avg):     0.898226
LOG-LOSS:                0.365303
LOG-LOSS REDUCTION:      84.125269

OVERALL RESULTS
---------------------------------------
ACCURACY(micro-avg): 0.8997 (0.0000)
ACCURACY(macro-avg): 0.8982 (0.0000)
LOG-LOSS:            0.3653 (0.0000)
LOG-LOSS REDUCTION: 84.1253 (0.0000)

---------------------------------------
2/1/2016 4:36:13 PM	 Time elapsed(s): 2.203

