maml.exe TrainTest test=F:\data\MNIST\Train-28x28.txt tr=MulticlassLogisticRegression{l2=0.1 ot=0.0001} loader=TextLoader{col=Label:R4:0 col=Features:R4:1-784} data=F:\data\MNIST\Test-28x28.txt
Automatically adding a MinMax normalization transform, use 'norm=Warn' or 'norm=No' to turn this behavior off.
Beginning optimization
   num vars: 7850
   term criterion: Mean Improvement

Iter n: new_value (term_crit)
-------------------------------------------------
Iter 0: 2.3026e0 (**********) 
Iter 1: 1.5070e0 (7.956e-1) 
Iter 2: 7.5425e-1 (7.613e-1) 
Iter 3: 6.9098e-1 (2.295e-1) 
Iter 4: 5.7505e-1 (1.440e-1) 
Iter 5: 5.3822e-1 (6.354e-2) 
Iter 6: 4.9624e-1 (4.737e-2) 
Iter 7: 4.8211e-1 (2.244e-2) 
Iter 8: 4.4661e-1 (3.224e-2) 
Iter 9: 4.3342e-1 (1.795e-2) 
Iter 10: 4.2010e-1 (1.448e-2) 
Iter 11: 4.0311e-1 (1.636e-2) 
Iter 12: 3.7701e-1 (2.367e-2) 
Iter 13: 3.6329e-1 (1.620e-2) 
Iter 14: 3.4584e-1 (1.714e-2) 
Iter 15: 3.3893e-1 (9.469e-3) 
Iter 16: 3.3196e-1 (7.590e-3) 
Iter 17: 3.2617e-1 (6.242e-3) 
Iter 18: 3.2093e-1 (5.494e-3) 
Iter 19: 3.1473e-1 (6.025e-3) 
Iter 20: 3.0970e-1 (5.275e-3) 
Iter 21: 3.0252e-1 (6.706e-3) 
Iter 22: 2.9996e-1 (3.593e-3) 
Iter 23: 2.9503e-1 (4.595e-3) 
Iter 24: 2.9293e-1 (2.726e-3) 
Iter 25: 2.9066e-1 (2.382e-3) 
Iter 26: 2.8766e-1 (2.846e-3) 
Iter 27: 2.8702e-1 (1.196e-3) 
Iter 28: 2.8227e-1 (3.861e-3) 
Iter 29: 2.8086e-1 (2.018e-3) 
Iter 30: 2.7895e-1 (1.942e-3) 
Iter 31: 2.7617e-1 (2.569e-3) .
Iter 32: 2.7472e-1 (1.731e-3) 
Iter 33: 2.7206e-1 (2.430e-3) .
Iter 34: 2.7140e-1 (1.098e-3) 
Iter 35: 2.7000e-1 (1.326e-3) 
Iter 36: 2.6825e-1 (1.645e-3) 
Iter 37: 2.6659e-1 (1.652e-3) 
Iter 38: 2.6487e-1 (1.706e-3) 
Iter 39: 2.6391e-1 (1.149e-3) 
Iter 40: 2.6299e-1 (9.735e-4) 
Iter 41: 2.6209e-1 (9.163e-4) 
Iter 42: 2.6021e-1 (1.642e-3) 
Iter 43: 2.5966e-1 (8.192e-4) 
Iter 44: 2.5745e-1 (1.868e-3) 
Iter 45: 2.5667e-1 (1.051e-3) 
Iter 46: 2.5566e-1 (1.016e-3) 
Iter 47: 2.5511e-1 (6.676e-4) 
Iter 48: 2.5393e-1 (1.056e-3) 
Iter 49: 2.5330e-1 (7.327e-4) 
Iter 50: 2.5269e-1 (6.438e-4) 
Iter 51: 2.5202e-1 (6.626e-4) 
Iter 52: 2.5117e-1 (7.989e-4) 
Iter 53: 2.5036e-1 (8.076e-4) 
Iter 54: 2.4969e-1 (7.079e-4) 
Iter 55: 2.4928e-1 (4.873e-4) 
Iter 56: 2.4877e-1 (4.980e-4) 
Iter 57: 2.4818e-1 (5.717e-4) 
Iter 58: 2.4780e-1 (4.232e-4) 
Iter 59: 2.4718e-1 (5.764e-4) 
Iter 60: 2.4665e-1 (5.371e-4) 
Iter 61: 2.4618e-1 (4.861e-4) 
Iter 62: 2.4581e-1 (3.979e-4) 
Iter 63: 2.4541e-1 (3.993e-4) 
Iter 64: 2.4510e-1 (3.359e-4) 
Iter 65: 2.4473e-1 (3.602e-4) 
Iter 66: 2.4440e-1 (3.382e-4) 
Iter 67: 2.4385e-1 (4.986e-4) 
Iter 68: 2.4323e-1 (5.924e-4) 
Iter 69: 2.4281e-1 (4.618e-4) 
Iter 70: 2.4247e-1 (3.652e-4) 
Iter 71: 2.4222e-1 (2.801e-4) 
Iter 72: 2.4182e-1 (3.735e-4) 
Iter 73: 2.4150e-1 (3.314e-4) 
Iter 74: 2.4115e-1 (3.465e-4) 
Iter 75: 2.4098e-1 (2.150e-4) 
Iter 76: 2.4074e-1 (2.335e-4) 
Iter 77: 2.4050e-1 (2.331e-4) 
Iter 78: 2.4030e-1 (2.085e-4) 
Iter 79: 2.4000e-1 (2.824e-4) 
Iter 80: 2.3973e-1 (2.699e-4) 
Iter 81: 2.3961e-1 (1.555e-4) 
Iter 82: 2.3942e-1 (1.852e-4) 
Iter 83: 2.3932e-1 (1.181e-4) 
Iter 84: 2.3920e-1 (1.190e-4) 
Iter 85: 2.3897e-1 (2.026e-4) 
Iter 86: 2.3887e-1 (1.271e-4) 
Iter 87: 2.3867e-1 (1.828e-4) 
Iter 88: 2.3846e-1 (2.045e-4) 
Iter 89: 2.3830e-1 (1.714e-4) 
Iter 90: 2.3806e-1 (2.200e-4) 
Iter 91: 2.3780e-1 (2.533e-4) 
Iter 92: 2.3763e-1 (1.916e-4) 
Iter 93: 2.3751e-1 (1.341e-4) 
Iter 94: 2.3736e-1 (1.481e-4) 
Iter 95: 2.3728e-1 (9.989e-5)
L1 regularization selected 3215 of 7850 weights.
Not training a calibrator because it is not needed.

 Confusion table (sampled)
          ||================================================================================
PREDICTED ||     0 |     1 |     2 |     3 |     4 |     5 |     6 |     7 |     8 |     9 | Recall
TRUTH     ||========================================================================================
       0  ||  5630 |     1 |    32 |    29 |    28 |    57 |    45 |     5 |    71 |    25 | 0.951
       1  ||     2 |  6536 |    34 |    18 |     9 |    48 |     4 |    23 |    52 |    16 | 0.969
       2  ||    27 |    74 |  5258 |    92 |   116 |    34 |    93 |   112 |   114 |    38 | 0.883
       3  ||    17 |    42 |   190 |  5240 |     6 |   294 |    31 |    82 |   126 |   103 | 0.855
       4  ||    16 |    23 |    64 |    14 |  5319 |     7 |    57 |    41 |    54 |   247 | 0.910
       5  ||    67 |    53 |    52 |   164 |    89 |  4603 |   144 |    23 |   150 |    76 | 0.849
       6  ||    38 |    31 |    77 |     3 |    49 |    75 |  5578 |    10 |    46 |    11 | 0.943
       7  ||    44 |    33 |    50 |    20 |    63 |    21 |     8 |  5780 |    19 |   227 | 0.923
       8  ||    34 |   183 |   133 |   163 |    25 |   210 |    72 |    24 |  4896 |   111 | 0.837
       9  ||    51 |    32 |    38 |    94 |   179 |    41 |     3 |   255 |    51 |  5205 | 0.875
      ======================================================================================
Precision || 0.950 | 0.933 | 0.887 | 0.898 | 0.904 | 0.854 | 0.924 | 0.910 | 0.878 | 0.859 |

ACCURACY(micro-avg):     0.900750
ACCURACY(macro-avg):     0.899359
LOG-LOSS:                0.359701
LOG-LOSS REDUCTION:      84.368683

OVERALL RESULTS
---------------------------------------
ACCURACY(micro-avg): 0.9008 (0.0000)
ACCURACY(macro-avg): 0.8994 (0.0000)
LOG-LOSS:            0.3597 (0.0000)
LOG-LOSS REDUCTION: 84.3687 (0.0000)

---------------------------------------
2/1/2016 4:37:33 PM	 Time elapsed(s): 1.864

