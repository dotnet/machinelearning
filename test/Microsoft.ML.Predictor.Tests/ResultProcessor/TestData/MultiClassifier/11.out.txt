maml.exe TrainTest test=F:\data\MNIST\Train-28x28.txt tr=MulticlassLogisticRegression{l2=0.1 l1=0 ot=0.0001 initwts=1} loader=TextLoader{col=Label:R4:0 col=Features:R4:1-784} data=F:\data\MNIST\Test-28x28.txt
Automatically adding a MinMax normalization transform, use 'norm=Warn' or 'norm=No' to turn this behavior off.
Beginning optimization
   num vars: 7850
   term criterion: Mean Improvement

Iter n: new_value (term_crit)
-------------------------------------------------
Iter 0: 3.8231e0 (**********) 
Iter 1: 2.8912e0 (9.319e-1) 
Iter 2: 2.0869e0 (8.298e-1) 
Iter 3: 1.0107e0 (1.018e0) 
Iter 4: 8.1092e-1 (4.018e-1) 
Iter 5: 7.0356e-1 (1.808e-1) 
Iter 6: 6.2951e-1 (1.007e-1) 
Iter 7: 5.7775e-1 (6.399e-2) 
Iter 8: 5.1079e-1 (6.622e-2) 
Iter 9: 4.6124e-1 (5.372e-2) 
Iter 10: 4.4502e-1 (2.560e-2) 
Iter 11: 4.0774e-1 (3.436e-2) 
Iter 12: 3.8691e-1 (2.421e-2) 
Iter 13: 3.6627e-1 (2.153e-2) 
Iter 14: 3.4648e-1 (2.022e-2) 
Iter 15: 3.3754e-1 (1.176e-2) 
Iter 16: 3.2342e-1 (1.353e-2) 
Iter 17: 3.1313e-1 (1.110e-2) 
Iter 18: 2.9539e-1 (1.608e-2) 
Iter 19: 2.8373e-1 (1.277e-2) 
Iter 20: 2.7511e-1 (9.658e-3) 
Iter 21: 2.6615e-1 (9.133e-3) 
Iter 22: 2.5530e-1 (1.042e-2) 
Iter 23: 2.4683e-1 (8.960e-3) 
Iter 24: 2.3959e-1 (7.664e-3) 
Iter 25: 2.3263e-1 (7.139e-3) 
Iter 26: 2.2564e-1 (7.024e-3) 
Iter 27: 2.2016e-1 (5.870e-3) 
Iter 28: 2.1863e-1 (2.617e-3) 
Iter 29: 2.1384e-1 (4.241e-3) 
Iter 30: 2.1222e-1 (2.280e-3) 
Iter 31: 2.0824e-1 (3.552e-3) 
Iter 32: 2.0369e-1 (4.297e-3) -
Iter 33: 2.0086e-1 (3.201e-3) 
Iter 34: 1.9554e-1 (4.789e-3) 
Iter 35: 1.9203e-1 (3.832e-3) 
Iter 36: 1.8911e-1 (3.149e-3) 
Iter 37: 1.8605e-1 (3.081e-3) 
Iter 38: 1.8213e-1 (3.706e-3) 
Iter 39: 1.7691e-1 (4.842e-3) 
Iter 40: 1.7325e-1 (3.960e-3) 
Iter 41: 1.6974e-1 (3.621e-3) 
Iter 42: 1.6794e-1 (2.254e-3) 
Iter 43: 1.6394e-1 (3.565e-3) 
Iter 44: 1.6257e-1 (1.915e-3) 
Iter 45: 1.5852e-1 (3.520e-3) 
Iter 46: 1.5666e-1 (2.273e-3) 
Iter 47: 1.5411e-1 (2.483e-3) 
Iter 48: 1.5048e-1 (3.343e-3) -
Iter 49: 1.4874e-1 (2.142e-3) 
Iter 50: 1.4577e-1 (2.758e-3) 
Iter 51: 1.4393e-1 (2.069e-3) 
Iter 52: 1.4167e-1 (2.217e-3) 
Iter 53: 1.3963e-1 (2.080e-3) 
Iter 54: 1.3660e-1 (2.796e-3) 
Iter 55: 1.3440e-1 (2.349e-3) 
Iter 56: 1.3269e-1 (1.872e-3) 
Iter 57: 1.3046e-1 (2.137e-3) 
Iter 58: 1.2947e-1 (1.278e-3) 
Iter 59: 1.2818e-1 (1.291e-3) 
Iter 60: 1.2738e-1 (9.185e-4) 
Iter 61: 1.2498e-1 (2.034e-3) 
Iter 62: 1.2380e-1 (1.388e-3) 
Iter 63: 1.2265e-1 (1.207e-3) 
Iter 64: 1.2101e-1 (1.532e-3) 
Iter 65: 1.1789e-1 (2.725e-3) 
Iter 66: 1.1606e-1 (2.051e-3) 
Iter 67: 1.1449e-1 (1.694e-3) 
Iter 68: 1.1347e-1 (1.186e-3) 
Iter 69: 1.1228e-1 (1.195e-3) 
Iter 70: 1.0953e-1 (2.356e-3) -
Iter 71: 1.0842e-1 (1.427e-3) 
Iter 72: 1.0692e-1 (1.480e-3) 
Iter 73: 1.0568e-1 (1.300e-3) 
Iter 74: 1.0447e-1 (1.233e-3) 
Iter 75: 1.0378e-1 (8.254e-4) 
Iter 76: 1.0182e-1 (1.673e-3) 
Iter 77: 1.0077e-1 (1.211e-3) 
Iter 78: 9.9999e-2 (8.775e-4) 
Iter 79: 9.9163e-2 (8.466e-4) -
Iter 80: 9.8516e-2 (6.971e-4) 
Iter 81: 9.7131e-2 (1.212e-3) 
Iter 82: 9.6100e-2 (1.077e-3) 
Iter 83: 9.5183e-2 (9.569e-4) 
Iter 84: 9.4146e-2 (1.017e-3) -
Iter 85: 9.3702e-2 (5.875e-4) 
Iter 86: 9.2772e-2 (8.442e-4) 
Iter 87: 9.1964e-2 (8.175e-4) 
Iter 88: 9.0848e-2 (1.041e-3) 
Iter 89: 8.9643e-2 (1.164e-3) -
Iter 90: 8.9164e-2 (6.497e-4) 
Iter 91: 8.8266e-2 (8.362e-4) 
Iter 92: 8.7615e-2 (6.973e-4) 
Iter 93: 8.6886e-2 (7.209e-4) 
Iter 94: 8.6204e-2 (6.921e-4) 
Iter 95: 8.5547e-2 (6.656e-4) 
Iter 96: 8.4967e-2 (6.015e-4) 
Iter 97: 8.4460e-2 (5.309e-4) 
Iter 98: 8.3877e-2 (5.694e-4) 
Iter 99: 8.3299e-2 (5.760e-4) 
Iter 100: 8.3016e-2 (3.561e-4) 
Iter 101: 8.2466e-2 (5.019e-4) 
Iter 102: 8.1968e-2 (4.989e-4) -
Iter 103: 8.1683e-2 (3.383e-4) 
Iter 104: 8.1260e-2 (4.022e-4) 
Iter 105: 8.0915e-2 (3.588e-4) 
Iter 106: 8.0312e-2 (5.420e-4) 
Iter 107: 7.9886e-2 (4.555e-4) 
Iter 108: 7.9423e-2 (4.610e-4) 
Iter 109: 7.9126e-2 (3.379e-4) 
Iter 110: 7.8489e-2 (5.620e-4) 
Iter 111: 7.7780e-2 (6.720e-4) 
Iter 112: 7.7078e-2 (6.946e-4) 
Iter 113: 7.6576e-2 (5.504e-4) 
Iter 114: 7.6183e-2 (4.321e-4) 
Iter 115: 7.5888e-2 (3.299e-4) 
Iter 116: 7.5555e-2 (3.322e-4) 
Iter 117: 7.4949e-2 (5.371e-4) 
Iter 118: 7.4329e-2 (5.997e-4) 
Iter 119: 7.3934e-2 (4.463e-4) 
Iter 120: 7.3560e-2 (3.914e-4) 
Iter 121: 7.3372e-2 (2.394e-4) 
Iter 122: 7.3305e-2 (1.102e-4) 
Iter 123: 7.3103e-2 (1.788e-4) 
Iter 124: 7.2951e-2 (1.585e-4) 
Iter 125: 7.2787e-2 (1.627e-4) 
Iter 126: 7.2537e-2 (2.283e-4) -
Iter 127: 7.2431e-2 (1.361e-4) 
Iter 128: 7.2163e-2 (2.353e-4) 
Iter 129: 7.1967e-2 (2.056e-4) 
Iter 130: 7.1806e-2 (1.724e-4) 
Iter 131: 7.1653e-2 (1.577e-4) 
Iter 132: 7.1565e-2 (1.054e-4) 
Iter 133: 7.1323e-2 (2.077e-4) 
Iter 134: 7.1145e-2 (1.860e-4) 
Iter 135: 7.0919e-2 (2.158e-4) 
Iter 136: 7.0620e-2 (2.781e-4) 
Iter 137: 7.0488e-2 (1.687e-4) 
Iter 138: 7.0333e-2 (1.585e-4) 
Iter 139: 7.0149e-2 (1.770e-4) 
Iter 140: 6.9915e-2 (2.203e-4) 
Iter 141: 6.9599e-2 (2.916e-4) 
Iter 142: 6.9390e-2 (2.298e-4) 
Iter 143: 6.9171e-2 (2.215e-4) 
Iter 144: 6.8983e-2 (1.970e-4) 
Iter 145: 6.8848e-2 (1.502e-4) 
Iter 146: 6.8772e-2 (9.474e-5)
Not training a calibrator because it is not needed.

 Confusion table (sampled)
          ||================================================================================
PREDICTED ||     0 |     1 |     2 |     3 |     4 |     5 |     6 |     7 |     8 |     9 | Recall
TRUTH     ||========================================================================================
       0  ||  5559 |     1 |    58 |    24 |    35 |    91 |    56 |     9 |    55 |    35 | 0.939
       1  ||     2 |  6502 |    42 |    32 |     9 |    45 |     9 |    25 |    58 |    18 | 0.964
       2  ||    34 |    76 |  5148 |   123 |   111 |    49 |   107 |   128 |   132 |    50 | 0.864
       3  ||    23 |    36 |   223 |  5090 |    16 |   314 |    33 |    71 |   184 |   141 | 0.830
       4  ||    21 |    25 |    95 |    26 |  5120 |    15 |    61 |    85 |    83 |   311 | 0.876
       5  ||    69 |    44 |    67 |   158 |    91 |  4496 |   147 |    33 |   237 |    79 | 0.829
       6  ||    48 |    30 |   150 |     3 |    58 |    93 |  5460 |     9 |    48 |    19 | 0.923
       7  ||    39 |    24 |    76 |    34 |    75 |    34 |     5 |  5695 |    35 |   248 | 0.909
       8  ||    41 |   156 |   234 |   224 |    39 |   277 |    73 |    18 |  4631 |   158 | 0.791
       9  ||    38 |    26 |    48 |   110 |   234 |    66 |     3 |   337 |    59 |  5028 | 0.845
      ======================================================================================
Precision || 0.946 | 0.940 | 0.838 | 0.874 | 0.885 | 0.820 | 0.917 | 0.888 | 0.839 | 0.826 |

ACCURACY(micro-avg):     0.878817
ACCURACY(macro-avg):     0.877128
LOG-LOSS:                0.694903
LOG-LOSS REDUCTION:      69.802067

OVERALL RESULTS
---------------------------------------
ACCURACY(micro-avg): 0.8788 (0.0000)
ACCURACY(macro-avg): 0.8771 (0.0000)
LOG-LOSS:            0.6949 (0.0000)
LOG-LOSS REDUCTION: 69.8021 (0.0000)

---------------------------------------
2/1/2016 4:37:08 PM	 Time elapsed(s): 2.294

