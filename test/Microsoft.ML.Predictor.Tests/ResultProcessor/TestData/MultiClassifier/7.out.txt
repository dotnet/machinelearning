maml.exe TrainTest test=F:\data\MNIST\Train-28x28.txt tr=MulticlassLogisticRegression{ot=0.0001 m=5 initwts=1} loader=TextLoader{col=Label:R4:0 col=Features:R4:1-784} data=F:\data\MNIST\Test-28x28.txt
Automatically adding a MinMax normalization transform, use 'norm=Warn' or 'norm=No' to turn this behavior off.
Beginning optimization
   num vars: 7850
   term criterion: Mean Improvement

Iter n: new_value (term_crit)
-------------------------------------------------
Iter 0: 4.6235e0 (**********) 
Iter 1: 3.4830e0 (1.140e0) 
Iter 2: 2.8213e0 (7.575e-1) 
Iter 3: 1.9260e0 (8.625e-1) 
Iter 4: 1.4827e0 (5.468e-1) 
Iter 5: 1.0967e0 (4.261e-1) 
Iter 6: 9.9812e-1 (1.804e-1) 
Iter 7: 8.9318e-1 (1.238e-1) 
Iter 8: 8.4867e-1 (6.433e-2) 
Iter 9: 7.6923e-1 (7.567e-2) .
Iter 10: 7.4447e-1 (3.749e-2) 
Iter 11: 7.0319e-1 (4.033e-2) 
Iter 12: 6.6901e-1 (3.571e-2) 
Iter 13: 6.3394e-1 (3.524e-2) 
Iter 14: 6.1205e-1 (2.522e-2) 
Iter 15: 5.8698e-1 (2.511e-2) 
Iter 16: 5.6268e-1 (2.450e-2) 
Iter 17: 5.4983e-1 (1.577e-2) 
Iter 18: 5.3533e-1 (1.481e-2) .
Iter 19: 5.3030e-1 (7.480e-3) 
Iter 20: 5.2442e-1 (6.279e-3) 
Iter 21: 5.1390e-1 (9.458e-3) 
Iter 22: 4.9856e-1 (1.387e-2) .
Iter 23: 4.9304e-1 (7.607e-3) 
Iter 24: 4.8508e-1 (7.871e-3) 
Iter 25: 4.7698e-1 (8.045e-3) 
Iter 26: 4.6665e-1 (9.757e-3) 
Iter 27: 4.5694e-1 (9.721e-3) 
Iter 28: 4.5051e-1 (7.259e-3) 
Iter 29: 4.4586e-1 (5.303e-3) 
Iter 30: 4.4018e-1 (5.583e-3) 
Iter 31: 4.2996e-1 (9.060e-3) 
Iter 32: 4.2225e-1 (8.046e-3) 
Iter 33: 4.1660e-1 (6.250e-3) 
Iter 34: 4.0993e-1 (6.562e-3) 
Iter 35: 4.0336e-1 (6.572e-3) 
Iter 36: 3.9529e-1 (7.698e-3) 
Iter 37: 3.8717e-1 (8.011e-3) 
Iter 38: 3.8343e-1 (4.809e-3) 
Iter 39: 3.7957e-1 (4.096e-3) 
Iter 40: 3.7155e-1 (7.041e-3) .
Iter 41: 3.6757e-1 (4.746e-3) 
Iter 42: 3.6133e-1 (5.868e-3) 
Iter 43: 3.5399e-1 (6.971e-3) 
Iter 44: 3.5032e-1 (4.495e-3) 
Iter 45: 3.4618e-1 (4.225e-3) 
Iter 46: 3.4368e-1 (2.933e-3) 
Iter 47: 3.4148e-1 (2.381e-3) 
Iter 48: 3.3748e-1 (3.595e-3) 
Iter 49: 3.3255e-1 (4.597e-3) 
Iter 50: 3.2760e-1 (4.865e-3) 
Iter 51: 3.2488e-1 (3.257e-3) 
Iter 52: 3.2207e-1 (2.922e-3) 
Iter 53: 3.2029e-1 (2.062e-3) 
Iter 54: 3.1626e-1 (3.540e-3) .
Iter 55: 3.1497e-1 (1.849e-3) 
Iter 56: 3.1297e-1 (1.968e-3) 
Iter 57: 3.1078e-1 (2.135e-3) 
Iter 58: 3.0829e-1 (2.402e-3) 
Iter 59: 3.0472e-1 (3.272e-3) 
Iter 60: 3.0173e-1 (3.063e-3) 
Iter 61: 2.9943e-1 (2.490e-3) 
Iter 62: 2.9718e-1 (2.309e-3) 
Iter 63: 2.9433e-1 (2.719e-3) 
Iter 64: 2.9322e-1 (1.512e-3) 
Iter 65: 2.9086e-1 (2.144e-3) 
Iter 66: 2.8901e-1 (1.925e-3) 
Iter 67: 2.8784e-1 (1.358e-3) 
Iter 68: 2.8594e-1 (1.767e-3) 
Iter 69: 2.8479e-1 (1.299e-3) 
Iter 70: 2.8349e-1 (1.304e-3) 
Iter 71: 2.8181e-1 (1.581e-3) 
Iter 72: 2.8064e-1 (1.272e-3) 
Iter 73: 2.7944e-1 (1.221e-3) 
Iter 74: 2.7835e-1 (1.119e-3) 
Iter 75: 2.7745e-1 (9.560e-4) 
Iter 76: 2.7652e-1 (9.354e-4) 
Iter 77: 2.7559e-1 (9.312e-4) 
Iter 78: 2.7399e-1 (1.435e-3) 
Iter 79: 2.7315e-1 (9.903e-4) 
Iter 80: 2.7241e-1 (8.057e-4) 
Iter 81: 2.7158e-1 (8.232e-4) 
Iter 82: 2.7086e-1 (7.468e-4) 
Iter 83: 2.7019e-1 (6.841e-4) 
Iter 84: 2.6958e-1 (6.311e-4) 
Iter 85: 2.6834e-1 (1.086e-3) 
Iter 86: 2.6799e-1 (5.350e-4) 
Iter 87: 2.6732e-1 (6.368e-4) 
Iter 88: 2.6699e-1 (4.063e-4) 
Iter 89: 2.6639e-1 (5.496e-4) 
Iter 90: 2.6556e-1 (7.641e-4) 
Iter 91: 2.6555e-1 (1.954e-4) 
Iter 92: 2.6441e-1 (9.071e-4) 
Iter 93: 2.6415e-1 (4.183e-4) 
Iter 94: 2.6370e-1 (4.441e-4) 
Iter 95: 2.6307e-1 (5.837e-4) 
Iter 96: 2.6250e-1 (5.691e-4) 
Iter 97: 2.6192e-1 (5.791e-4) 
Iter 98: 2.6164e-1 (3.573e-4) 
Iter 99: 2.6130e-1 (3.424e-4) 
Iter 100: 2.6118e-1 (1.729e-4) 
Iter 101: 2.6075e-1 (3.706e-4) 
Iter 102: 2.6057e-1 (2.289e-4) 
Iter 103: 2.6038e-1 (1.985e-4) 
Iter 104: 2.6010e-1 (2.606e-4) 
Iter 105: 2.5979e-1 (2.944e-4) 
Iter 106: 2.5944e-1 (3.364e-4) 
Iter 107: 2.5913e-1 (3.158e-4) 
Iter 108: 2.5889e-1 (2.584e-4) 
Iter 109: 2.5871e-1 (1.994e-4) 
Iter 110: 2.5855e-1 (1.730e-4) 
Iter 111: 2.5832e-1 (2.122e-4) 
Iter 112: 2.5813e-1 (1.995e-4) 
Iter 113: 2.5793e-1 (1.976e-4) 
Iter 114: 2.5773e-1 (2.030e-4) 
Iter 115: 2.5757e-1 (1.683e-4) 
Iter 116: 2.5741e-1 (1.598e-4) 
Iter 117: 2.5727e-1 (1.475e-4) 
Iter 118: 2.5714e-1 (1.345e-4) 
Iter 119: 2.5700e-1 (1.345e-4) 
Iter 120: 2.5682e-1 (1.693e-4) 
Iter 121: 2.5671e-1 (1.248e-4) 
Iter 122: 2.5657e-1 (1.389e-4) 
Iter 123: 2.5651e-1 (7.944e-5)
L1 regularization selected 3543 of 7850 weights.
Not training a calibrator because it is not needed.

 Confusion table (sampled)
          ||================================================================================
PREDICTED ||     0 |     1 |     2 |     3 |     4 |     5 |     6 |     7 |     8 |     9 | Recall
TRUTH     ||========================================================================================
       0  ||  5623 |     1 |    31 |    27 |    28 |    62 |    54 |     6 |    66 |    25 | 0.949
       1  ||     2 |  6538 |    32 |    23 |     7 |    48 |     3 |    21 |    53 |    15 | 0.970
       2  ||    28 |    69 |  5262 |    84 |   121 |    38 |    92 |   111 |   119 |    34 | 0.883
       3  ||    19 |    48 |   185 |  5246 |     6 |   283 |    33 |    77 |   130 |   104 | 0.856
       4  ||    14 |    24 |    58 |     8 |  5346 |     7 |    53 |    34 |    55 |   243 | 0.915
       5  ||    66 |    58 |    54 |   159 |    84 |  4619 |   137 |    24 |   145 |    75 | 0.852
       6  ||    36 |    32 |    73 |     3 |    46 |    68 |  5596 |    10 |    44 |    10 | 0.946
       7  ||    44 |    35 |    55 |    19 |    63 |    21 |     7 |  5775 |    18 |   228 | 0.922
       8  ||    33 |   190 |   124 |   156 |    21 |   201 |    73 |    27 |  4916 |   110 | 0.840
       9  ||    50 |    32 |    34 |    92 |   170 |    43 |     3 |   255 |    50 |  5220 | 0.877
      ======================================================================================
Precision || 0.951 | 0.930 | 0.891 | 0.902 | 0.907 | 0.857 | 0.925 | 0.911 | 0.878 | 0.861 |

ACCURACY(micro-avg):     0.902350
ACCURACY(macro-avg):     0.901011
LOG-LOSS:                0.346282
LOG-LOSS REDUCTION:      84.951841

OVERALL RESULTS
---------------------------------------
ACCURACY(micro-avg): 0.9024 (0.0000)
ACCURACY(macro-avg): 0.9010 (0.0000)
LOG-LOSS:            0.3463 (0.0000)
LOG-LOSS REDUCTION: 84.9518 (0.0000)

---------------------------------------
2/1/2016 4:36:40 PM	 Time elapsed(s): 2.305

