maml.exe TrainTest test=F:\data\MNIST\Train-28x28.txt tr=MulticlassLogisticRegression{l2=0 l1=0 m=50 initwts=0.1} loader=TextLoader{col=Label:R4:0 col=Features:R4:1-784} data=F:\data\MNIST\Test-28x28.txt
Automatically adding a MinMax normalization transform, use 'norm=Warn' or 'norm=No' to turn this behavior off.
Beginning optimization
   num vars: 7850
   term criterion: Mean Improvement

Iter n: new_value (term_crit)
-------------------------------------------------
Iter 0: 2.3296e0 (**********) 
Iter 1: 1.5121e0 (8.175e-1) 
Iter 2: 7.2460e-1 (7.935e-1) 
Iter 3: 6.4508e-1 (2.495e-1) 
Iter 4: 5.5085e-1 (1.326e-1) 
Iter 5: 5.0413e-1 (6.812e-2) 
Iter 6: 4.5785e-1 (5.174e-2) 
Iter 7: 4.3921e-1 (2.692e-2) 
Iter 8: 4.0841e-1 (2.983e-2) 
Iter 9: 3.9240e-1 (1.946e-2) 
Iter 10: 3.7718e-1 (1.628e-2) 
Iter 11: 3.5551e-1 (2.033e-2) 
Iter 12: 3.4337e-1 (1.418e-2) 
Iter 13: 3.2222e-1 (1.941e-2) 
Iter 14: 3.1273e-1 (1.197e-2) 
Iter 15: 3.0069e-1 (1.202e-2) 
Iter 16: 2.9199e-1 (9.530e-3) 
Iter 17: 2.7756e-1 (1.321e-2) 
Iter 18: 2.6757e-1 (1.080e-2) 
Iter 19: 2.6101e-1 (7.620e-3) 
Iter 20: 2.5336e-1 (7.640e-3) 
Iter 21: 2.4439e-1 (8.636e-3) 
Iter 22: 2.3689e-1 (7.781e-3) 
Iter 23: 2.3000e-1 (7.112e-3) 
Iter 24: 2.2545e-1 (5.192e-3) 
Iter 25: 2.1933e-1 (5.887e-3) 
Iter 26: 2.1177e-1 (7.144e-3) 
Iter 27: 2.0559e-1 (6.421e-3) 
Iter 28: 2.0129e-1 (4.830e-3) 
Iter 29: 1.9703e-1 (4.400e-3) 
Iter 30: 1.9204e-1 (4.843e-3) 
Iter 31: 1.8599e-1 (5.751e-3) 
Iter 32: 1.8053e-1 (5.535e-3) 
Iter 33: 1.7599e-1 (4.785e-3) 
Iter 34: 1.6996e-1 (5.722e-3) 
Iter 35: 1.6535e-1 (4.885e-3) 
Iter 36: 1.6098e-1 (4.498e-3) 
Iter 37: 1.5606e-1 (4.818e-3) 
Iter 38: 1.4938e-1 (6.212e-3) 
Iter 39: 1.4648e-1 (3.732e-3) 
Iter 40: 1.4115e-1 (4.929e-3) 
Iter 41: 1.3876e-1 (3.022e-3) 
Iter 42: 1.3578e-1 (2.988e-3) 
Iter 43: 1.3170e-1 (3.807e-3) 
Iter 44: 1.2455e-1 (6.316e-3) 
Iter 45: 1.1632e-1 (7.752e-3) 
Iter 46: 1.1019e-1 (6.538e-3) 
Iter 47: 1.0678e-1 (4.188e-3) 
Iter 48: 1.0201e-1 (4.629e-3) 
Iter 49: 9.5948e-2 (5.701e-3) 
Iter 50: 9.1303e-2 (4.910e-3) 
Iter 51: 8.6475e-2 (4.848e-3) 
Iter 52: 8.2891e-2 (3.900e-3) 
Iter 53: 7.9888e-2 (3.228e-3) 
Iter 54: 7.5453e-2 (4.133e-3) 
Iter 55: 6.9426e-2 (5.553e-3) -
Iter 56: 6.7299e-2 (2.983e-3) 
Iter 57: 6.4585e-2 (2.782e-3) 
Iter 58: 6.2735e-2 (2.083e-3) 
Iter 59: 6.1183e-2 (1.684e-3) 
Iter 60: 5.9629e-2 (1.587e-3) -
Iter 61: 5.8977e-2 (8.851e-4) 
Iter 62: 5.7440e-2 (1.374e-3) 
Iter 63: 5.5325e-2 (1.930e-3) 
Iter 64: 5.3405e-2 (1.923e-3) 
Iter 65: 5.1222e-2 (2.118e-3) -
Iter 66: 5.0385e-2 (1.157e-3) 
Iter 67: 4.8072e-2 (2.024e-3) 
Iter 68: 4.5584e-2 (2.372e-3) 
Iter 69: 4.2688e-2 (2.764e-3) 
Iter 70: 4.0685e-2 (2.194e-3) 
Iter 71: 3.8280e-2 (2.353e-3) 
Iter 72: 3.6519e-2 (1.908e-3) 
Iter 73: 3.4207e-2 (2.212e-3) 
Iter 74: 3.2246e-2 (2.023e-3) 
Iter 75: 3.0394e-2 (1.895e-3) 
Iter 76: 2.8329e-2 (2.023e-3) 
Iter 77: 2.5991e-2 (2.259e-3) 
Iter 78: 2.4432e-2 (1.734e-3) 
Iter 79: 2.2591e-2 (1.814e-3) 
Iter 80: 2.0984e-2 (1.659e-3) 
Iter 81: 1.9810e-2 (1.295e-3) 
Iter 82: 1.7181e-2 (2.296e-3) 
Iter 83: 1.4335e-2 (2.708e-3) 
Iter 84: 1.1596e-2 (2.732e-3) 
Iter 85: 9.5465e-3 (2.220e-3) 
Iter 86: 7.6347e-3 (1.989e-3) 
Iter 87: 5.7700e-3 (1.896e-3) 
Iter 88: 4.0812e-3 (1.741e-3) 
Iter 89: 2.6135e-3 (1.536e-3) 
Iter 90: 1.3675e-3 (1.318e-3) 
Iter 91: 9.1624e-4 (6.681e-4) 
Iter 92: 5.5871e-4 (4.352e-4) 
Iter 93: 3.0846e-4 (2.965e-4) 
Iter 94: 1.6258e-4 (1.835e-4) 
Iter 95: 8.7334e-5 (1.023e-4) 
Iter 96: 4.3977e-5 (5.810e-5) 
Iter 97: 2.2450e-5 (3.067e-5) 
Iter 98: 1.1749e-5 (1.569e-5) 
Iter 99: 6.8497e-6 (7.598e-6) 
Iter 100: 5.1453e-6 (3.178e-6) 
Iter 101: 3.3554e-6 (2.137e-6) 
Iter 102: 1.5762e-6 (1.869e-6) -
Iter 103: 1.4679e-6 (5.484e-7) 
Iter 104: 1.4236e-6 (1.703e-7) 
Iter 105: 1.2146e-6 (1.994e-7) 
Iter 106: 1.0330e-6 (1.860e-7) ---------- +------------Not training a calibrator because it is not needed.

 Confusion table (sampled)
          ||================================================================================
PREDICTED ||     0 |     1 |     2 |     3 |     4 |     5 |     6 |     7 |     8 |     9 | Recall
TRUTH     ||========================================================================================
       0  ||  5549 |     1 |    64 |    23 |    33 |    93 |    58 |    10 |    56 |    36 | 0.937
       1  ||     2 |  6439 |    78 |    45 |     7 |    58 |    11 |    32 |    54 |    16 | 0.955
       2  ||    32 |    98 |  5067 |   151 |    98 |    58 |   117 |   133 |   150 |    54 | 0.850
       3  ||    22 |    26 |   263 |  4992 |    19 |   313 |    33 |    78 |   214 |   171 | 0.814
       4  ||    21 |    24 |   117 |    41 |  5020 |    26 |    64 |    93 |    96 |   340 | 0.859
       5  ||    76 |    38 |    67 |   159 |   108 |  4400 |   139 |    39 |   303 |    92 | 0.812
       6  ||    53 |    28 |   213 |     4 |    47 |    99 |  5401 |     6 |    46 |    21 | 0.913
       7  ||    46 |    21 |    83 |    44 |    77 |    43 |     4 |  5625 |    35 |   287 | 0.898
       8  ||    34 |   158 |   291 |   250 |    42 |   306 |    71 |    17 |  4505 |   177 | 0.770
       9  ||    40 |    24 |    64 |   130 |   294 |    72 |     2 |   410 |    58 |  4855 | 0.816
      ======================================================================================
Precision || 0.945 | 0.939 | 0.803 | 0.855 | 0.874 | 0.805 | 0.915 | 0.873 | 0.817 | 0.803 |

ACCURACY(micro-avg):     0.864217
ACCURACY(macro-avg):     0.862409
LOG-LOSS:                4.122146
LOG-LOSS REDUCTION:      -79.133437

OVERALL RESULTS
---------------------------------------
ACCURACY(micro-avg): 0.8642 (0.0000)
ACCURACY(macro-avg): 0.8624 (0.0000)
LOG-LOSS:            4.1221 (0.0000)
LOG-LOSS REDUCTION: -79.1334 (0.0000)

---------------------------------------
2/1/2016 4:37:47 PM	 Time elapsed(s): 2.111

