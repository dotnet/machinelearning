maml.exe TrainTest test=F:\data\MNIST\Train-28x28.txt tr=MultiClassLogisticRegression{l2=0 l1=0 initwts=0.1} loader=TextLoader{col=Label:R4:0 col=Features:R4:1-784} data=F:\data\MNIST\Test-28x28.txt
Automatically adding a MinMax normalization transform, use 'norm=Warn' or 'norm=No' to turn this behavior off.
Beginning optimization
   num vars: 7850
   term criterion: Mean Improvement

Iter n: new_value (term_crit)
-------------------------------------------------
Iter 0: 2.3498e0 (**********) 
Iter 1: 1.6767e0 (6.730e-1) 
Iter 2: 9.8600e-1 (6.872e-1) 
Iter 3: 7.5539e-1 (3.393e-1) 
Iter 4: 5.8707e-1 (2.106e-1) 
Iter 5: 5.1195e-1 (1.089e-1) 
Iter 6: 4.6881e-1 (5.956e-2) 
Iter 7: 4.3842e-1 (3.768e-2) 
Iter 8: 4.0879e-1 (3.165e-2) 
Iter 9: 3.9517e-1 (1.812e-2) 
Iter 10: 3.7631e-1 (1.868e-2) 
Iter 11: 3.5336e-1 (2.188e-2) 
Iter 12: 3.2910e-1 (2.367e-2) 
Iter 13: 3.1282e-1 (1.812e-2) 
Iter 14: 2.9271e-1 (1.962e-2) 
Iter 15: 2.8552e-1 (1.029e-2) 
Iter 16: 2.7118e-1 (1.333e-2) 
Iter 17: 2.6454e-1 (8.311e-3) 
Iter 18: 2.5769e-1 (7.217e-3) 
Iter 19: 2.4821e-1 (8.909e-3) 
Iter 20: 2.3814e-1 (9.782e-3) 
Iter 21: 2.3071e-1 (8.020e-3) 
Iter 22: 2.2609e-1 (5.471e-3) 
Iter 23: 2.2124e-1 (5.004e-3) 
Iter 24: 2.1747e-1 (4.078e-3) 
Iter 25: 2.1159e-1 (5.427e-3) -
Iter 26: 2.0842e-1 (3.736e-3) 
Iter 27: 2.0355e-1 (4.584e-3) 
Iter 28: 2.0027e-1 (3.612e-3) 
Iter 29: 1.9765e-1 (2.861e-3) -
Iter 30: 1.9577e-1 (2.125e-3) 
Iter 31: 1.9152e-1 (3.723e-3) 
Iter 32: 1.8734e-1 (4.061e-3) 
Iter 33: 1.8392e-1 (3.584e-3) 
Iter 34: 1.8150e-1 (2.713e-3) 
Iter 35: 1.7933e-1 (2.303e-3) 
Iter 36: 1.7362e-1 (4.859e-3) 
Iter 37: 1.7100e-1 (3.182e-3) 
Iter 38: 1.6806e-1 (2.999e-3) 
Iter 39: 1.6430e-1 (3.567e-3) 
Iter 40: 1.6114e-1 (3.264e-3) 
Iter 41: 1.5781e-1 (3.317e-3) 
Iter 42: 1.5430e-1 (3.458e-3) 
Iter 43: 1.5152e-1 (2.952e-3) 
Iter 44: 1.4861e-1 (2.915e-3) 
Iter 45: 1.4498e-1 (3.455e-3) 
Iter 46: 1.4179e-1 (3.255e-3) 
Iter 47: 1.3941e-1 (2.602e-3) 
Iter 48: 1.3710e-1 (2.383e-3) 
Iter 49: 1.3606e-1 (1.370e-3) 
Iter 50: 1.3421e-1 (1.736e-3) 
Iter 51: 1.3178e-1 (2.254e-3) 
Iter 52: 1.3040e-1 (1.597e-3) 
Iter 53: 1.2804e-1 (2.168e-3) 
Iter 54: 1.2531e-1 (2.588e-3) 
Iter 55: 1.2300e-1 (2.382e-3) 
Iter 56: 1.2063e-1 (2.374e-3) 
Iter 57: 1.1850e-1 (2.188e-3) 
Iter 58: 1.1556e-1 (2.756e-3) 
Iter 59: 1.1322e-1 (2.446e-3) 
Iter 60: 1.1068e-1 (2.511e-3) 
Iter 61: 1.0818e-1 (2.503e-3) 
Iter 62: 1.0508e-1 (2.951e-3) 
Iter 63: 1.0142e-1 (3.485e-3) 
Iter 64: 9.8731e-2 (2.888e-3) 
Iter 65: 9.6062e-2 (2.724e-3) 
Iter 66: 9.4365e-2 (1.954e-3) 
Iter 67: 9.0539e-2 (3.358e-3) 
Iter 68: 8.7540e-2 (3.089e-3) 
Iter 69: 8.4895e-2 (2.756e-3) 
Iter 70: 8.2954e-2 (2.144e-3) 
Iter 71: 8.1115e-2 (1.916e-3) 
Iter 72: 7.9563e-2 (1.642e-3) -
Iter 73: 7.8453e-2 (1.243e-3) 
Iter 74: 7.5820e-2 (2.286e-3) 
Iter 75: 7.3262e-2 (2.490e-3) 
Iter 76: 7.1766e-2 (1.744e-3) 
Iter 77: 7.0128e-2 (1.665e-3) 
Iter 78: 6.9180e-2 (1.127e-3) 
Iter 79: 6.6826e-2 (2.047e-3) 
Iter 80: 6.4850e-2 (1.994e-3) 
Iter 81: 6.2824e-2 (2.018e-3) 
Iter 82: 6.1630e-2 (1.400e-3) 
Iter 83: 5.9882e-2 (1.661e-3) 
Iter 84: 5.8505e-2 (1.448e-3) 
Iter 85: 5.6529e-2 (1.844e-3) 
Iter 86: 5.4796e-2 (1.760e-3) 
Iter 87: 5.3043e-2 (1.755e-3) 
Iter 88: 5.0521e-2 (2.331e-3) 
Iter 89: 4.9048e-2 (1.687e-3) 
Iter 90: 4.7579e-2 (1.524e-3) 
Iter 91: 4.6446e-2 (1.230e-3) 
Iter 92: 4.5212e-2 (1.233e-3) 
Iter 93: 4.3792e-2 (1.374e-3) 
Iter 94: 4.2054e-2 (1.647e-3) 
Iter 95: 4.0906e-2 (1.273e-3) 
Iter 96: 3.9970e-2 (1.020e-3) 
Iter 97: 3.8975e-2 (1.001e-3) 
Iter 98: 3.7818e-2 (1.118e-3) 
Iter 99: 3.6256e-2 (1.451e-3) 
Iter 100: 3.5817e-2 (6.917e-4) 
Iter 101: 3.4319e-2 (1.297e-3) 
Iter 102: 3.3599e-2 (8.638e-4) 
Iter 103: 3.2602e-2 (9.639e-4) 
Iter 104: 3.1333e-2 (1.193e-3) -
Iter 105: 3.0523e-2 (9.056e-4) 
Iter 106: 2.9398e-2 (1.070e-3) 
Iter 107: 2.8509e-2 (9.343e-4) 
Iter 108: 2.7290e-2 (1.147e-3) 
Iter 109: 2.5624e-2 (1.536e-3) 
Iter 110: 2.2753e-2 (2.538e-3) -
Iter 111: 2.1899e-2 (1.275e-3) 
Iter 112: 1.9929e-2 (1.796e-3) 
Iter 113: 1.8211e-2 (1.737e-3) 
Iter 114: 1.7369e-2 (1.066e-3) 
Iter 115: 1.5652e-2 (1.554e-3) 
Iter 116: 1.5076e-2 (8.202e-4) 
Iter 117: 1.4122e-2 (9.205e-4) 
Iter 118: 1.3079e-2 (1.013e-3) 
Iter 119: 1.1455e-2 (1.471e-3) 
Iter 120: 1.0610e-2 (1.002e-3) 
Iter 121: 9.6946e-3 (9.369e-4) 
Iter 122: 9.3150e-3 (5.190e-4) 
Iter 123: 8.7853e-3 (5.270e-4) 
Iter 124: 8.3689e-3 (4.441e-4) 
Iter 125: 7.5232e-3 (7.453e-4) 
Iter 126: 7.0593e-3 (5.342e-4) 
Iter 127: 6.6535e-3 (4.380e-4) 
Iter 128: 6.1527e-3 (4.851e-4) 
Iter 129: 5.3778e-3 (7.024e-4) 
Iter 130: 4.7046e-3 (6.805e-4) 
Iter 131: 4.3185e-3 (4.597e-4) 
Iter 132: 4.0178e-3 (3.404e-4) 
Iter 133: 3.7640e-3 (2.755e-4) 
Iter 134: 3.3143e-3 (4.061e-4) 
Iter 135: 2.5227e-3 (6.952e-4) -
Iter 136: 2.1740e-3 (4.354e-4) 
Iter 137: 1.8743e-3 (3.336e-4) 
Iter 138: 1.6185e-3 (2.752e-4) 
Iter 139: 1.3720e-3 (2.537e-4) 
Iter 140: 1.0991e-3 (2.681e-4) 
Iter 141: 9.0393e-4 (2.134e-4) 
Iter 142: 7.0997e-4 (1.988e-4) -
Iter 143: 6.2010e-4 (1.171e-4) 
Iter 144: 5.1601e-4 (1.073e-4) 
Iter 145: 4.4853e-4 (7.745e-5) 
Iter 146: 3.4886e-4 (9.412e-5) 
Iter 147: 2.7436e-4 (7.940e-5) 
Iter 148: 1.9860e-4 (7.667e-5) 
Iter 149: 1.6978e-4 (4.078e-5) 
Iter 150: 1.0197e-4 (6.106e-5) 
Iter 151: 6.2151e-5 (4.513e-5) 
Iter 152: 5.0229e-5 (2.022e-5) 
Iter 153: 3.4589e-5 (1.679e-5) 
Iter 154: 2.0286e-5 (1.492e-5) 
Iter 155: 1.2289e-5 (9.729e-6) 
Iter 156: 6.6986e-6 (6.625e-6) 
Iter 157: 5.1125e-6 (2.846e-6) 
Iter 158: 1.5137e-6 (3.411e-6) 
Iter 159: 1.4908e-6 (8.698e-7) 
Iter 160: 1.4099e-6 (2.781e-7) 
Iter 161: 1.0689e-6 (3.253e-7) +---------- +------------Not training a calibrator because it is not needed.

 Confusion table (sampled)
          ||================================================================================
PREDICTED ||     0 |     1 |     2 |     3 |     4 |     5 |     6 |     7 |     8 |     9 | Recall
TRUTH     ||========================================================================================
       0  ||  5535 |     1 |    61 |    25 |    32 |   101 |    58 |    10 |    61 |    39 | 0.934
       1  ||     2 |  6427 |    81 |    58 |     6 |    53 |    10 |    32 |    58 |    15 | 0.953
       2  ||    32 |    98 |  5028 |   165 |   107 |    56 |   119 |   134 |   169 |    50 | 0.844
       3  ||    23 |    30 |   257 |  4991 |    19 |   325 |    32 |    75 |   217 |   162 | 0.814
       4  ||    25 |    23 |   101 |    33 |  5030 |    18 |    57 |    94 |   109 |   352 | 0.861
       5  ||    77 |    44 |    65 |   164 |   101 |  4397 |   143 |    36 |   309 |    85 | 0.811
       6  ||    57 |    32 |   203 |     5 |    49 |    98 |  5394 |     5 |    54 |    21 | 0.911
       7  ||    42 |    21 |    79 |    46 |    82 |    43 |     5 |  5606 |    40 |   301 | 0.895
       8  ||    36 |   152 |   289 |   243 |    44 |   324 |    72 |    17 |  4503 |   171 | 0.770
       9  ||    40 |    25 |    58 |   126 |   291 |    70 |     1 |   398 |    71 |  4869 | 0.818
      ======================================================================================
Precision || 0.943 | 0.938 | 0.808 | 0.852 | 0.873 | 0.802 | 0.916 | 0.875 | 0.805 | 0.803 |

ACCURACY(micro-avg):     0.863000
ACCURACY(macro-avg):     0.861219
LOG-LOSS:                4.034470
LOG-LOSS REDUCTION:      -75.323386

OVERALL RESULTS
---------------------------------------
ACCURACY(micro-avg): 0.8630 (0.0000)
ACCURACY(macro-avg): 0.8612 (0.0000)
LOG-LOSS:            4.0345 (0.0000)
LOG-LOSS REDUCTION: -75.3234 (0.0000)

---------------------------------------
2/1/2016 4:36:17 PM	 Time elapsed(s): 2.487

