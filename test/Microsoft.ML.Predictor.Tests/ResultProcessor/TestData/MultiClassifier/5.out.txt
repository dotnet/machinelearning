maml.exe TrainTest test=F:\data\MNIST\Train-28x28.txt tr=MulticlassLogisticRegression{l1=0.1 ot=0.0001 initwts=1} loader=TextLoader{col=Label:R4:0 col=Features:R4:1-784} data=F:\data\MNIST\Test-28x28.txt
Automatically adding a MinMax normalization transform, use 'norm=Warn' or 'norm=No' to turn this behavior off.
Beginning optimization
   num vars: 7850
   term criterion: Mean Improvement

Iter n: new_value (term_crit)
-------------------------------------------------
Iter 0: 4.8055e0 (**********) 
Iter 1: 3.5716e0 (1.234e0) 
Iter 2: 2.9485e0 (7.453e-1) 
Iter 3: 2.0446e0 (8.661e-1) 
Iter 4: 1.4020e0 (6.978e-1) 
Iter 5: 9.5808e-1 (5.072e-1) 
Iter 6: 7.8564e-1 (2.561e-1) 
Iter 7: 7.0093e-1 (1.275e-1) 
Iter 8: 6.3557e-1 (8.090e-2) 
Iter 9: 5.8210e-1 (6.033e-2) 
Iter 10: 5.4418e-1 (4.352e-2) 
Iter 11: 5.0559e-1 (3.982e-2) 
Iter 12: 4.8122e-1 (2.823e-2) 
Iter 13: 4.6119e-1 (2.208e-2) 
Iter 14: 4.3394e-1 (2.596e-2) 
Iter 15: 4.0556e-1 (2.777e-2) 
Iter 16: 3.7623e-1 (2.894e-2) 
Iter 17: 3.5489e-1 (2.324e-2) 
Iter 18: 3.4706e-1 (1.168e-2) 
Iter 19: 3.3804e-1 (9.688e-3) 
Iter 20: 3.2447e-1 (1.260e-2) 
Iter 21: 3.1067e-1 (1.350e-2) 
Iter 22: 3.0163e-1 (1.015e-2) 
Iter 23: 2.9532e-1 (7.271e-3) 
Iter 24: 2.9063e-1 (5.331e-3) 
Iter 25: 2.8638e-1 (4.520e-3) 
Iter 26: 2.8325e-1 (3.482e-3) 
Iter 27: 2.7524e-1 (6.874e-3) 
Iter 28: 2.7137e-1 (4.622e-3) 
Iter 29: 2.6813e-1 (3.585e-3) 
Iter 30: 2.6180e-1 (5.642e-3) 
Iter 31: 2.5614e-1 (5.661e-3) 
Iter 32: 2.5231e-1 (4.289e-3) 
Iter 33: 2.4918e-1 (3.420e-3) 
Iter 34: 2.4735e-1 (2.224e-3) 
Iter 35: 2.4502e-1 (2.307e-3) 
Iter 36: 2.4236e-1 (2.568e-3) 
Iter 37: 2.3977e-1 (2.586e-3) 
Iter 38: 2.3699e-1 (2.728e-3) 
Iter 39: 2.3420e-1 (2.778e-3) 
Iter 40: 2.3194e-1 (2.387e-3) 
Iter 41: 2.2890e-1 (2.879e-3) 
Iter 42: 2.2597e-1 (2.917e-3) 
Iter 43: 2.2366e-1 (2.459e-3) 
Iter 44: 2.2058e-1 (2.925e-3) 
Iter 45: 2.2005e-1 (1.129e-3) 
Iter 46: 2.1633e-1 (3.076e-3) 
Iter 47: 2.1539e-1 (1.473e-3) 
Iter 48: 2.1368e-1 (1.651e-3) 
Iter 49: 2.1127e-1 (2.218e-3) 
Iter 50: 2.1032e-1 (1.264e-3) 
Iter 51: 2.0784e-1 (2.178e-3) 
Iter 52: 2.0710e-1 (1.098e-3) 
Iter 53: 2.0583e-1 (1.226e-3) 
Iter 54: 2.0317e-1 (2.303e-3) .
Iter 55: 2.0239e-1 (1.163e-3) 
Iter 56: 2.0062e-1 (1.620e-3) 
Iter 57: 1.9934e-1 (1.362e-3) 
Iter 58: 1.9826e-1 (1.150e-3) 
Iter 59: 1.9690e-1 (1.309e-3) 
Iter 60: 1.9608e-1 (9.453e-4) 
Iter 61: 1.9457e-1 (1.364e-3) 
Iter 62: 1.9323e-1 (1.349e-3) .
Iter 63: 1.9254e-1 (8.565e-4) 
Iter 64: 1.9108e-1 (1.303e-3) 
Iter 65: 1.8981e-1 (1.284e-3) 
Iter 66: 1.8837e-1 (1.400e-3) 
Iter 67: 1.8764e-1 (8.936e-4) 
Iter 68: 1.8646e-1 (1.108e-3) 
Iter 69: 1.8523e-1 (1.204e-3) 
Iter 70: 1.8430e-1 (1.000e-3) 
Iter 71: 1.8346e-1 (8.762e-4) 
Iter 72: 1.8285e-1 (6.805e-4) 
Iter 73: 1.8238e-1 (5.177e-4) 
Iter 74: 1.8170e-1 (6.387e-4) 
Iter 75: 1.8102e-1 (6.747e-4) 
Iter 76: 1.8050e-1 (5.539e-4) 
Iter 77: 1.8005e-1 (4.796e-4) 
Iter 78: 1.7926e-1 (7.116e-4) 
Iter 79: 1.7898e-1 (3.837e-4) 
Iter 80: 1.7850e-1 (4.619e-4) 
Iter 81: 1.7819e-1 (3.485e-4) 
Iter 82: 1.7766e-1 (4.799e-4) 
Iter 83: 1.7733e-1 (3.673e-4) 
Iter 84: 1.7669e-1 (5.761e-4) 
Iter 85: 1.7605e-1 (6.242e-4) 
Iter 86: 1.7550e-1 (5.693e-4) 
Iter 87: 1.7518e-1 (3.797e-4) 
Iter 88: 1.7473e-1 (4.317e-4) 
Iter 89: 1.7451e-1 (2.753e-4) 
Iter 90: 1.7411e-1 (3.666e-4) 
Iter 91: 1.7373e-1 (3.756e-4) 
Iter 92: 1.7344e-1 (3.103e-4) 
Iter 93: 1.7342e-1 (9.550e-5)
L1 regularization selected 6218 of 7850 weights.
Not training a calibrator because it is not needed.

 Confusion table (sampled)
          ||================================================================================
PREDICTED ||     0 |     1 |     2 |     3 |     4 |     5 |     6 |     7 |     8 |     9 | Recall
TRUTH     ||========================================================================================
       0  ||  5606 |     1 |    46 |    27 |    28 |    65 |    58 |    10 |    59 |    23 | 0.946
       1  ||     2 |  6530 |    41 |    18 |     9 |    45 |     5 |    19 |    57 |    16 | 0.969
       2  ||    30 |    75 |  5282 |    87 |   118 |    39 |    85 |   101 |   105 |    36 | 0.887
       3  ||    20 |    42 |   213 |  5229 |    10 |   271 |    33 |    73 |   128 |   112 | 0.853
       4  ||    20 |    20 |    80 |    15 |  5312 |    11 |    50 |    39 |    56 |   239 | 0.909
       5  ||    63 |    46 |    61 |   186 |    97 |  4564 |   144 |    23 |   167 |    70 | 0.842
       6  ||    39 |    27 |   115 |     2 |    55 |    74 |  5545 |     8 |    45 |     8 | 0.937
       7  ||    39 |    33 |    60 |    25 |    72 |    26 |     5 |  5755 |    24 |   226 | 0.919
       8  ||    35 |   167 |   190 |   177 |    29 |   223 |    72 |    17 |  4827 |   114 | 0.825
       9  ||    44 |    23 |    47 |   102 |   201 |    43 |     3 |   274 |    49 |  5163 | 0.868
      ======================================================================================
Precision || 0.950 | 0.938 | 0.861 | 0.891 | 0.896 | 0.851 | 0.924 | 0.911 | 0.875 | 0.859 |

ACCURACY(micro-avg):     0.896883
ACCURACY(macro-avg):     0.895407
LOG-LOSS:                0.384789
LOG-LOSS REDUCTION:      83.278463

OVERALL RESULTS
---------------------------------------
ACCURACY(micro-avg): 0.8969 (0.0000)
ACCURACY(macro-avg): 0.8954 (0.0000)
LOG-LOSS:            0.3848 (0.0000)
LOG-LOSS REDUCTION: 83.2785 (0.0000)

---------------------------------------
2/1/2016 4:36:32 PM	 Time elapsed(s): 1.983

