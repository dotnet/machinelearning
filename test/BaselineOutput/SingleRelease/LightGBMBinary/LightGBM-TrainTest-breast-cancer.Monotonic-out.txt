maml.exe TrainTest test=%Data% tr=LightGBMBinary{nt=1 nl=5 mil=5 lr=0.25 iter=20 mb=255 mc=pos:0 mc=neg:1} cache=- dout=%Output% loader=Text{sparse- col=Attr:TX:6 col=Label:0 col=Features:1-5,6,7-9} data=%Data% out=%Output% seed=1
Not adding a normalizer.
Auto-tuning parameters: UseCat = False
LightGBM objective=binary
Not training a calibrator because it is not needed.
TEST POSITIVE RATIO:	0.3448 (241.0/(241.0+458.0))
Confusion table
          ||======================
PREDICTED || positive | negative | Recall
TRUTH     ||======================
 positive ||      239 |        2 | 0.9917
 negative ||       14 |      444 | 0.9694
          ||======================
Precision ||   0.9447 |   0.9955 |
OVERALL 0/1 ACCURACY: 0.977110
LOG LOSS/instance:  0.093528
Test-set entropy (prior Log-Loss/instance): 0.929318
LOG-LOSS REDUCTION (RIG): 89.935846
AUC:                0.996924

OVERALL RESULTS
---------------------------------------
AUC:                0.996924 (0.0000)
Accuracy:           0.977110 (0.0000)
Positive precision: 0.944664 (0.0000)
Positive recall:    0.991701 (0.0000)
Negative precision: 0.995516 (0.0000)
Negative recall:    0.969432 (0.0000)
Log-loss:           0.093528 (0.0000)
Log-loss reduction: 89.935846 (0.0000)
F1 Score:           0.967611 (0.0000)
AUPRC:              0.992861 (0.0000)

---------------------------------------
Physical memory usage(MB): %Number%
Virtual memory usage(MB): %Number%
%DateTime%	 Time elapsed(s): %Number%

--- Progress log ---
[1] 'Loading data for LightGBM' started.
[1] 'Loading data for LightGBM' finished in %Time%.
[2] 'Training with LightGBM' started.
[2] 'Training with LightGBM' finished in %Time%.
[3] 'Saving model' started.
[3] 'Saving model' finished in %Time%.
