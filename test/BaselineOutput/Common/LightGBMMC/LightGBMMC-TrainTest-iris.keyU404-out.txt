maml.exe TrainTest test=%Data% tr=LightGBMMC{nt=1 iter=10 v=- lr=0.2 mil=10 nl=20} dout=%Output% loader=Text{col=Label:U4[0-2]:0 col=Features:1-4} data=%Data% out=%Output% seed=1
Not adding a normalizer.
Auto-tuning parameters: UseCategoricalSplit = False
Auto-tuning parameters: UseSoftmax = False
LightGBM objective=multiclassova
Not training a calibrator because it is not needed.

Confusion table
          ||========================
PREDICTED ||     0 |     1 |     2 | Recall
TRUTH     ||========================
        0 ||    50 |     0 |     0 | 1.0000
        1 ||     0 |    47 |     3 | 0.9400
        2 ||     0 |     1 |    49 | 0.9800
          ||========================
Precision ||1.0000 |0.9792 |0.9423 |
Accuracy(micro-avg): 0.973333
Accuracy(macro-avg): 0.973333
Log-loss:           0.161048
Log-loss reduction: 0.853408

OVERALL RESULTS
---------------------------------------
Accuracy(micro-avg): 0.973333 (0.0000)
Accuracy(macro-avg): 0.973333 (0.0000)
Log-loss:           0.161048 (0.0000)
Log-loss reduction: 0.853408 (0.0000)

---------------------------------------
Physical memory usage(MB): %Number%
Virtual memory usage(MB): %Number%
%DateTime%	 Time elapsed(s): %Number%

--- Progress log ---
[1] 'Loading data for LightGBM' started.
[1] 'Loading data for LightGBM' finished in %Time%.
[2] 'Training with LightGBM' started.
[2] 'Training with LightGBM' finished in %Time%.
[3] 'Saving model' started.
[3] 'Saving model' finished in %Time%.
